<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<feed xmlns="http://www.w3.org/2005/Atom">

	<title>Community Redis Aggregator</title>
	<link rel="self" href="http://www.planetredis.org/atom.xml"/>
	<link href="http://www.planetredis.org/"/>
	<id>http://www.planetredis.org/atom.xml</id>
	<updated>2015-06-08T06:00:09+00:00</updated>
	<generator uri="http://www.planetplanet.org/">Planet/2.0 +http://www.planetplanet.org</generator>

	<entry>
		<title type="html">Adventures in message queues</title>
		<link href="http://antirez.com/news/88"/>
		<id>http://antirez.com/news/88</id>
		<updated>2015-06-03T01:40:05+00:00</updated>
		<content type="html">EDIT: In case you missed it, Disque source code is now available at http://github.com/antirez/disque
&lt;br /&gt;
&lt;br /&gt;It is a few months that I spend ~ 15-20% of my time, mostly hours stolen to nights and weekends, working to a new system. It’s a message broker and it’s called Disque. I’ve an implementation of 80% of what was in the original specification, but still I don’t feel like it’s ready to be released. Since I can’t ship, I’ll at least blog… so that’s the story of how it started and a few details about what it is.
&lt;br /&gt;
&lt;br /&gt;~ First steps ~
&lt;br /&gt;
&lt;br /&gt;Many developers use Redis as a message queue, often wrappered via some library abstracting away Redis low level primitives, other times directly building a simple, ad-hoc queue, using the Redis raw API. This use case is covered mainly using blocking list operations, and list push operations. Redis apparently is at the same time the best and the worst system to use like that. It’s good because it is fast, easy to inspect, deploy and use, and in many environments it was already one piece of the infrastructure. However it has disadvantages because Redis mutable data structures are very different than immutable messages. Redis HA / Cluster tradeoffs are totally biased towards large mutable values, but the same tradeoffs are not the best ones to deal with messages.
&lt;br /&gt;
&lt;br /&gt;One thing that is important to guarantee for a message broker is that a message is delivered either at least one time, or at most one time. In short given that to guarantee an exact single delivery of a message (where for delivery we intent a message that was received *and* processed by a worker) is practically impossible, the choices are that the message broker is able to guarantee either 0 or 1 deliveries, or 1 to infinite deliveries. This is often referred as at-most-once semantics, and at-least-once semantics. There are use cases for the first, but the most interesting and practical semantics is the latter, that is, to guarantee that a message is delivered at least one time, and deliver multiple times if there are failures.
&lt;br /&gt;
&lt;br /&gt;So a few months ago I started to think at some client-side protocol to use a set of Redis masters (without replication or clustering whatsoever) in a way that provides these guarantees. Sometimes with small changes in the way Redis is used for an use case, it is possible to end with a better system. For example for distributed locks I tried to document an algorithm which is trivial to implement but more robust than the single-instance + failover implementation (http://redis.io/topics/distlock).
&lt;br /&gt;
&lt;br /&gt;However after a few days of work my design draft suggested that it was a better bet to design an ad-hoc system, since the client-side algorithm ended being too complex, non optimal, and certain things I absolutely wanted were impossible or very hard to do. To add more things to Redis sounded like a bad idea, it does a lot of things already, and to cover messaging well I needed things which are very different than the way Redis operates. But why to design a new system given that the world is full of message brokers? Because an impressive number of users were using Redis instead of systems specifically designed for this goal, and this was strange. A few can be wrong, but so many need to get some reason. Maybe Redis low barrier of entry, easy API, speed, were not what most people were accustomed to when they looked at the message brokers landscape. It seems populated by solutions that are either too simple, asking the application to do too much, or too complex, but super full featured. Maybe there is some space for the “Redis of messaging”?
&lt;br /&gt;
&lt;br /&gt;~ Redis brutally forked ~
&lt;br /&gt;
&lt;br /&gt;For the first time in my life I didn’t started straight away to write code. For weeks I looked at the design from time to time, converted it into a new system and not a Redis client library, and tried to understand, as an user, what would make me very happy in a message broker. The original use case remained the same: delayed jobs. Disque is a general system, but 90% of times in the design the “reference” was an user that has to solve the problem of sending messages that are likely jobs to process. If something was against this use case, it was removed.
&lt;br /&gt;
&lt;br /&gt;When the design was ready, I finally started to code. But where to start? “vi main.c”? Fortunately Redis is, in part, a framework to write distributed systems in C. I had a protocol, network libraries, clients handling, node-to-node message bus. To rewrite all this from scratch sounded like a huge waste. At the same time I wanted Disque to be able to completely diverge from Redis in any details possible if this is needed, and I wanted it to be a side project without impacts on Redis itself. So instead of trying the huge undertake of splitting Redis into an actual separated framework, and the Redis implementation, I took a more pragmatic approach: I forked the code, and removed everything that was Redis specific from the source code, in order to end with a skeleton. At this point I was ready to implement my specification.
&lt;br /&gt;
&lt;br /&gt;~ What is Disque? ~
&lt;br /&gt;
&lt;br /&gt;After a few months of very non intense work and just 200 commits I’ve finally a system that no longer looks like a toy: it looked like a toy for many weeks so I was afraid of even talking about it, since the probability of me just deleting the source tree was big. Now that most of the idea is working code with tests, I’m finally sure this will be released in the future, and to talk about the tradeoffs I took in the design.
&lt;br /&gt;
&lt;br /&gt;Disque is a distributed system, by default. Since it is an AP system, it made no sense to have like in Redis a single-node mode and a distributed mode. A single Disque node is just a particular case of a cluster, having just one node.
&lt;br /&gt;So this was of the important points in the design: fault tolerant, resistant to partitions, and available no matter how many nodes are still up, aka AP. I also wanted a system that was inherently able to scale in different scenarios, both when the issue is many producers and consumers with many queues, and when instead all this producers and consumers are all focusing on a single queue, that may be distributed into multiple nodes.
&lt;br /&gt;
&lt;br /&gt;My requirements were telling me aloud one thing… that Disque was going to make a big design sacrifice. Message ordering. Disque only provides best-effort ordering. However because of this sacrifice, there is a lot to gain… tradeoffs are interesting since sometimes they totally open the design space.
&lt;br /&gt;
&lt;br /&gt;I could continue recounting you what Disque is like that, however a few months ago I saw a comment in Hacker News, written by Jacques Chester, see https://news.ycombinator.com/item?id=8709146 [EDIT: SORRY I made an error cut&amp;amp;pasting the wrong name of Adrian (Hi Adrian, sorry for misquoting you!)]. Jacques, that happens to work for Pivotal like me, was commenting how different messaging systems have very different set of features, properties, and without the details it is almost impossible to evaluate the different choices, and to evaluate if one is faster than the other because it has a better implementation, or simple offers a lot less guarantees. So he wrote a set of questions one should ask when evaluating a messaging system. I’ll use his questions, and add a few more, in order to describe what Disque is, in the hope that I don’t end just hand waving, but providing some actual information.
&lt;br /&gt;
&lt;br /&gt;Q: Are messages delivered at least once?
&lt;br /&gt;
&lt;br /&gt;In Disque you can chose at least once delivery (the default), or at most once delivery. This property can be set per message. At most once delivery is just a special case of at least once delivery, setting the “retry” parameter of the message to 0, and replicating the message to a single node.
&lt;br /&gt;
&lt;br /&gt;Q: Are messages acknowledged by consumers?
&lt;br /&gt;
&lt;br /&gt;Yes, the only way for a consumer to tell the system the message got delivered correctly, is to acknowledge it.
&lt;br /&gt;
&lt;br /&gt;Q: Are messages delivered multiple times if not acknowledged?
&lt;br /&gt;
&lt;br /&gt;Yes, Disque will automatically deliver the message again, after a “retry” time, forever (up to the max TTL time for the message).
&lt;br /&gt;When messages are acknowledged, the acknowledge is propagated to the nodes having a copy of the message. If the system believes everybody was reached, the message is finally garbage collected and removed. Acknowledged messages are also evicted during memory pressure.
&lt;br /&gt;
&lt;br /&gt;Nodes run a best-effort algorithm to avoid to queue the same message multiple times, in order to approximate single delivery better. However during failures, multiple nodes may re-deliver the same message multiple times at the same time.
&lt;br /&gt;
&lt;br /&gt;Q: Is queueing durable or ephemeral.
&lt;br /&gt;
&lt;br /&gt;Durable.
&lt;br /&gt;
&lt;br /&gt;Q: Is durability achieved by writing every message to disk first, or by replicating messages across servers?
&lt;br /&gt;
&lt;br /&gt;By default Disque runs in-memory only, and uses synchronous replication to achieve durability (however you can ask, per message, to use asynchronous replication). It is possible to turn AOF (similarly to Redis) if desired, if the setup is likely to see a mass-reboot or alike. When the system is upgraded it is possible to write the AOF on disk just for the upgrade in order to don’t lose the state after a restart even if normally disk persistence is not used.
&lt;br /&gt;
&lt;br /&gt;Q: Is queueing partially/totally consistent across a group of servers or divided up for maximal throughput?
&lt;br /&gt;
&lt;br /&gt;Divided up for throughput, however message ordering is preserved in a best-effort way. Each message has an immutable “ctime” which is a wall-clock milliseconds timestamp plus an incremental ID for messages generated in the same millisecond. Nodes use this ctime in order to sort messages for delivery.
&lt;br /&gt;
&lt;br /&gt;Q: Can messages be dropped entirely under pressure? (aka best effort)
&lt;br /&gt;
&lt;br /&gt;No, however new messages may be refused if there is no space in memory. When 75% of memory is in use, nodes receiving messages try to externally replicate them, just to outer nodes, without taking a copy, but it many not work if also the other nodes are in an out of memory condition.
&lt;br /&gt;
&lt;br /&gt;Q: Can consumers and producers look into the queue, or is it totally opaque?
&lt;br /&gt;
&lt;br /&gt;There are commands to “PEEK” into queues.
&lt;br /&gt;
&lt;br /&gt;Q: Is queueing unordered, FIFO or prioritised?
&lt;br /&gt;
&lt;br /&gt;Best-effort FIFO-ish as explained.
&lt;br /&gt;
&lt;br /&gt;Q: Is there a broker or no broker?
&lt;br /&gt;
&lt;br /&gt;Broker as a set of masters. Clients can talk to whatever node they want.
&lt;br /&gt;
&lt;br /&gt;Q: Does the broker own independent, named queues (topics, routes etc) or do producers and consumers need to coordinate their connections?
&lt;br /&gt;
&lt;br /&gt;Named queues. Producers and consumers does not need to coordinate, since nodes use federation to discover routes inside the cluster and pass messages as they are needed by consumers. However the client is provided with hints in case it is willing to relocate where more consumers are.
&lt;br /&gt;
&lt;br /&gt;Q: Is message posting transactional?
&lt;br /&gt;
&lt;br /&gt;Yes, once the command to add a message returns, the system guarantees that there are the desired number of copies inside the cluster.
&lt;br /&gt;
&lt;br /&gt;Q: Is message receiving transactional?
&lt;br /&gt;
&lt;br /&gt;I guess not, since Disque will try to deliver the same message again if not acknowledged.
&lt;br /&gt;
&lt;br /&gt;Q: Do consumers block on receive or can they check for new messages?
&lt;br /&gt;
&lt;br /&gt;Both behaviors are supported, by default it blocks.
&lt;br /&gt;
&lt;br /&gt;Q: Do producers block on send or can they check for queue fullness? 
&lt;br /&gt;
&lt;br /&gt;The producer may ask to get an error when adding a new message if the message length is already greater than a specified value in the local node it is pushing the message.
&lt;br /&gt;
&lt;br /&gt;Moreover the producer may ask to replicate the message asynchronously if it want to run away ASAP and let the cluster replicate the message in a best-effort way.
&lt;br /&gt;
&lt;br /&gt;There is no way to block the consumer if there are too many messages in the queue, and unblock it as soon as there are less messages.
&lt;br /&gt;
&lt;br /&gt;Q: Are delayed jobs supported?
&lt;br /&gt;
&lt;br /&gt;Yes, with second granularity, up to years. However they’ll use memory.
&lt;br /&gt;
&lt;br /&gt;Q: Can consumers and producers connect to different nodes?
&lt;br /&gt;
&lt;br /&gt;Yes.
&lt;br /&gt;
&lt;br /&gt;I hope with this post Disque is a bit less vaporware. Sure, without looking at the code it is hard to tell, but if your best feature is out you can already complain at least.
&lt;br /&gt;How much of the above is already implemented and working well? Everything but AOF disk persistence, and a few minor things I want to refine in the API, so first release should not be too far, but working at it so rarely it is hard to get super fast.
&lt;a href=&quot;http://antirez.com/news/88&quot;&gt;Comments&lt;/a&gt;</content>
		<author>
			<name>Antirez</name>
			<uri>http://antirez.com</uri>
		</author>
		<source>
			<title type="html">Antirez weblog</title>
			<subtitle type="html">Description pending</subtitle>
			<link rel="self" href="http://antirez.com/rss"/>
			<id>http://antirez.com/rss</id>
			<updated>2015-06-08T06:00:08+00:00</updated>
		</source>
	</entry>

	<entry>
		<title type="html">Redis Conference 2015</title>
		<link href="http://antirez.com/news/87"/>
		<id>http://antirez.com/news/87</id>
		<updated>2015-06-03T01:40:05+00:00</updated>
		<content type="html">I’m back home, after a non easy trip, since to travel from San Francisco to Sicily is kinda NP complete: there are no solutions involving less than three flights. However it was definitely worth it, because the Redis Conference 2015 was very good, SF was wonderful as usually and I was able to meet with many interesting people. Here I’ll limit myself to writing a short account of the conference, but the trip was also an incredible experience because I discovered old and new friends, that are not just smart programmers, but also people I could imagine being my friends here in Sicily. I never felt alone while I was 10k kilometers away from my home.
&lt;br /&gt;
&lt;br /&gt;The conference was organized by RackSpace in a magistral way, with RedisLabs, Heroku, and Hulu, sponsoring it as well. I can’t say thank you enough times to everybody. Many people traveled from different parts of US and outside US to SF just for a couple of days, the venue was incredibly cool, and everything organized in the finest details.
&lt;br /&gt;
&lt;br /&gt;There was even an incredible cake for the Redis 6th birthday :-)
&lt;br /&gt;
&lt;br /&gt;However the killer features of the conference were, the number and the quality of the attenders (mostly actual Redis users), around 250 people, and the quality of the talks. The conference was free, even if it did not looked like a free conference at all, at any level. An incredible stage where to talk, very high quality food, plenty of space. All this honestly helped to create a setup for interesting exchanges. Everybody was using Redis for something, to get actual things done, and a lot of people shared their experiences. Among the talks I found Hulu and Heroku ones extremely interesting, because they covered details about different use cases and operational challenges. I also happen to agree with Bill Andersen (from RackSpace) vision on benchmarking Redis in a use-case oriented fashion, even if I missed the initial part of his talk because I was being interviewed, but the cool thing is, there will be recordings of the talks, so it will be possible for everybody to watch them when available at the conf site, which is, http://redisconference.com
&lt;br /&gt;
&lt;br /&gt;I was approached by several VeryLargeCompanies recounting stories of how they are using or are going to use Redis to do VeryLargeUseCase. Basically at this point Redis is everywhere.
&lt;br /&gt;
&lt;br /&gt;Redis Conference was a big gift to the Redis community… and in some way it shows very well how much there is a Redis outside Redis, I mean, at this point it has a life outside the borders of the server and client libraries repositories. It is a technology with many users that exchange ideas and that work with it in different ways: internally to companies to provide it as a technology to cover a number of use cases, and also in the context of cloud providers, that are providing it as a service to other companies.
&lt;br /&gt;
&lt;br /&gt;One thing I did not liked was Matt Stancliff talk. He tried to uncover different problems in the Redis development process, and finally proposed the community to replace me as the project leader, with him. In my opinion what Matt actually managed to do was to cherry-pick from my IRC, Twitter and Github issues posts in a very unfair way, in order to provide a bad imagine of myself. I think this was a big mistake. Moreover he did the talk as the last talk, not providing a right to reply. Matt and I happen to be persons with very different visions in many ways, however Redis is a project I invested many years into, and I’m not going to change my vision, I’m actually afraid I merged some code under pressure that I now find non well written and designed.
&lt;br /&gt;
&lt;br /&gt;What prevents Redis for becoming a monoculture is its license, if the community at some point really believes it is possible to do much better, or simply to do things in a very different way, some forks will appear, and darwinian selection will work to make sure we have the best Redis possible. Technical leadership is a reward for the work you are capable to do, is not asked at conferences. Moreover technology is not just code, is also about human interactions, and life is too short to interact with people we don’t share the same fundamental values of what a good behavior is.
&lt;br /&gt;
&lt;br /&gt;Well, even if this left some bitter taste, overall the Redis Conference was a magical experience, and even Matt talk actually helped me to understand what to do in the future and what I want for this project. Thank you to who made it possible and to all the attenders, I hope to see you again next year.
&lt;a href=&quot;http://antirez.com/news/87&quot;&gt;Comments&lt;/a&gt;</content>
		<author>
			<name>Antirez</name>
			<uri>http://antirez.com</uri>
		</author>
		<source>
			<title type="html">Antirez weblog</title>
			<subtitle type="html">Description pending</subtitle>
			<link rel="self" href="http://antirez.com/rss"/>
			<id>http://antirez.com/rss</id>
			<updated>2015-06-08T06:00:08+00:00</updated>
		</source>
	</entry>

	<entry>
		<title type="html">Side projects</title>
		<link href="http://antirez.com/news/86"/>
		<id>http://antirez.com/news/86</id>
		<updated>2015-06-03T01:40:05+00:00</updated>
		<content type="html">Today Redis is six years old. This is an incredible accomplishment for me, because in the past I switched to the next thing much faster. There are things that lasted six years in my past, but not like Redis, where after so much time, I still focus most of my everyday energies into.
&lt;br /&gt;
&lt;br /&gt;How did I stopped doing new things to focus into an unique effort, drastically monopolizing my professional life? It was a too big sacrifice to do, for an human being with a limited life span. Fortunately I simply never did this, I never stopped doing new things.
&lt;br /&gt;
&lt;br /&gt;If I look back at those 6 years, it was an endless stream of side projects, sometimes related to Redis, sometimes not.
&lt;br /&gt;
&lt;br /&gt;1) Load81, children programming environment.
&lt;br /&gt;2) Dump1090, software defined radio ADS-B decoder.
&lt;br /&gt;3) A Javascript ray tracer.
&lt;br /&gt;4) lua-cmsgpack, C implementation of msgpack for Lua.
&lt;br /&gt;5) linenoise line editing library. Used in Redis, but well, was not our top priority.
&lt;br /&gt;6) lamernews, Redis-based HN clone.
&lt;br /&gt;7) Gitan, a small Git web interface.
&lt;br /&gt;8) shapeme, images evolver using simulated annealing.
&lt;br /&gt;9) Disque, a distributed queue (work in progress right now).
&lt;br /&gt;
&lt;br /&gt;And there are much more throw-away projects not listed here.
&lt;br /&gt;The interesting thing is that many of the projects listed above are not random hacking efforts that had as an unique goal to make me happy. A few found their way into other people’s code.
&lt;br /&gt;
&lt;br /&gt;Because of the side projects, I was able to do different things when I was stressed and impoverished from doing again and again the same thing. I could later refocus on Redis, and find again the right motivations to have fun with it, because small projects are cool, but to work for years at a single project can provide more value for others in the long run.
&lt;br /&gt;
&lt;br /&gt;So currently I’m using something like 20% of my time to hack on Disque, a distributed message queue. So only 80% is left for Redis development, right? Wrong. The deal is between 80% of focus on Redis and 20% on something else, or 0% of focus on Redis in the long term, because in order to have a long term engagement, you need a long term alternative to explore new things.
&lt;br /&gt;
&lt;br /&gt;Side projects are the projects making your bigger projects possible. Moreover they are often the start of new interesting projects. Redis itself was a side project of LLOOGG. Sometimes you stop working at your main project because of side projects, but when this happens it is not because your side project captured your focus, it is because you managed to find a better use for your time, since the side project is more important, interesting, and compelling than the main project.
&lt;br /&gt;
&lt;br /&gt;Redis is six years old today, but is aging well: it continues to capture the attention of more developers, and it continues to improve in order to provide a bit more value to users every week. However for me, more users, more pull requests, and more pressure, does not mean to change my setup. What Redis is today is the sum of the work we put into it, and the endurance in the course of six years. To continue along the same path, I’ll make sure to have a few side projects for the next years.
&lt;br /&gt;
&lt;br /&gt;UPDATE: Damian Janowski provided an incredible present for the Redis community today, the renewed Redis.io web site is online now! http://redis.io. Thanks Damian!
&lt;br /&gt;
&lt;br /&gt;HN comments here: https://news.ycombinator.com/item?id=9112250
&lt;a href=&quot;http://antirez.com/news/86&quot;&gt;Comments&lt;/a&gt;</content>
		<author>
			<name>Antirez</name>
			<uri>http://antirez.com</uri>
		</author>
		<source>
			<title type="html">Antirez weblog</title>
			<subtitle type="html">Description pending</subtitle>
			<link rel="self" href="http://antirez.com/rss"/>
			<id>http://antirez.com/rss</id>
			<updated>2015-06-08T06:00:08+00:00</updated>
		</source>
	</entry>

	<entry>
		<title type="html">Why we don’t have benchmarks comparing Redis with other DBs</title>
		<link href="http://antirez.com/news/85"/>
		<id>http://antirez.com/news/85</id>
		<updated>2015-06-03T01:40:05+00:00</updated>
		<content type="html">Redis speed could be one selling point for new users, so following the trend of comparative “advertising” it should be logical to have a few comparisons at Redis.io. However there are two problems with this. One is of goals: I don’t want to convince developers to adopt Redis, we just do our best in order to provide a suitable product, and we are happy if people can get work done with it, that’s where my marketing wishes end. There is more: it is almost always impossible to compare different systems in a fair way.
&lt;br /&gt;
&lt;br /&gt;When you compare two databases, to get fair numbers, they need to share *a lot*: data model, exact durability guarantees, data replication safety, availability during partitions, and so forth: often a system will score in a lower way than another system since it sacrifices speed to provide less “hey look at me” qualities but that are very important nonetheless. Moreover the testing suite is a complex matter as well unless different database systems talk the same exact protocol: differences in the client library alone can contribute for large differences.
&lt;br /&gt;
&lt;br /&gt;However there are people that beg to differ, and believe comparing different database systems for speed is a good idea anyway. For example, yesterday a benchmark of Redis and AerospikeDB was published here: http://lynnlangit.com/2015/01/28/lessons-learned-benchmarking-nosql-on-the-aws-cloud-aerospikedb-and-redis/.
&lt;br /&gt;
&lt;br /&gt;I’ll use this benchmark to show my point about how benchmarks are misleading beasts. In the benchmark huge EC2 instances are used, for some strange reason, since the instances are equipped with 244 GB of RAM (!). Those are R3.8xlarge instances. For my tests I’ll use a more real world m3.medium instance.
&lt;br /&gt;
&lt;br /&gt;Using such a beast of an instance Redis scored, in the single node case, able to provide 128k ops per second. My EC2 instance is much more limited, testing from another EC2 instance with Redis benchmark, not using pipelining, and with the same 100 bytes data size, I get 32k ops/sec, so my instance is something like 4 times slower, in the single process case.
&lt;br /&gt;
&lt;br /&gt;Let’s see with Redis INFO command how the system is using the CPU during this benchmark:
&lt;br /&gt;
&lt;br /&gt;# CPU
&lt;br /&gt;used_cpu_sys:181.78
&lt;br /&gt;used_cpu_user:205.05
&lt;br /&gt;used_cpu_sys_children:0.12
&lt;br /&gt;used_cpu_user_children:0.87
&lt;br /&gt;127.0.0.1:6379&gt; info cpu
&lt;br /&gt;
&lt;br /&gt;… after 10 seconds of test …
&lt;br /&gt;
&lt;br /&gt;# CPU
&lt;br /&gt;used_cpu_sys:184.52
&lt;br /&gt;used_cpu_user:206.42
&lt;br /&gt;used_cpu_sys_children:0.12
&lt;br /&gt;used_cpu_user_children:0.87
&lt;br /&gt;
&lt;br /&gt;Redis spent ~ 3 seconds of system time, and only ~ 1.5 seconds in user space. What happens here is that for each request the biggest part of the work is to perform the read() and write() call. Also since it’s one-query one-reply workload for each client, we pay a full RTT for each request of each client.
&lt;br /&gt;
&lt;br /&gt;Now let’s check what happens if I use pipelining instead, a feature very known and much exploited by Redis users, since it’s the only way to maximize the server usage, and there are usually a number of places in the application where you can perform multiple operations at a given time.
&lt;br /&gt;
&lt;br /&gt;With a pipeline of 32 operations the numbers changed drastically. My tiny instance was able to deliver 250k ops/sec using a single core, which is 25% of the *top* result using 32 (each faster) cores in the mentioned benchmark.
&lt;br /&gt;
&lt;br /&gt;Let’s look at the CPU time:
&lt;br /&gt;
&lt;br /&gt;# CPU
&lt;br /&gt;used_cpu_sys:189.16
&lt;br /&gt;used_cpu_user:216.46
&lt;br /&gt;used_cpu_sys_children:0.12
&lt;br /&gt;used_cpu_user_children:0.87
&lt;br /&gt;127.0.0.1:6379&gt; info cpu
&lt;br /&gt;
&lt;br /&gt;… after 10 seconds of test …
&lt;br /&gt;
&lt;br /&gt;# CPU
&lt;br /&gt;used_cpu_sys:190.60
&lt;br /&gt;used_cpu_user:224.92
&lt;br /&gt;used_cpu_sys_children:0.12
&lt;br /&gt;used_cpu_user_children:0.87
&lt;br /&gt;
&lt;br /&gt;This time we are actually using the database engine to serve queries with our CPU, we are not just losing much of the time context switching. We used ~1.5 seconds of system time, and 8.46 seconds into the Redis process itself.
&lt;br /&gt;
&lt;br /&gt;Using lower numbers in the pipeline gets us results in the middle. Pipeline of 4 = 100k ops/sec (that should translate to ~ 400k ops/sec in the bigger instance used in the benchmark), pipeline 8 = 180k ops/sec, and so forth.
&lt;br /&gt;
&lt;br /&gt;So basically it is not a coincidence that benchmarking Redis and AerospikeDB in this way we get remarkably similar results. More or less you are not testing the databases, but the network stack and the kernel. If the DB can serve queries using a read and a write system call without any other huge waste, this is what we get, and here the time to serve the actual query is small since we are talking about data fitting into memory (just a note, 10M keys of 100k in Redis will use a fraction of the memory that was allocated in those instances).
&lt;br /&gt;
&lt;br /&gt;However there is more about that. What about the operations we can perform? To test Redis doing GET/SET is like to test a Ferrari checking how good it is at cleaning the mirror when it rains.
&lt;br /&gt;
&lt;br /&gt;A fundamental part of the Redis architecture is that largely different operations have similar costs, so what about our huge Facebook game posting scores of the finished games to create the leaderboard?
&lt;br /&gt;
&lt;br /&gt;The same single process can do 110k ops/sec when the query is: ZADD myscores  .
&lt;br /&gt;
&lt;br /&gt;But let’s demand more, what about estimating the cardinality with the HyperLogLog, at the same time adding new elements and reading the current guess with two redis-benchmark processes? Set size is 10 millions again. So during this test I spawned a benchmark executing PFADD with random elements of the set, and another doing PFCOUNT at the same time in the same HyperLogLog. Both scored simultaneously at 250k ops/sec, for a total of half a million ops per second with a single Redis process.
&lt;br /&gt;
&lt;br /&gt;In Redis doing complex operations is similar to pipelining. You want to do *more* for each read/write, otherwise your performance is dominated by I/O.
&lt;br /&gt;
&lt;br /&gt;Ok, so a few useful remarks. 1) GET/SET Benchmarks are not a great way to compare different database systems. 2) A better performance comparison is by use case. You say, for a given specific use case, using different data model, schema, queries, strategies, how much instances I need to handle the same traffic for the same app with two different database systems? 3) Test with instance types most people are going to actually use, huge instance types can mask inefficiencies of certain database systems, and is anyway not what most people are going to use.
&lt;br /&gt;
&lt;br /&gt;We’ll continue to optimize Redis for speed, and will continue to avoid posting comparative benchmarks.
&lt;br /&gt;
&lt;br /&gt;[Thanks to Amazon AWS for providing me free access to EC2]
&lt;a href=&quot;http://antirez.com/news/85&quot;&gt;Comments&lt;/a&gt;</content>
		<author>
			<name>Antirez</name>
			<uri>http://antirez.com</uri>
		</author>
		<source>
			<title type="html">Antirez weblog</title>
			<subtitle type="html">Description pending</subtitle>
			<link rel="self" href="http://antirez.com/rss"/>
			<id>http://antirez.com/rss</id>
			<updated>2015-06-08T06:00:08+00:00</updated>
		</source>
	</entry>

	<entry>
		<title type="html">Redis latency spikes and the Linux kernel: a few more details</title>
		<link href="http://antirez.com/news/84"/>
		<id>http://antirez.com/news/84</id>
		<updated>2015-06-03T01:40:05+00:00</updated>
		<content type="html">Today I was testing Redis latency using m3.medium EC2 instances. I was able to replicate the usual latency spikes during BGSAVE, when the process forks, and the child starts saving the dataset on disk. However something was not as expected. The spike did not happened because of disk I/O, nor during the fork() call itself.
&lt;br /&gt;
&lt;br /&gt;The test was performed with a 1GB of data in memory, with 150k writes per second originating from a different EC2 instance, targeting 5 million keys (evenly distributed). The pipeline was set to 4 commands. This translates to the following command line of redis-benchmark:
&lt;br /&gt;
&lt;br /&gt;    ./redis-benchmark -P 4 -t set -r 5000000 -n 1000000000
&lt;br /&gt;
&lt;br /&gt;Every time BGSAVE was triggered, I could see ~300 milliseconds latency spikes of unknown origin, since fork was taking 6 milliseconds. Fortunately Redis has a software watchdog feature, that is able to produce a stack trace of the process during a latency event. It’s quite a simple trick but works great: we setup a SIGALRM to be delivered by the kernel. Each time the serverCron() function is called, the scheduled signal is cleared, so actually Redis never receives it if the control returns fast enough to the Redis process. If instead there is a blocking condition, the signal is delivered by the kernel, and the signal handler prints the stack trace.
&lt;br /&gt;
&lt;br /&gt;Instead of getting stack traces with the fork call, the process was always blocked near MOV* operations happening in the context of the parent process just after the fork. I started to develop the theory that Linux was “lazy forking” in some way, and the actual heavy stuff was happening later when memory was accessed and pages had to be copy-on-write-ed.
&lt;br /&gt;
&lt;br /&gt;Next step was to read the fork() implementation of the Linux kernel. What the system call does is indeed to copy all the mapped regions (vm_area_struct structures). However a traditional implementation would also duplicate the PTEs at this point, and this was traditionally performed by copy_page_range(). However something changed… as an optimization years ago: now Linux does not just performs lazy page copying, as most modern kernels. The PTEs are also copied in a lazy way on faults. Here is the top comment of copy_range_range():
&lt;br /&gt;
&lt;br /&gt;         * Don't copy ptes where a page fault will fill them correctly.
&lt;br /&gt;         * Fork becomes much lighter when there are big shared or private
&lt;br /&gt;         * readonly mappings. The tradeoff is that copy_page_range is more
&lt;br /&gt;         * efficient than faulting.
&lt;br /&gt;
&lt;br /&gt;Basically as soon as the parent process performs an access in the shared regions with the child process, during the page fault Linux does the big amount of work skipped by fork, and this is why I could see always a MOV instruction in the stack trace.
&lt;br /&gt;
&lt;br /&gt;While this behavior is not good for Redis, since to copy all the PTEs in a single operation is more efficient, it is much better for the traditional use case of fork() on POSIX systems, which is, fork()+exec*() in order to spawn a new process.
&lt;br /&gt;
&lt;br /&gt;This issue is not EC2 specific, however virtualized instances are slower at copying PTEs, so the problem is less noticeable with physical servers.
&lt;br /&gt;
&lt;br /&gt;However this is definitely not the full story. While I was testing this stuff in my Linux box, I remembered that using the libc malloc, instead of jemalloc, in  certain conditions I was able to measure less latency spikes in the past. So I tried to check if there was some relation with that.
&lt;br /&gt;
&lt;br /&gt;Indeed compiling with MALLOC=libc I was not able to measure any latency in the physical server, while with jemalloc I could observe the same behavior observed with the EC2 instance. To understand better the difference I setup a test with 15 million keys and a larger pipeline in order to stress more the system and make more likely that page faults of all the mmaped regions could happen in a very small interval of time. Then I repeated the same test with jemalloc and libc malloc:
&lt;br /&gt;
&lt;br /&gt;bare metal, 675k/sec writes to 15 million keys, jemalloc: max spike 339 milliseconds.
&lt;br /&gt;bare metal, 675k/sec writes to 15 million keys, malloc: max spike 21 milliseconds.
&lt;br /&gt;
&lt;br /&gt;I quickly tried to replicate the same result with EC2, same stuff, the spike was a fraction with malloc.
&lt;br /&gt;
&lt;br /&gt;The next logical thing after this findings is to inspect what is the difference in the memory layout of a Redis system running with libc malloc VS one running with jemalloc. The Linux proc filesystem is handy to investigate the process internals (in this case I used /proc//smaps file).
&lt;br /&gt;
&lt;br /&gt;Jemalloc memory is allocated in this region:
&lt;br /&gt;
&lt;br /&gt;7f8002c00000-7f8062400000 rw-p 00000000 00:00 0
&lt;br /&gt;Size:            1564672 kB
&lt;br /&gt;Rss:             1564672 kB
&lt;br /&gt;Pss:             1564672 kB
&lt;br /&gt;Shared_Clean:          0 kB
&lt;br /&gt;Shared_Dirty:          0 kB
&lt;br /&gt;Private_Clean:         0 kB
&lt;br /&gt;Private_Dirty:   1564672 kB
&lt;br /&gt;Referenced:      1564672 kB
&lt;br /&gt;Anonymous:       1564672 kB
&lt;br /&gt;AnonHugePages:   1564672 kB
&lt;br /&gt;Swap:                  0 kB
&lt;br /&gt;KernelPageSize:        4 kB
&lt;br /&gt;MMUPageSize:           4 kB
&lt;br /&gt;Locked:                0 kB
&lt;br /&gt;VmFlags: rd wr mr mw me ac sd
&lt;br /&gt;
&lt;br /&gt;While libc big region looks like this:
&lt;br /&gt;
&lt;br /&gt;0082f000-8141c000 rw-p 00000000 00:00 0                                  [heap]
&lt;br /&gt;Size:            2109364 kB
&lt;br /&gt;Rss:             2109276 kB
&lt;br /&gt;Pss:             2109276 kB
&lt;br /&gt;Shared_Clean:          0 kB
&lt;br /&gt;Shared_Dirty:          0 kB
&lt;br /&gt;Private_Clean:         0 kB
&lt;br /&gt;Private_Dirty:   2109276 kB
&lt;br /&gt;Referenced:      2109276 kB
&lt;br /&gt;Anonymous:       2109276 kB
&lt;br /&gt;AnonHugePages:         0 kB
&lt;br /&gt;Swap:                  0 kB
&lt;br /&gt;KernelPageSize:        4 kB
&lt;br /&gt;MMUPageSize:           4 kB
&lt;br /&gt;Locked:                0 kB
&lt;br /&gt;VmFlags: rd wr mr mw me ac sd
&lt;br /&gt;
&lt;br /&gt;Looks like here there are a couple different things.
&lt;br /&gt;
&lt;br /&gt;1) There is [heap] in the first line only for libc malloc.
&lt;br /&gt;2) AnonHugePages field is zero for libc malloc but is set to the size of the region in the case of jemalloc.
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;Basically, the difference in latency appears to be due to the fact that malloc is using transparent huge pages, a kernel feature that allows to transparently glue multiple normal 4k pages into a few huge pages, which are 2048k each. This in turn means that copying the PTEs for this regions is much faster.
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;EDIT: Unfortunately I just spotted that I'm totally wrong, the huge pages apparently are only used by jemalloc: I just mis-read the outputs since this seemed so obvious. So on the contrary, it appears that the high latency is due to the huge pages thing for some unknown reason. So actually it is malloc that, while NOT using huge pages, is going much faster. I've no idea about what is happening here, so please disregard the above conclusions.
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;Meanwhile for low latency applications you may want to build Redis with “make MALLOC=libc”, however make sure to use “make distclean” before, and be aware that depending on the work load, libc malloc suffers fragmentation more than jemalloc.
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;More news soon…
&lt;br /&gt;
&lt;br /&gt;EDIT2: Oh wait... since the problem is huge pages, this is MUCH better, since we can disable it. And I just verified that it works:
&lt;br /&gt;
&lt;br /&gt;echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled
&lt;br /&gt;
&lt;br /&gt;This is the new Redis mantra apparently.
&lt;br /&gt;
&lt;br /&gt;UPDATE: While this seemed unrealistic to me, I experimentally verified that the huge pages memory spike is due to the fact that with 50 clients writing at the same time, with N queued requests each, the Redis process can touch in the space of *a single event loop iteration* all the process pages, so its copy-on-writing the entire process address space. This means that not only huge pages are horrible for latency, but that are also horrible for memory usage.
&lt;a href=&quot;http://antirez.com/news/84&quot;&gt;Comments&lt;/a&gt;</content>
		<author>
			<name>Antirez</name>
			<uri>http://antirez.com</uri>
		</author>
		<source>
			<title type="html">Antirez weblog</title>
			<subtitle type="html">Description pending</subtitle>
			<link rel="self" href="http://antirez.com/rss"/>
			<id>http://antirez.com/rss</id>
			<updated>2015-06-08T06:00:08+00:00</updated>
		</source>
	</entry>

	<entry xml:lang="en-US">
		<title type="html">6 Free Technical Classes From Pivotal Education</title>
		<link href="http://blog.pivotal.io/big-data-pivotal/features/6-free-technical-classes-from-pivotal-education"/>
		<id>http://blog.pivotal.io/?p=32930</id>
		<updated>2015-06-01T22:21:02+00:00</updated>
		<content type="html">&lt;p&gt;&lt;img class=&quot;alignleft wp-image-33049 size-full&quot; src=&quot;http://blog.pivotal.io/wp-content/uploads/2015/06/sfeatured-pivotal-education-free.png&quot; alt=&quot;sfeatured-pivotal-education-free&quot; width=&quot;440&quot; height=&quot;220&quot; /&gt;Pivotal Education makes it easy to fully realize the capabilities of our technologies by offering a series of free introductory training courses. Ideal for developers, system architects, and data practitioners, these online courses engage students through a hands-on, sandbox lab environment and provide a technical foundation for more advanced topics taught in other Pivotal courses. The offerings are also uniquely designed to enable technologists at any point of engagement with Pivotal—whether during evaluation or after deployment—to become more well-versed, efficient, and effective in their efforts.&lt;/p&gt;
&lt;p&gt;Our free, introductory training classes span across a wide array of Pivotal technologies, including:  &lt;a href=&quot;http://pivotal.io/big-data/pivotal-hd&quot; target=&quot;_blank&quot;&gt;Pivotal HD&lt;/a&gt;, &lt;a href=&quot;http://pivotal.io/platform-as-a-service/pivotal-cloud-foundry&quot; target=&quot;_blank&quot;&gt;Pivotal Cloud Foundry&lt;/a&gt;, &lt;a href=&quot;http://pivotal.io/big-data/pivotal-greenplum-database&quot; target=&quot;_blank&quot;&gt;Pivotal Greenplum Database&lt;/a&gt;, Pivotal &lt;a href=&quot;http://pivotal.io/big-data/pivotal-hawq&quot; target=&quot;_blank&quot;&gt;HAWQ&lt;/a&gt;, &lt;a href=&quot;http://redis.io/&quot; target=&quot;_blank&quot;&gt;Redis&lt;/a&gt;, and &lt;a href=&quot;http://pivotal.io/big-data/pivotal-gemfire&quot; target=&quot;_blank&quot;&gt;Pivotal GemFire&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://pivotal.io/training#introduction-to-greenplum-database&quot; target=&quot;_blank&quot;&gt;Intro to Pivotal Greenplum Database&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt;
This course is designed to provide an introduction to students new to Pivotal Greenplum Database (GPDB). As a one day course, students will gain a high level overview of features found in GPDB. This introduction will serve as a foundation for further courses that are offered for administrators, DBAs and end users of GPDB.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://pivotal.io/training#introduction-to-gemfire&quot; target=&quot;_blank&quot;&gt;Intro to GemFire&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt;
This course is designed to provide an introduction to students new to Pivotal GemFire. As a 1 day course, students will gain a high level overview of features found in GemFire. This introduction will serve as a foundation for further courses that are offered for administrator, developers and architects using GemFire. Some basic high level lab exercises are also offered to enhance the learning experience.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://pivotal.io/training#introduction-to-pivotal-hd&quot; target=&quot;_blank&quot;&gt;Intro to Pivotal HD&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt;
This course is designed to provide an introduction to students new to the advanced analytic capabilities of Pivotal HD, our &lt;a href=&quot;https://hadoop.apache.org/&quot; target=&quot;_blank&quot;&gt;Apache Hadoop®&lt;/a&gt; distribution. As a one day course, students will gain a high level overview of features found in the Pivotal HD. This introduction will serve as a foundation for further courses that are offered for administrators and architects using Pivotal HD.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://pivotal.io/training#ntroduction-to-pivotal-hd-and-hawq&quot; target=&quot;_blank&quot;&gt;Intro to Pivotal HAWQ&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt;
This course is designed to provide an introduction to students new to running SQL on Hadoop®. In this one day course, students will gain a high level overview of features found in HAWQ. This introduction will serve as a foundation for further courses that are offered for administrators, architects and end users of HAWQ.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://pivotal.io/training#enterprise-paas&quot; target=&quot;_blank&quot;&gt;Intro to Pivotal Cloud Foundry&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt;
This course provides students with the concepts and hands-on experience needed to work with and deploy applications on Pivotal Cloud Foundry. Students will gain familiarity in general Cloud Foundry concepts (applications, buildpacks, manifests, organizations, spaces, users, roles, domains, routes, services), how to push applications to Cloud Foundry in various languages, services, user provided services, manifests, YAML, environment variables, autoconfiguration, logging and loggregator.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://pivotal.io/training#introduction-to-redis&quot; target=&quot;_blank&quot;&gt;Intro to Redis (2 day course)&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt;
This hands-on course provides a broad overview of the Redis NoOSQL datastore, starting with an explanation of product functionality and typical use cases. Students will learn how to install and configure Redis, and examine some of the common commands for manipulating data. They will have the opportunity to create a simple Redis Java application, and review tips and techniques for improving performance. The course explains the use of persistence and replication in Redis, and also includes coverage of some administrative topics such as security and monitoring.&lt;/p&gt;
&lt;p&gt;Whether you are evaluating Pivotal’s products for your company, have already deployed these technologies, or are a student training looking to expand your skillset, Pivotal Education courses present an opportunity to become well-versed in these topics with minimal time commitment and no financial cost.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://pivotal.io/training&quot; target=&quot;_blank&quot;&gt;&lt;b&gt;Learn more about Pivotal’s Education and Training Courses&lt;/b&gt;&lt;/a&gt;&lt;/p&gt;</content>
		<author>
			<name>Pivotal</name>
			<uri>http://blog.pivotal.io</uri>
		</author>
		<source>
			<title type="html">Pivotal P.O.V. » Redis</title>
			<subtitle type="html">The Pivotal blog explores how people are harnessing sophisticated data fabrics and the cloud to build applications that achieve extraordinary things.</subtitle>
			<link rel="self" href="http://blog.pivotal.io/tag/redis/feed"/>
			<id>http://blog.pivotal.io/tag/redis/feed</id>
			<updated>2015-06-05T18:40:07+00:00</updated>
		</source>
	</entry>

	<entry xml:lang="en-US">
		<title type="html">Pivotal Data Roadshow: A View From the Road</title>
		<link href="http://blog.pivotal.io/big-data-pivotal/products/pivotal-data-roadshow-a-view-from-the-road"/>
		<id>http://blog.pivotal.io/?p=11883</id>
		<updated>2015-04-06T15:09:30+00:00</updated>
		<content type="html">&lt;p&gt;&lt;img class=&quot;alignleft size-full wp-image-11895&quot; src=&quot;http://blog.pivotal.io/wp-content/uploads/2015/04/featured-BigDataRoadshow.png&quot; alt=&quot;featured-BigDataRoadshow&quot; width=&quot;200&quot; height=&quot;200&quot; /&gt;People in the market are hungry for hands-on experience and training with big data technologies.  We are seeing this through the dramatic response to the &lt;a title=&quot;Pivotal Data Roadshow&quot; href=&quot;https://pivotal.io/big-data/data-roadshow&quot; target=&quot;_blank&quot;&gt;Pivotal Data Roadshow&lt;/a&gt;, which is touring cities across North America this month.  We have been packing the house with nearly 100 participants in each city.  We think this is a good gauge of just how much Apache HadoopⓇ and &lt;a title=&quot;SQL on Hadoop&quot; href=&quot;http://pivotal.io/big-data/hadoop/sql-on-hadoop&quot; target=&quot;_blank&quot;&gt;SQL on Hadoop&lt;/a&gt; have been embraced within the enterprise to handle big data storage and analytics workloads.&lt;/p&gt;
&lt;p&gt;Big Data technology continues to advance quickly. Just 18 months ago, Pivotal released the world’s first closed-loop platform for analytics and data-driven applications. While big data and &lt;a title=&quot;Apache HadoopⓇ&quot; href=&quot;http://hadoop.apache.org&quot; target=&quot;_blank&quot;&gt;Apache HadoopⓇ&lt;/a&gt; were moving from niche concerns to tech buzzword status, Pivotal was merging HadoopⓇ with &lt;a title=&quot;HAWQ&quot; href=&quot;http://pivotal.io/big-data/pivotal-hawq&quot; target=&quot;_blank&quot;&gt;HAWQ&lt;/a&gt;, a robust SQL-on-Hadoop engine, and with real time and &lt;a title=&quot;in-memory data processin&quot; href=&quot;http://pivotal.io/big-data/pivotal-gemfire&quot; target=&quot;_blank&quot;&gt;in-memory data processing&lt;/a&gt; to round out our vision of how companies should operationalize big data over a &lt;a title=&quot;Business Data Lake&quot; href=&quot;http://pivotal.io/big-data/businessdatalake&quot; target=&quot;_blank&quot;&gt;Business Data Lake&lt;/a&gt;. It is exciting for us to learn from the roadshows that enterprises are now eagerly embracing SQL-on-Hadoop and in-memory processing technologies to solve business problems, substantiating Pivotal’s strategy and investments over the past five years.&lt;/p&gt;
&lt;p&gt;Attendees of the Pivotal Data Roadshows have included executives, administrators and operations staff, developers, data scientists, and even college students. This variety demonstrates the enormous impact these technologies have on entire industries today and how important they are to each of their futures. Many people in our audiences have already been leveraging our Data Lake solutions as &lt;a title=&quot;Pivotal Big Data Suite&quot; href=&quot;http://pivotal.io/big-data/pivotal-big-data-suite&quot; target=&quot;_blank&quot;&gt;Pivotal Big Data Suite&lt;/a&gt; customers. These attendees have been impressed by &lt;a title=&quot;new customer-centric entitlements&quot; href=&quot;http://blog.pivotal.io/big-data-pivotal/news-2/pivotal-big-data-suite-open-agile-cloud-ready&quot; target=&quot;_blank&quot;&gt;new customer-centric entitlements&lt;/a&gt; within the Pivotal Big Data Suite, which includes an instance of &lt;a title=&quot;Pivotal Cloud Foundry&quot; href=&quot;http://pivotal.io/platform-as-a-service/pivotal-cloud-foundry&quot; target=&quot;_blank&quot;&gt;Pivotal Cloud Foundry&lt;/a&gt;, as well as powerful new additions for real time data ingestion and analysis via tools such as &lt;a title=&quot;Spring XD&quot; href=&quot;http://blog.pivotal.io/pivotal/products/spring-xd-for-real-time-analytics&quot; target=&quot;_blank&quot;&gt;Spring XD &lt;/a&gt;and &lt;a title=&quot;Redis&quot; href=&quot;http://blog.pivotal.io/pivotal/case-studies-2/using-redis-at-pinterest-for-billions-of-relationships&quot; target=&quot;_blank&quot;&gt;Redis&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone size-full wp-image-11897&quot; src=&quot;http://blog.pivotal.io/wp-content/uploads/2015/04/image011.png&quot; alt=&quot;image01&quot; width=&quot;615&quot; height=&quot;466&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Demonstrating another key component of &lt;a title=&quot;our 2015 strategy&quot; href=&quot;http://pivotal.io/new-approach-to-big-data&quot; target=&quot;_blank&quot;&gt;our 2015 strategy&lt;/a&gt;, the entire technical portion of the Pivotal Data Roadshow really highlights our robust cloud capabilities. All our attendee sessions are hosted in an enterprise public cloud infrastructure, spun up quickly and seamlessly in minutes.  This enables us to rapidly provision 50 IoT analytics labs across our entire product set. Attendees quickly initiate analytics workflows that ingest Twitter streams and execute real time analytics across our in memory database, Gemfire. Most impressively, this all gets developed in under 5 minutes, using only two Spring XD commands, replacing what used to be hours of complex and specialized programming.  Spring XD really has emerged as the star of the show, thanks to the streamlined, easy-to-understand interface it provides for our products, third party technologies, and its ability to manage either private, public, big, or fast data jobs.&lt;/p&gt;
&lt;p&gt;Attendees have stayed hours past the scheduled end time, enjoying building out new analytics pipelines with our expert engineering teams. They also left with their lab environments equipped to continue their experiences on their own cloud, learning into the future.&lt;/p&gt;
&lt;p&gt;If you’re interested in how big data and advanced analytics are changing companies, and would like to get your hands directly on the world’s best technology for leveraging them, be sure to check out one of the upcoming Pivotal Data Roadshows.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;alignnone size-full wp-image-11898&quot; src=&quot;http://blog.pivotal.io/wp-content/uploads/2015/04/image001.png&quot; alt=&quot;image00&quot; width=&quot;787&quot; height=&quot;273&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Upcoming Dates for the Pivotal Data Roadshow&lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Toronto, April 14&lt;/li&gt;
&lt;li&gt;New York City, April 16&lt;/li&gt;
&lt;li&gt;Washington, DC, April 21&lt;/li&gt;
&lt;li&gt;Atlanta, April 23&lt;/li&gt;
&lt;li&gt;Denver, April 28&lt;/li&gt;
&lt;li&gt;Dallas/Fort Worth, April 30&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;b&gt;Further information:&lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a title=&quot;Register for the Pivotal Data Roadshow&quot; href=&quot;https://pivotal.io/big-data/data-roadshow&quot; target=&quot;_blank&quot;&gt;Register for the Pivotal Data Roadshow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a title=&quot;Learn more about the Pivotal Big Data Suite&quot; href=&quot;http://pivotal.io/big-data/pivotal-big-data-suite&quot; target=&quot;_blank&quot;&gt;Learn more about the Pivotal Big Data Suite&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learn more about &lt;a title=&quot;Spring XD&quot; href=&quot;http://blog.pivotal.io/pivotal/products/spring-xd-for-real-time-analytics&quot; target=&quot;_blank&quot;&gt;Spring XD&lt;/a&gt; and &lt;a title=&quot;Redis&quot; href=&quot;http://blog.pivotal.io/pivotal/case-studies-2/using-redis-at-pinterest-for-billions-of-relationships&quot; target=&quot;_blank&quot;&gt;Redis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;i&gt;Editor’s Note: Apache, Apache Hadoop, Hadoop, and the yellow elephant logo are either registered trademarks or trademarks of the Apache Software Foundation in the United States and/or other countries.&lt;/i&gt;&lt;/p&gt;</content>
		<author>
			<name>Pivotal</name>
			<uri>http://blog.pivotal.io</uri>
		</author>
		<source>
			<title type="html">Pivotal P.O.V. » Redis</title>
			<subtitle type="html">The Pivotal blog explores how people are harnessing sophisticated data fabrics and the cloud to build applications that achieve extraordinary things.</subtitle>
			<link rel="self" href="http://blog.pivotal.io/tag/redis/feed"/>
			<id>http://blog.pivotal.io/tag/redis/feed</id>
			<updated>2015-06-05T18:40:07+00:00</updated>
		</source>
	</entry>

	<entry xml:lang="en-US">
		<title type="html">Mobile Video Big Data Architecture with Spring XD/Hadoop/HAWQ/Redis: Measuring Live Usage</title>
		<link href="http://blog.pivotal.io/pivotal/case-studies-2/mobile-video-big-data-architecture-with-spring-xdhadoophawqredis-measuring-live-usage"/>
		<id>http://blog.pivotal.io/?p=10844</id>
		<updated>2014-10-13T09:43:58+00:00</updated>
		<content type="html">&lt;p&gt;&lt;a href=&quot;http://blog.pivotal.io/wp-content/uploads/2014/10/featured-football-spectators-phone.jpg&quot;&gt;&lt;img class=&quot;alignleft size-full wp-image-10870&quot; src=&quot;http://blog.pivotal.io/wp-content/uploads/2014/10/featured-football-spectators-phone.jpg&quot; alt=&quot;featured-football-spectators-phone&quot; width=&quot;200&quot; height=&quot;200&quot; /&gt;&lt;/a&gt;Earlier this year, &lt;a title=&quot;mobile video usage overtook video usage on personal computers&quot; href=&quot;http://www.emarketer.com/Article/Tablets-Challenge-PCs-Leading-Digital-Video-Channel-US/1010807&quot; target=&quot;_blank&quot;&gt;mobile video usage overtook video usage on personal computers&lt;/a&gt; and research continues to show how important mobile video is to consumers. With these trends, mobile carriers and media companies are trying to learn more about usage to improve user experience and advertising revenues, especially with big advertising money makers like sports.&lt;/p&gt;
&lt;p&gt;Earlier this year, a major sports network and mobile carrier approached Pivotal to create a system for measuring cellular data and live video usage through mobile applications. The approach used &lt;a title=&quot;Spring XD&quot; href=&quot;http://projects.spring.io/spring-xd/&quot; target=&quot;_blank&quot;&gt;Spring XD&lt;/a&gt;, &lt;a title=&quot;Pivotal HD&quot; href=&quot;http://www.pivotal.io/big-data/pivotal-hd&quot; target=&quot;_blank&quot;&gt;Pivotal HD&lt;/a&gt;, &lt;a title=&quot;Redis&quot; href=&quot;http://www.pivotal.io/big-data/redis&quot; target=&quot;_blank&quot;&gt;Redis&lt;/a&gt;, and &lt;a title=&quot;Spring Boot&quot; href=&quot;http://projects.spring.io/spring-boot/&quot; target=&quot;_blank&quot;&gt;Spring Boot&lt;/a&gt; to quickly build a big data platform and set of analytical dashboards. The resulting analytics application is helping business executives from both companies understand how live video is used and what peak data usage looks like. The whole project only took a few resources and a few months.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Mobile/Media Big Data Architecture Overview&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;At a high level, data moves through a factory as depicted in the diagram below. Raw data is captured from mobile phones in JSON format by a Spring XD cluster where several processes are performed. The data is then stored in an HDFS cluster where Spring XD batch jobs use SQL via the HAWQ interface to HDFS and store the calculated reports in Redis. Spring Boot is then used with Angular.js and D3.js to show analytics to end users.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://blog.pivotal.io/wp-content/uploads/2014/10/ProLeague+Mobile-SolutionsArchitecture.png&quot;&gt;&lt;img class=&quot;alignnone size-full wp-image-10871&quot; src=&quot;http://blog.pivotal.io/wp-content/uploads/2014/10/ProLeague+Mobile-SolutionsArchitecture.png&quot; alt=&quot;ProLeague+Mobile-SolutionsArchitecture&quot; width=&quot;615&quot; height=&quot;239&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Capturing Raw Data from Mobile Devices in JSON&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;To produce the raw data, a mobile device is instrumented to capture the number of bytes used while viewing a video. The mobile application sends a “heartbeat” message that is formatted in JSON and contains the number of bytes consumed since the last report. Generally, the mobile app sends one of these reports every minute. Since there can be a large number of viewers during a live game, the system receives millions of heartbeat messages per hour. When designing the system, we decided to keep all of the heartbeat data over time, as with the concept of keeping all data inside a &lt;a title=&quot;data lake&quot; href=&quot;http://blog.pivotal.io/pivotal/news-2/from-data-silos-to-data-lakes-realizing-the-accessible-dream&quot; target=&quot;_blank&quot;&gt;data lake&lt;/a&gt;. This way, teams could use the data to produce additional analysis in the future. In addition, the customers had a quick timeline. They needed the system in place prior to the start of the next sports season.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Ingesting and Processing Data with Spring XD and HDFS&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;To handle the data ingestion, the Pivotal team utilized &lt;a title=&quot;Spring XD&quot; href=&quot;http://projects.spring.io/spring-xd/&quot; target=&quot;_blank&quot;&gt;Spring XD&lt;/a&gt; to receive and transform the raw data. The processed data was stored in an &lt;a title=&quot;Apache Hadoop&quot; href=&quot;http://hadoop.apache.org&quot; target=&quot;_blank&quot;&gt;Apache Hadoop®&lt;/a&gt; File System (HDFS) cluster as part of &lt;a title=&quot;Pivotal HD&quot; href=&quot;http://www.pivotal.io/big-data/pivotal-hd&quot; target=&quot;_blank&quot;&gt;Pivotal HD&lt;/a&gt;. Spring XD and HDFS provided a flexible, scalable solution to handle the massive amount of incoming JSON heartbeat data—as much as a half terabyte per season of sports.&lt;/p&gt;
&lt;p&gt;After the data was stored in HDFS, we used Pivotal HAWQ and SQL to generate the necessary reports from the data stored in HDFS. Spring XD’s batch job capability was leveraged to execute the reports after a game was done. The batch jobs produced JSON reports and stored them in &lt;a title=&quot;Redis&quot; href=&quot;http://blog.pivotal.io/tag/redis&quot; target=&quot;_blank&quot;&gt;Redis&lt;/a&gt;, providing immediate access to a web-based dashboard. Spring XD and Pivotal HD provided clustering and failover support to ensure business continuity in the event of a server failure.&lt;/p&gt;
&lt;p&gt;Spring XD turned out to be a perfect solution because it was designed with a distributed and extensible pipeline for ingesting data, processing it, performing real-time analytics on the received data, and then exporting it. Using Spring XD, we quickly implemented a system to receive the raw heartbeat data from the mobile applications. Spring XD provides several “sources” and “sinks” out of the box, and we were able to modify the existing HTTP source and HDFS sink to receive and store the raw heartbeat data. We also created a custom “processor” module that enabled us to filter and validate the raw data, ensuring that it was formatted as expected and had valid data in the expected JSON fields. Spring XD also provided a mechanism to “tap” data from one stream to another. Using a tap, it was simple for us to configure a stream to count the number of heartbeat packets received. This allowed us to ensure the system worked as expected—we could see the amount of live data going through the system in real-time. Since Spring XD supports a distributed runtime deployment, it also ensured that inbound data would continue to process in the event of any server failure.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Creating Analytical Reports with Spring XD, HAWQ, Redis, and Spring Boot&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;With HDFS storing the raw data, we used the &lt;a title=&quot;Pivotal Xtension Framework&quot; href=&quot;http://blog.pivotal.io/pivotal/products/json-on-hadoop-example-for-extending-hawq-data-formats-using-pivotal-extension-framework-pxf&quot; target=&quot;_blank&quot;&gt;Pivotal Xtension Framework&lt;/a&gt; (PXF) and &lt;a title=&quot;HAWQ&quot; href=&quot;http://blog.pivotal.io/tag/hawq&quot; target=&quot;_blank&quot;&gt;HAWQ&lt;/a&gt; to perform SQL queries directly against the JSON data. PXF maps the JSON to columns in a database, enabling HAWQ to execute SQL queries directly against the JSON data in HDFS. Using SQL, the product manager could then define the report queries.&lt;/p&gt;
&lt;p&gt;For the next step, we turned to Spring XD’s batch capabilities. Custom “jobs” were configured with the appropriate SQL queries to execute. The jobs are scheduled to execute early in the morning, after a game has occurred, and the report results are stored in Redis. Redis stores all the reports so that users can view any of them without needing to spend the time computing them on demand. Finally, we implemented a simple REST API, using &lt;a title=&quot;Spring Boot&quot; href=&quot;http://blog.pivotal.io/tag/spring-boot&quot; target=&quot;_blank&quot;&gt;Spring Boot&lt;/a&gt;, to serve the JSON reports to the user interface.&lt;/p&gt;
&lt;p&gt;A web-based dashboard was also built and allows users to easily view the weekly reports as they are available. Here, Spring Boot was used with JavaScript libraries like &lt;a title=&quot;Angular.js&quot; href=&quot;https://angularjs.org/&quot; target=&quot;_blank&quot;&gt;Angular.js&lt;/a&gt; and &lt;a title=&quot;D3.js&quot; href=&quot;http://d3js.org/&quot; target=&quot;_blank&quot;&gt;D3.js&lt;/a&gt;. Spring Boot and Spring Security were used to quickly secure the dashboard by adding a login page and requiring HTTPS for access.&lt;/p&gt;
&lt;p&gt;Together, Spring XD, Pivotal HD and HAWQ, Redis, and Spring Boot were used to quickly create a big data solution for massive ingesting, analyzing, and reporting on cellular data use for live, game-day videos. Now, the client can identify trends and optimize media revenue.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Learn More:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spring XD: &lt;a title=&quot;Project and Documentation&quot; href=&quot;http://projects.spring.io/spring-xd/&quot; target=&quot;_blank&quot;&gt;Project and Documentation&lt;/a&gt; | &lt;a title=&quot;Blog Articles&quot; href=&quot;http://blog.pivotal.io/tag/spring-xd&quot; target=&quot;_blank&quot;&gt;Blog Articles&lt;/a&gt; | &lt;a title=&quot;Using XD for Real Time Twitter&quot; href=&quot;http://blog.pivotal.io/pivotal/products/spring-xd-for-real-time-analytics&quot; target=&quot;_blank&quot;&gt;Using XD for Real Time Twitter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pivotal HD (SQL on HDFS via HAWQ): &lt;a title=&quot;Product&quot; href=&quot;http://www.pivotal.io/big-data/pivotal-hd&quot; target=&quot;_blank&quot;&gt;Product&lt;/a&gt; | &lt;a title=&quot;Documentation&quot; href=&quot;http://pivotalhd.docs.pivotal.io/doc/2010/index.html&quot; target=&quot;_blank&quot;&gt;Documentation&lt;/a&gt; | &lt;a title=&quot;Blog Articles&quot; href=&quot;http://blog.pivotal.io/tag/pivotal-hd&quot; target=&quot;_blank&quot;&gt;Blog Articles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Redis: &lt;a title=&quot;Website&quot; href=&quot;http://blog.pivotal.io/tag/redis/redis.io&quot; target=&quot;_blank&quot;&gt;Website&lt;/a&gt; | &lt;a title=&quot;Blog Articles&quot; href=&quot;http://blog.pivotal.io/tag/redis&quot; target=&quot;_blank&quot;&gt;Blog Articles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Spring Boot: &lt;a title=&quot;Project and Documentation&quot; href=&quot;http://projects.spring.io/spring-boot/&quot; target=&quot;_blank&quot;&gt;Project and Documentation&lt;/a&gt; | &lt;a title=&quot;Blog Articles&quot; href=&quot;http://blog.pivotal.io/tag/spring-boot&quot; target=&quot;_blank&quot;&gt;Blog Articles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pivotal Open Source: &lt;a title=&quot;Products and Projects&quot; href=&quot;http://www.pivotal.io/oss&quot; target=&quot;_blank&quot;&gt;Products and Projects&lt;/a&gt; | &lt;a title=&quot;Blog Articles&quot; href=&quot;http://blog.pivotal.io/tag/open-source&quot; target=&quot;_blank&quot;&gt;Blog Articles&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Editor&amp;#8217;s Note&lt;/strong&gt;: Apache, Apache Hadoop, Hadoop, and the yellow elephant logo are either registered trademarks or trademarks of the Apache Software Foundation in the United States and/or other countries.&lt;/em&gt;&lt;/p&gt;</content>
		<author>
			<name>Pivotal</name>
			<uri>http://blog.pivotal.io</uri>
		</author>
		<source>
			<title type="html">Pivotal P.O.V. » Redis</title>
			<subtitle type="html">The Pivotal blog explores how people are harnessing sophisticated data fabrics and the cloud to build applications that achieve extraordinary things.</subtitle>
			<link rel="self" href="http://blog.pivotal.io/tag/redis/feed"/>
			<id>http://blog.pivotal.io/tag/redis/feed</id>
			<updated>2015-06-05T18:40:07+00:00</updated>
		</source>
	</entry>

	<entry xml:lang="en-US">
		<title type="html">Apache Way at Pivotal</title>
		<link href="http://blog.pivotal.io/pivotal/news-2/apache-way-at-pivotal"/>
		<id>http://blog.pivotal.io/?p=10829</id>
		<updated>2014-10-08T16:27:06+00:00</updated>
		<content type="html">&lt;p&gt;&lt;img class=&quot;alignleft size-full wp-image-10839&quot; src=&quot;http://blog.pivotal.io/wp-content/uploads/2014/10/featured-feathers.png&quot; alt=&quot;featured-feathers&quot; width=&quot;200&quot; height=&quot;200&quot; /&gt;Ever since I joined Pivotal, one of my favorite topics of cocktail conversation has become explaining to folks unfamiliar with the company just how much of a &lt;a title=&quot;Open Source is Pivotal&quot; href=&quot;http://blog.pivotal.io/pivotal/products/open-source-is-pivotal&quot; target=&quot;_blank&quot;&gt;quintessential open source shop we are&lt;/a&gt;. Just rattling off the names of the projects and communities we stand behind feels like listing &amp;#8216;who&amp;#8217;s who&amp;#8217; of open source. Being a betting man as I am, I typically introduce Pivotal by making a $1 bet that whoever I&amp;#8217;m talking to is likely to have used software we contribute to at some point in their career. Given that this list includes &lt;a title=&quot;Cloud Foundry&quot; href=&quot;http://www.pivotal.io/platform-as-a-service/pivotal-cf&quot; target=&quot;_blank&quot;&gt;Cloud Foundry&lt;/a&gt;, &lt;a title=&quot;Spring Framework&quot; href=&quot;http://projects.spring.io/spring-framework/&quot; target=&quot;_blank&quot;&gt;Spring Framework&lt;/a&gt;, &lt;a title=&quot;Groovy&quot; href=&quot;http://groovy.codehaus.org/&quot; target=&quot;_blank&quot;&gt;Groovy&lt;/a&gt;, &lt;a title=&quot;Grails&quot; href=&quot;https://grails.org/&quot; target=&quot;_blank&quot;&gt;Grails&lt;/a&gt;, &lt;a title=&quot;Redis&quot; href=&quot;http://www.pivotal.io/big-data/redis&quot; target=&quot;_blank&quot;&gt;Redis&lt;/a&gt;, &lt;a title=&quot;RabbitMQ&quot; href=&quot;http://www.pivotal.io/products/pivotal-rabbitmq&quot; target=&quot;_blank&quot;&gt;RabbitMQ&lt;/a&gt;, &lt;a title=&quot;Tomcat&quot; href=&quot;http://tomcat.apache.org/&quot; target=&quot;_blank&quot;&gt;Tomcat&lt;/a&gt; and most recently &lt;a title=&quot;Apache Hadoop&quot; href=&quot;http://hadoop.apache.org&quot; target=&quot;_blank&quot;&gt;Apache Hadoop®&lt;/a&gt; the only folks I ever lose to are the poor souls still stuck on Windows 95 or mainframes. Paying $1 to them feels less like losing a bet, and more like a random act of kindness.&lt;/p&gt;
&lt;p&gt;As a side note, this incredible diversity of Pivotal&amp;#8217;s open source portfolio helps our business model a great deal. After all, the days when one could have a sizable stand-alone business built around a single open source project are as distant as dial-up internet access. There&amp;#8217;s a lot to be said on what a winning open-source strategy looks like (I may dare defining one in one of my future blog posts). Whatever it is, if you are an enterprise infrastructure vendor in 2014, you can&amp;#8217;t survive without having one. The market has clearly spoken in favor of enterprise infrastructure products built around open source projects. With this came a realization that the only sustainable way of doing so was to build long-term relationships with open source communities. This, of course, meant that business now found themselves thinking deep and hard about open source community governance.&lt;/p&gt;
&lt;p&gt;It is perhaps ironic, that the single most important detail of how open source communities operate doesn&amp;#8217;t get as much air time as licensing or patenting. Somehow there&amp;#8217;s a perception that open source communities don&amp;#8217;t even need much of governance. They are expected to self-organize into utopian worker collectives that could serve as a living illustration to &amp;#8220;from each according to his ability, to each according to his need&amp;#8221; principle. Nothing can be further from the truth. Each big, successful open source community is unique and typically very particular about the governance model. Open source communities, and by extension projects that they work on, live and die by how effective their governance model is. A great community can alway fix technology, a poor community is bound to ruin the best technology that is entrusted to it.&lt;/p&gt;
&lt;p&gt;Now, remember my earlier point about the sheer size of Pivotal&amp;#8217;s open source portfolio? Community governance is where it starts to matter yet again. Because you see, regardless of how much a runaway success Spring is (and standing at 8+ million users nobody would argue it isn&amp;#8217;t) the governance model that made it so successful is unique to it. It is true that Spring uses Apache License, but the similarities with the Apache Software Foundation (ASF) governance model ends there.&lt;/p&gt;
&lt;p&gt;ASF governance model, also known as &amp;#8220;The Apache Way&amp;#8221;, is a very unique, extremely well thought out approach to building successful open source communities. Communities that are capable of innovating on a diverse set of projects all living under the Foundation&amp;#8217;s umbrella. &amp;#8220;The Apache Way&amp;#8221; has been covered in great details over the years, its elevator pitch fits in just three words: &amp;#8220;Community over code&amp;#8221;. No really, that is it—everything else is an implementation detail.&lt;/p&gt;
&lt;p&gt;It must be said, that even before I joined Pivotal, the company has had a relationship with the ASF. We have been a constant sponsor of the Foundation and with guys like Mark Thomas and Bill Rowe working for Spring side of Pivotal &amp;#8220;The Apache Way&amp;#8221; was practiced quite well when it comes to Apache Tomcat and Apache HTTPD. At the same time, the scale at which Datafabrics side of Pivotal has embraced ASF projects around Apache Hadoop® ecosystem required a different level of focus on ASF.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.pivotal.io/careers&quot;&gt;&lt;img class=&quot;alignnone size-full wp-image-9320&quot; src=&quot;http://blog.pivotal.io/wp-content/uploads/2014/06/cta-hiring.png&quot; alt=&quot;cta-hiring&quot; width=&quot;600&quot; height=&quot;135&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So that was part of my mission when joining Pivotal: become a resident ASF guy for data technologies organization and &lt;em&gt;make sure that in a year&amp;#8217;s time Pivotal becomes a company that grows the pool of Apache Committers, not a company that just fights over the existing pool.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This project started with putting a framework in place that enables Pivotal engineers to collaborate on ASF software with minimum distraction (this required a close collaboration with our outstanding legal team). Our next step was to review our product roadmap and based on customer&amp;#8217;s feedback start focusing on a project that they needed. This is what our &lt;a title=&quot;Strengthening Apache Hadoop in the Enterprise with Apache Ambari&quot; href=&quot;http://blog.pivotal.io/pivotal/features/strengthening-hadoop-in-the-enterprise-with-apache-ambari&quot; target=&quot;_blank&quot;&gt;previous announcement around Apache® Ambari&lt;/a&gt; was all about.&lt;/p&gt;
&lt;p&gt;In the short couple of month we have not only contributed a &lt;a title=&quot;AMBARI-7292&quot; href=&quot;https://issues.apache.org/jira/browse/AMBARI-7292&quot; target=&quot;_blank&quot;&gt;wide&lt;/a&gt; &lt;a title=&quot;AMBARI-7551&quot; href=&quot;https://issues.apache.org/jira/browse/AMBARI-7551&quot; target=&quot;_blank&quot;&gt;variety&lt;/a&gt; of improvements to it, but also helped the project get more stable by working on&lt;a title=&quot;Ambari-trunk-Commit&quot; href=&quot;https://builds.apache.org/view/All/job/Ambari-trunk-Commit/&quot; target=&quot;_blank&quot;&gt; build &lt;/a&gt;and &lt;a title=&quot;AMBARI-7209&quot; href=&quot;https://issues.apache.org/jira/browse/AMBARI-7209&quot; target=&quot;_blank&quot;&gt;CI automation&lt;/a&gt; and suggesting &lt;a title=&quot;Proposal&quot; href=&quot;http://markmail.org/message/rs5j5j365txapalm&quot; target=&quot;_blank&quot;&gt;process improvements&lt;/a&gt; to innovate even quicker while not losing the sense of project stability. Pivotal&amp;#8217;s product has benefited from this relationship, but so did Apache® Ambari: it has become better and more featureful, but most importantly its community has grown stronger and more diverse. In fact, recognizing all the hard work that one of Pivotal&amp;#8217;s engineers Jun Aoki has put into the project, the Apache® Ambari PMC has &lt;a title=&quot;Jun invited to Ambari&quot; href=&quot;http://markmail.org/message/dwfwtercyj3s6moq&quot; target=&quot;_blank&quot;&gt;voted&lt;/a&gt; Jun in as a committer.&lt;/p&gt;
&lt;p&gt;This may be a small step for Jun (and, to be sure, the first one in many other ASF projects that he is going to join), but it actually signifies a giant leap for Pivotal and the kind of role it is assuming in ASF from now on. We&amp;#8217;re already delivering on that fundamental vision of making sure that our engineers get fully empowered to lead most impactful open source projects within the Foundation. If you are an engineer dreaming to add the coveted Committer/PMC of Apache X to your resume, join us. We can&amp;#8217;t guarantee that you will get that line (after all, ASF recognizes your personal merit and your personal merit alone) but we can guarantee that we will move heaven and earth to empower you to do so.&lt;/p&gt;
&lt;p&gt;After all a stronger Apache Software Foundation (ASF) will benefit the entire industry. It is, you see, that proverbial tide that lifts all boats big and small.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Editor&amp;#8217;s Note&lt;/strong&gt;: Apache, Apache Hadoop, Hadoop, and the yellow elephant logo are either registered trademarks or trademarks of the Apache Software Foundation in the United States and/or other countries.&lt;/em&gt;&lt;/p&gt;</content>
		<author>
			<name>Pivotal</name>
			<uri>http://blog.pivotal.io</uri>
		</author>
		<source>
			<title type="html">Pivotal P.O.V. » Redis</title>
			<subtitle type="html">The Pivotal blog explores how people are harnessing sophisticated data fabrics and the cloud to build applications that achieve extraordinary things.</subtitle>
			<link rel="self" href="http://blog.pivotal.io/tag/redis/feed"/>
			<id>http://blog.pivotal.io/tag/redis/feed</id>
			<updated>2015-06-05T18:40:07+00:00</updated>
		</source>
	</entry>

	<entry xml:lang="en-US">
		<title type="html">Case Study: How shrebo, the Sharing Economy Platform, Innovates with Cloud Foundry</title>
		<link href="http://blog.pivotal.io/pivotal-cloud-foundry/case-studies-2/case-study-how-shrebo-the-sharing-economy-platform-innovates-with-cloud-foundry"/>
		<id>http://blog.pivotal.io/?p=10435</id>
		<updated>2014-09-03T17:30:43+00:00</updated>
		<content type="html">&lt;p&gt;&lt;a href=&quot;http://blog.pivotal.io/wp-content/uploads/2014/09/featured-time-to-innovate.png&quot;&gt;&lt;img class=&quot;alignleft size-full wp-image-10437&quot; src=&quot;http://blog.pivotal.io/wp-content/uploads/2014/09/featured-time-to-innovate.png&quot; alt=&quot;featured-time-to-innovate&quot; width=&quot;200&quot; height=&quot;200&quot; /&gt;&lt;/a&gt;In this post, former banking technology architect and current CTO, Patrick Senti, allows us to pick his brain and learn about his experiences with Cloud Foundry and Python/Django.&lt;/p&gt;
&lt;p&gt;As background, Senti is an award-winning architect and programmer who is also certified in machine learning. He brings over 20 years of software experience from working on high profile projects at companies like IBM, SAS, and Credit Suisse.&lt;/p&gt;
&lt;p&gt;Recently Senti founded &lt;a title=&quot;shrebo&quot; href=&quot;https://www.shrebo.com/lang/?lang=en&amp;next=/business/&quot; target=&quot;_blank&quot;&gt;shrebo&lt;/a&gt; and is currently CTO. Much like Uber, AirBnB, and Zipcar, his company uses cloud platforms to create new ways of sharing resources.  shrebo recently won an innovation award from Swiss Federal Railways (who is also a customer) and provides an entire web-services-based SaaS platform—&lt;a title=&quot;a set of APIs&quot; href=&quot;https://www.shrebo.com/lang/?lang=en&amp;next=/developer/&quot; target=&quot;_blank&quot;&gt;a set of APIs&lt;/a&gt; for application developers to build with. Shrebo is the cloud platform for the sharing economy—allowing you to build your own AirBnB or Lyft on shrebo.&lt;/p&gt;
&lt;p&gt;The functionality includes search, booking, scheduling, provisioning, payment, and social functionality endpoints surrounding the entire process of sharing resources. With their services, third parties can cost effectively and quickly create sharing applications in mobile, social, local and other contexts to help automate logistics, optimize machinery, increase asset and fleet utilization, and run services for sharing any type of resource like cars, bikes, or parking spaces.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In three short sentences, what defined your needs for a platform like Cloud Foundry?&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Architecting a start-up with a big vision means we needed stability at the ground level.&lt;/li&gt;
&lt;li&gt;We need developers to focus on solving problems and coding instead of technical details.&lt;/li&gt;
&lt;li&gt;While stacks like Amazon were interesting, we needed to be higher up the stack to make service distribution easier.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;If you could summarize, what would you say the outcomes have been so far?&lt;/strong&gt;&lt;br /&gt;
This PaaS platform reduces risk and creates a lot of efficiency and effectiveness. &lt;a title=&quot;Cloud Foundry&quot; href=&quot;http://www.pivotal.io/platform-as-a-service/pivotal-cf&quot; target=&quot;_blank&quot;&gt;Cloud Foundry&lt;/a&gt; is a real blast of an experience as a developer. We essentially package the application along with the services manifest and press a button. Everything else is done by the platform. This type of thing used to take days or weeks. Now, it takes minutes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;OK, let’s cover some background. How did you, as an entrepreneur, decide to start this company?&lt;/strong&gt;&lt;br /&gt;
Well, it was really three ideas that came together.&lt;/p&gt;
&lt;p&gt;A few years back, I shared a sailboat with other people. It was always difficult to manage the logistics because smartphones were not as common. However, the idea of making the process easier…that stuck with me. I thought, “It would be really useful to have a sharing service for any private resource.”&lt;/p&gt;
&lt;p&gt;Two, it’s easy to see how more and more resources are shared between people across companies. If you work at the same company, everyone is on a shared calendar; so, it’s not needed. When people come together across companies, like in a start-up environment or if you share some large asset like trains, you need a better way to share. It seemed like a good idea to have an API-based platform to help build solutions like this.&lt;/p&gt;
&lt;p&gt;Lastly, I have done a lot with analytics. I realized there could be process improvement and optimization if people and companies could successfully share information and results. For example, we cooperate with Swiss Federal Railways, who awarded us a winner for innovation as a start-up, and they operate tons of trains and cars each day. The trains are very overused in many city areas. When they are full, commuters don’t know where to sit. In fact, this is a sharable resource. So, my “aha” moment here was about connecting people via a smartphone app—we could allow them to see and chose travel based on load and occupancy. They could make a conscious decision to board a certain train. We could really help improve this if we use predictive analytics. So, we do that too—this is much like managing meeting rooms but on a really large scale.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Could you tell us more about shrebo as a business and how Pivotal and Cloud Foundry fit?&lt;/strong&gt;&lt;br /&gt;
In a nutshell, companies use our stuff to build their own end-user sharing applications.&lt;/p&gt;
&lt;p&gt;We are a platform software services for any sharing solution. If you are familiar with car sharing, like a renting process or a smartphone app for scheduling a car like &lt;a title=&quot;Zipcar&quot; href=&quot;http://www.zipcar.com/&quot; target=&quot;_blank&quot;&gt;Zipcar&lt;/a&gt; or &lt;a title=&quot;Uber&quot; href=&quot;https://www.uber.com/&quot; target=&quot;_blank&quot;&gt;Uber&lt;/a&gt; in the U.S., our platform supports all the functionality you would need to build such an app through a set of developer APIs. The shrebo platform allows companies of any size to more easily and cost effectively &lt;a title=&quot;develop their own sharing applications&quot; href=&quot;https://www.shrebo.com/lang/?lang=en&amp;next=/developer/&quot; target=&quot;_blank&quot;&gt;develop their own sharing applications&lt;/a&gt; and online services and mix them with other mobile, social, and local apps.&lt;/p&gt;
&lt;p&gt;In our initial architecture process, we evaluated several PaaS providers. Ultimately, we chose &lt;a title=&quot;App Fog&quot; href=&quot;https://www.appfog.com/&quot; target=&quot;_blank&quot;&gt;App Fog&lt;/a&gt;, who is now &lt;a title=&quot;owned by CenturyLink&quot; href=&quot;http://blog.pivotal.io/cloud-foundry-pivotal/news-2/pivotal-moves-to-establish-open-governance-model-for-cloud-foundry&quot; target=&quot;_blank&quot;&gt;owned by CenturyLink&lt;/a&gt;. They run &lt;a title=&quot;Cloud Foundry&quot; href=&quot;http://cloudfoundry.org/&quot; target=&quot;_blank&quot;&gt;Cloud Foundry&lt;/a&gt; with Python, Java, Node.js, PHP, and Ruby. Our technology framework allows us to elastically provision services through App Fog’s services API. Also, Pivotal provides us with Redis as a data structure service and RabbitMQ as a message broker service.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How will your customer experience be improved through our technologies?&lt;/strong&gt;&lt;br /&gt;
Well, our app is not directly visible through some user interface for end users, it is more of a white label solution. However, we provide app and data services that enable others to provide end-user applications. So, those companies can rely on our API to meet SLAs cost effectively. Eventually, they need their apps to perform or users will go somewhere else. If we fail, so do they.&lt;/p&gt;
&lt;p&gt;In the context of us powering other technologies, we use Cloud Foundry and AppFog to provision services much faster, at a lower cost, and with greater scale as real-time demand shifts. This PaaS is certainly more cost effective than Amazon, and it’s less complex and risky than traditional hosting providers who make us build the stack by ourselves. It’s also very automated. All of these things allow us to deliver code, deploy updates, and fix issues on our platform quickly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Do you have any examples where Cloud Foundry’s Elastic Runtime Service delivered something unexpected?&lt;/strong&gt;&lt;br /&gt;
Yes, absolutely. At one point in time, the AppFog infrastructure needed an upgrade. They had some planned downtime and planned to make changes to one data center at a time across Singapore, Ireland, and the U.S. With Cloud Foundry, we had zero down time even though they did. As they were bringing down a data center to make updates, we set up new instances on the fly and shifted workloads. As they moved around the world, we automatically migrated across data centers. We didn’t have any downtime even though they did! Before Cloud Foundry, this was really not possible. We could increase and decrease instances on the fly. Quite amazing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How does Cloud Foundry compare to other platforms you have worked with?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Provisioning is so different. So, I spent a lot of time in the financial services industry—for about 10 years. We primarily worked with traditional hosting platforms. You get a Linux machine, then you do an install. Along the process, a group of experts all touch it before it is ready to deploy code. Cloud Foundry it totally different.&lt;/p&gt;
&lt;p&gt;Like I said before, Cloud Foundry is a real blast of an experience as a developer. This type of thing used to take days or weeks. Now, it takes minutes. As a CTO, this is the experience every developer has been looking for over the past 10 years. We get the power of scripting—an event can trigger just about anything we need.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What other overarching Cloud Foundry capabilities are top of mind?&lt;/strong&gt;&lt;br /&gt;
Flexibility and agility are top of the list. I see extensibility as a major benefit too. The whole architecture is built with open source contributions in mind. Adding new stacks is encouraged.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How does Cloud Foundry and AppFog support your stack?&lt;/strong&gt;&lt;br /&gt;
Cloud Foundry is the PaaS and AppFog integrates and operates the PaaS and IaaS. They also provide a Django/Python stack on Cloud Foundry. We didn’t have to do anything to get this set up. It was provided when we turned on our account. This is the way development should work!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Could you tell us a bit about the other services you use on Cloud Foundry?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We use &lt;a title=&quot;Redis&quot; href=&quot;http://docs.pivotal.io/p1-services/Redis.html&quot; target=&quot;_blank&quot;&gt;Redis&lt;/a&gt; and &lt;a title=&quot;RabbitMQ&quot; href=&quot;http://docs.pivotal.io/rabbitmq-cf/index.html&quot; target=&quot;_blank&quot;&gt;RabbitMQ&lt;/a&gt;—these are both very powerful services for data structure and messaging middleware, and we have two main use cases. We use MySQL for persistence, also provisioned and managed by Cloud Foundry.&lt;/p&gt;
&lt;p&gt;First, we provide elements of a web app and APIs as a service to the outside world. The runtime is a traditional Python process intended for short-lived, synchronous workloads. Second, there are many asynchronous processes that trigger over time. For example, the website generates a lot of events. If someone is booking a resource, confirming a booking, or cancels, these are all events. Workflow workloads run asynchronously in the background and do various things like send an SMS or email. There is a separation of concerns here between the two types of workloads, short-lived web requests, and longer-lived  background tasks, and we use RabbitMQ to decouple these sub-systems, and it is used on both sides.&lt;/p&gt;
&lt;p&gt;Our RabbitMQ uses Redis as an in-memory data store to transport the messages between the two areas—the web APIs and the messaging, which is based on &lt;a title=&quot;Celery&quot; href=&quot;http://celery.readthedocs.org/en/latest/getting-started/brokers/rabbitmq.html&quot; target=&quot;_blank&quot;&gt;Celery&lt;/a&gt; as well. In addition, we use Redis as a distributed log instance—as soon as a reservation is made, we want to ensure no one else can grab the resource. Since we have four or five instances on the web API side, we run into a distributed lock problem. Redis solves this problem for us. So, RabbitMQ is the broker, Redis powers the state of notification events, and it also acts as a data store to log transaction events—for all events and messages across the two workloads.&lt;/p&gt;
&lt;p&gt;The workflows, events, and messages store data in an analytics warehouse. At this point, we are using &lt;a title=&quot;Apache Hadoop&quot; href=&quot;http://hadoop.apache.org&quot; target=&quot;_blank&quot;&gt;Apache Hadoop®&lt;/a&gt;, but we haven’t deployed it at a large scale quite yet. We are also looking at Storm.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.pivotal.io/sites/default/files/documents/shrebo_Case%20Study_082914_0.pdf&quot;&gt;&lt;img class=&quot;alignnone size-full wp-image-10832&quot; src=&quot;http://blog.pivotal.io/wp-content/uploads/2014/10/cta-casestudy-shrebo.png&quot; alt=&quot;cta-casestudy-shrebo&quot; width=&quot;600&quot; height=&quot;135&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;While you are a CTO, you were a developer at one point and probably get your hands dirty at times. What makes Cloud Foundry so cool for developers?&lt;/strong&gt;&lt;br /&gt;
Cloud Foundry has a command line interface, and AppFog has built an API. As I mentioned earlier, this gives developers the power of scripting. We spend more time coding and less time dealing with the foundational platform. So, all deployment tasks—infrastructure, new instances, deploying code, unit testing, integration testing, screen tests—all of this can be scripted. We push a button to trigger these processes. While this is possible with any modern infrastructure, Cloud Foundry really gives us the ability to stay at a high level in the stack and avoid getting into details.&lt;/p&gt;
&lt;p&gt;We also really like the separation between applications and code. When we write the deployment manifest and need services like &lt;a title=&quot;MySQL&quot; href=&quot;http://docs.pivotal.io/p-mysql/index.html&quot; target=&quot;_blank&quot;&gt;MySQL&lt;/a&gt; or &lt;a title=&quot;Redis&quot; href=&quot;http://docs.pivotal.io/p1-services/Redis.html&quot; target=&quot;_blank&quot;&gt;Redis&lt;/a&gt;, we just tell the system we need an instance of it in a few lines of configuration without having to worry about the config of the underlying service. The underlying stuff is all handled by the platform.&lt;/p&gt;
&lt;p&gt;We have gotten off the ground very quickly. In fact, Facebook has this concept of hiring developers to deploy code in the same day they started. Now, we have the same type of environment—one of our new developers can deploy code the same day they start with shrebo. This is one of the most exciting facts about Cloud Foundry—we can have a small team of 4 people work on the app and run the infrastructure, and we can scale people this way too. Five years ago, it would have required 10 or more people to run this same infrastructure and do development.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In terms of monitoring, are you using any of Cloud Foundry’s built in capabilities?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Well, we aren’t using the Cloud Foundry health check services API directly. We decided to use New Relic as the application-level performance metrics monitor. From outside, we use Pingdom to verify uptimes. New Relic as extremely easy to plug into Cloud Foundry applications—they have a Python extension that works out of the box with Cloud Foundry. As described we use RabbitMQ to do app-level event processing, but we also use it to send New Relic custom events to their services infrastructure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;As a CTO/CIO level person with a financial services background, you have a clear handle on ROI, C-level metrics, and risk management. What about Cloud Foundry is powerful from this perspective?&lt;/strong&gt;&lt;br /&gt;
If I talk about this from my banking background, I can easily see how Cloud Foundry produces efficiency and effectiveness—this helps reduce costs and risk. Let me explain.&lt;/p&gt;
&lt;p&gt;When we release code &lt;em&gt;without &lt;/em&gt;Cloud Foundry, it might take 4-6 months at a traditional bank with more complexity and legacy systems. With Cloud Foundry, you can reduce that time to days or weeks. There is an order of magnitude level of improvement in terms of these process metrics. There are fewer people involved, it is easier for developers to fix problems, we can scale dynamically without emailing anyone—this all reduces cost and time, and as a result increases the productivity of the whole organization&lt;/p&gt;
&lt;p&gt;In terms of risk, Cloud Foundry clearly reduces operational risk. This is largely related to the lower number of people that need to be involved. For example, if an app shows signs of crashing, we don’t have to involve a team to resolve. A developer can fix the code, and an automated, daily build can update the problems or a button can redeploy without five people having to coordinate on a conference call or group chat. As well, I’ve described how the scale and uptime scenarios work—this reduces downtime and ensures SLA support. This is particularly important for mission-critical, revenue generating apps. So, Cloud Foundry has a direct impact on the risk of our top and bottom line.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Would you mind sharing an architecture slide on your stack?&lt;/strong&gt;&lt;br /&gt;
Sure, here see below. We use HTML5/Bootstrap, JSON/REST, Redis, MongoDB, RabbitMQ, MySQL, and Python/Django for the online services or real-time runtime. There is some overlap here with our batch or periodic workloads. This part of the architecture is another conversation all together. However, we use R, Elastic Search, Storm, Apache Hadoop®, and much more. The infrastructure includes Nginx, Varnish, Gunicorn, Cloud Foundry, Cloudinary, Searchly, Amazon AWS, Mailgun, and GlobalSMS.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thank you so much, is there anything you’d like to add from a personal perspective? What do you like to do in your off time? Anything you’d like to accomplish before you leave this little rock we call Earth?&lt;/strong&gt;&lt;br /&gt;
Sure. In my spare time I like to spend time with friends and family. I also very much enjoy to ride the bike in the woods, or sail that very boat that we used to share, which is based on a small lake in Switzerland. In extension of this, I plan to do a lot more sailing at sea, in fact one of my dreams is to become a certified skipper for maritime sailing yachts.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Editor&amp;#8217;s Note&lt;/strong&gt;: Apache, Apache Hadoop, Hadoop, and the yellow elephant logo are either registered trademarks or trademarks of the Apache Software Foundation in the United States and/or other countries.&lt;/em&gt;&lt;/p&gt;</content>
		<author>
			<name>Pivotal</name>
			<uri>http://blog.pivotal.io</uri>
		</author>
		<source>
			<title type="html">Pivotal P.O.V. » Redis</title>
			<subtitle type="html">The Pivotal blog explores how people are harnessing sophisticated data fabrics and the cloud to build applications that achieve extraordinary things.</subtitle>
			<link rel="self" href="http://blog.pivotal.io/tag/redis/feed"/>
			<id>http://blog.pivotal.io/tag/redis/feed</id>
			<updated>2015-06-05T18:40:07+00:00</updated>
		</source>
	</entry>

</feed>
