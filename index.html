<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en">
<head>
	<title>Community MongoDB Aggregator</title>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<meta name="robots" content="index,follow" />
	<link rel="stylesheet" type="text/css" href="css/style.css" />
	<link rel="alternate" type="application/rss+xml" href="http://www.planetmongo.org/rss20.xml" title="Community MongoDB Aggregator RSS" />
	<script src="/css/yui-combined.min.js"></script>
	<script src="/css/core.js"></script>
	<link rel="stylesheet" type="text/css" media="all" href="//fonts.googleapis.com/css?family=Ubuntu:300,400,400italic" />
</head>

<body class="blog">
<div id="wrapper" class="hfeed">

<div id="header">

<div id="orangeHeader">
	<h1>
		<a href="http://www.planetmongo.org/" title="Community MongoDB Aggregator"><span>Community MongoDB Aggregator</span></a>
	</h1>
</div>

</div>

	<div id="main">
		<div id="container">
			<div id="content">



	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.xaprb.com/blog/2015/05/22/percona-mongodb-mysql-history-repeat/" title="">History Repeats: MySQL, MongoDB, Percona, and Open Source</a></h2>

<h3>Baron Schwartz</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<p>History is repeating again. MongoDB is breaking out of the niche into the
mainstream, performance and instrumentation are terrible in specific cases,
MongoDB isn&rsquo;t able to fix all the problems alone, and an ecosystem is growing.</p>

<p><img src="http://www.xaprb.com/media/2015/05/leaf.jpg" alt="Leaf" />
</p>

<p>This should really be a series of blog posts, because there&rsquo;s a book&rsquo;s worth of
things happening, but I&rsquo;ll summarize instead. Randomly ordered:</p>

<ul>
<li>MongoDB is in many respects closely following MySQL&rsquo;s development, 10 years
offset. Single index per query, MyISAM-like storage engine, etc.
<a href="http://www.xaprb.com/blog/2013/04/29/what-tokudb-might-mean-for-mongodb/">Background</a>.</li>
<li>Tokutek built an excellent transactional storage engine and replaced
MongoDB&rsquo;s, calling it TokuMX. Results were dramatically better performance
(plus ACID). MongoDB&rsquo;s response was to buy WiredTiger and make it the default
storage engine in MongoDB 3.0.</li>
<li>Percona acquired Tokutek. A book should be written about this someday. The
impact to both the MySQL and MongoDB communities cannot be overstated. This
changes everything. It also changes everything for Percona, which now has a
truly differentiated product for both database offerings. This moves them
solidly into being a product company, not just support/services/consulting; it
is a good answer to the quandary of trying to keep up with the InnoDB
engineers.</li>
<li>Facebook acquired Parse, which is probably one of the larger MongoDB
installations.</li>
<li>Facebook&rsquo;s Mark Callaghan, among others, stopped spending all his time on
InnoDB mutexes and so forth. For the last year or so he&rsquo;s been extremely
active in the MongoDB community. The MongoDB community is lucky to have a
genius of Mark&rsquo;s caliber finding and solving problems. There are others, but
if Mark Callaghan is working on your open source product in earnest, you&rsquo;ve
arrived.</li>
<li>Just as in MySQL, but even earlier, there are lots of -As-A-Service providers
for MongoDB, and it&rsquo;s likely a significant portion of future growth happens
here.</li>
<li>MongoDB&rsquo;s conference is jaw-droppingly expensive for a vendor, to the point of
being exclusive. At the same time, MongoDB hasn&rsquo;t quite recognized and
embraced some of the things going on outside their walls. If you remember <a href="https://www.percona.com/blog/2009/02/05/announcing-percona-performance-conference-2009-on-april-22-23/">the
events of 2009 in the MySQL
community</a>,
Percona&rsquo;s <a href="https://www.percona.com/news-and-events/mongodb-events/mongodb-community-openhouse">announcement of an alternative MongoDB
conference</a>
might feel a little like deja vu. I&rsquo;m not sure of the backstory behind this,
though.</li>
</ul>

<p>At the same time that history is repeating in the MongoDB world, a tremendous
amount of stuff is happening quietly in other major communities too. Especially
MySQL, but also in PostgreSQL, ElasticSearch, Cassandra and other opensource
databases. I&rsquo;m probably only qualified to write about the MySQL side of things;
I&rsquo;m pretty sure most people don&rsquo;t know a lot of the interesting things that are
going on behind the scenes that will have long-lasting effects. Maybe I&rsquo;ll write
about that someday.</p>

<p>In the meanwhile, I think we&rsquo;re all in for an exciting ride as MongoDB <a href="http://www.xaprb.com/blog/2013/01/10/bold-predictions-on-which-nosql-databases-will-survive/">proves me right</a>.</p>

<p>PS: VividCortex is building a MongoDB monitoring solution that will address many
of the shortcomings of existing ones. (We have been a bit quiet about it, just
out of busyness rather than a desire for secrecy, but now you know.) It&rsquo;s in
beta now.</p>

<p><a href="https://www.flickr.com/photos/96dpi/3645537177/">Cropped image by 96dpi</a></p>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.xaprb.com/blog/2015/05/22/percona-mongodb-mysql-history-repeat/"> on May 22, 2015 06:51 PM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.acmebenchmarking.com/2015/01/diving-deeper-into-mongodb-28.html" title="Acme Benchmarking">Diving deeper into MongoDB 2.8 collection level locking performance</a></h2>

<h3>Tim Callaghan</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<span>Last month I <a href="http://www.acmebenchmarking.com/2014/12/benchmarking-mongodb-28-mmapv1.html" target="_blank">wrote a blog</a> about the closing of MongoDB ticket <a href="https://jira.mongodb.org/browse/SERVER-1240" target="_blank">SERVER-1240</a>, which brings Collection Level Locking (CLL) to the MMAPV1 storage engine in MongoDB 2.8.&nbsp;</span><span>In MongoDB 2.6 there is a writer lock at the database level, so each database only allows one writer at a time. In concurrent write workloads, this means that all writers essentially form a single line and do their writes one at a time. In MongoDB 2.8 this lock has been moved to the collection level. Better yet is document level locking, but even though this feature was shown at <a href="http://www.mongodb.com/mongodb-world/presentations" target="_blank">MongoDB World 2014</a>&nbsp;it's not going to ship. But it did make for one amazing demo by <a href="https://www.linkedin.com/pub/dan-pasette/2/2b2/448" target="_blank">Dan</a> and <a href="https://www.linkedin.com/in/eliothorowitz" target="_blank">Eliot</a>. Seriously, watch the <a href="http://www.mongodb.com/presentations/mongodb-world-2014-keynote-eliot-horowitz" target="_blank">presentation</a> for yourself.</span><br /><span><br /></span><span>The MMAPV1 engine is still important in MongoDB as it will be the default in 2.8, though the <a href="http://www.zdnet.com/article/mongodb-cto-how-our-new-wiredtiger-storage-engine-will-earn-its-stripes/" target="_blank">plan is to make WiredTiger the default engine in MongoDB 3.0</a>. Perhaps that plan explains why the performance gains aren't nearly as interesting as one might expect, as MongoDB might simply be focusing their resources on the future default engine.</span><br /><span><br /></span><span>I ran a <a href="http://www.acmebenchmarking.com/2014/12/benchmarking-mongodb-28-mmapv1.html" target="_blank">series of benchmarks</a> last month to see for myself how much of a performance improvement CLL makes, and the performance gains were not what I expected to see.&nbsp;</span><br /><span><br /></span><span>This month I decided to drill deeper into a specific use-case, simply inserting data into a single collection. The experiment starts with loading just one collection, then simultaneously loading two, then three, and then four. All collections for the experiments are in the same database, as the point of CLL was to eliminate the workaround of putting each collection into a different database (and each database has it's own write lock). To eliminate variables I ran on a server with plenty of RAM, so the test is not IO limited. The test environment was as follows:</span><br /><blockquote class="tr_bq"><span><i>Dell R710 server : Ubuntu 14.04, 2 x Intel Xeon L5520 CPUs, 48GB RAM, 8 x 10K SAS in RAID10 (~2000 Random IOPs)</i></span></blockquote><blockquote class="tr_bq"><span><i>MongoDB 2.6.6 and 2.8.0.RC4</i></span></blockquote><span><b><br /></b></span><span><b>Benchmark 1 : Load data into 1 .. 4 collections</b></span><br /><span><br /></span><br /><div class="separator"><a href="http://3.bp.blogspot.com/-xMRx0jpCfNc/VLPtpJt2BJI/AAAAAAAAB4s/BoV1BFiql-U/s1600/28-cll-benchmark-26-vs-28.png"><img border="0" height="354" src="http://3.bp.blogspot.com/-xMRx0jpCfNc/VLPtpJt2BJI/AAAAAAAAB4s/BoV1BFiql-U/s1600/28-cll-benchmark-26-vs-28.png" width="640" /></a></div><span><br /></span><span>The raw performance by thread count is as follows:</span><br /><br /><span>MongoDB 2.6.6 =&nbsp;20399,&nbsp;24387,&nbsp;24787,&nbsp;23808</span><br /><span>MongoDB 2.8.0.RC4 =&nbsp;22720,&nbsp;30764,&nbsp;34879,&nbsp;36691</span><br /><br /><span>Making the performance increase from 1-2, 2-3, and 3-4 threads:</span><br /><blockquote class="tr_bq"><span>MongoDB 2.6.6 = +19.5%,&nbsp;+1.6%, <span>-3.9%</span></span><span><br /></span><span>MongoDB 2.8.0.RC4 =&nbsp;+35.4%,&nbsp;+11.8%,&nbsp;+5.2%</span></blockquote><div><span>This shows that some improvement was made, but it feels to me that there is some other bottleneck in the system. A perfectly scaling system could theoretically increase 100% from 1-2 threads, 50% from 2-3 threads, and 33% from 3-4 threads.</span></div><div><span><br /></span></div><div><span>I decided to rerun the experiment using MongoDB 2.8.0.RC4 with journaling disabled.</span></div><div><br /></div><div><br /></div><div><b>Benchmark 2 : Load data into 1 .. 4 collections, with Journaling ON and OFF</b></div><div><span><br /></span></div><div class="separator"><a href="http://2.bp.blogspot.com/-Apl8swJB8NA/VLP1p2DX19I/AAAAAAAAB48/nHSbB3rbOKc/s1600/28-cll-benchmark-28-journal-on-vs-off.png"><img border="0" height="354" src="http://2.bp.blogspot.com/-Apl8swJB8NA/VLP1p2DX19I/AAAAAAAAB48/nHSbB3rbOKc/s1600/28-cll-benchmark-28-journal-on-vs-off.png" width="640" /></a></div><div><span><br /></span></div><div><span>The raw performance by thread count is as follows:</span><br /></div><div><span>Journal Enabled =&nbsp;22720,&nbsp;30764,&nbsp;34879,&nbsp;36691</span><br /><span>Journal Disabled = 37034, 71410, 103430, 129013</span><br /><br /><span>Making the performance increase from 1-2, 2-3, and 3-4 threads:</span><br /><blockquote class="tr_bq"><span>Journal Enabled =&nbsp;</span><span>+35.4%,&nbsp;+11.8%,&nbsp;+5.2%</span><span><br /></span><span>Journal Disabled =&nbsp;<b>+92.8%</b>,&nbsp;<b>+44.8%</b>,&nbsp;<b>+24.7%</b></span></blockquote><div><span><b>The performance scaling looks much better here, in fact it's almost perfectly linear.</b> My assumption is that MongoDB did indeed add collection level locking to the MMAPV1 storage engine in 2.8 but that a high-performance group commit algorithm was not included (or considered important).</span></div></div><div><span><br /></span></div><div><span>If anyone with more information wants to share, please do. I'm extremely curious as to why the performance with journaling enabled hasn't improved more with collection level locking. And running without a journal is a really bad idea, IMHO.</span></div><div><br /></div>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.acmebenchmarking.com/2015/01/diving-deeper-into-mongodb-28.html"> on May 21, 2015 09:02 PM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.acmebenchmarking.com/2015/05/percona-acquires-tokutek-my-thoughts-3.html" title="Acme Benchmarking">Percona Acquires Tokutek : My Thoughts #3 : Fractal Tree Indexes</a></h2>

<h3>Tim Callaghan</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<span>Last week I wrote up my thoughts about the Percona acquisition of Tokutek from the perspective of <a href="http://www.acmebenchmarking.com/2015/04/percona-acquires-tokutek-my-thoughts-1.html" target="_blank">TokuDB</a> and <a href="http://www.acmebenchmarking.com/2015/04/percona-acquires-tokutek-my-thoughts-2.html" target="_blank">TokuMX[se]</a>. In this third blog of the trilogy I'll cover the acquisition and the future of the Fractal Tree Index. The Fractal Tree Index is the foundational technology upon which all Tokutek products are built.</span><br /><br /><div class="separator"><a href="http://4.bp.blogspot.com/-1xCQlcZCiDk/VV5vv7vKcoI/AAAAAAAACGo/LYcTiWV1Thk/s1600/FracTree_SS.jpg"><img border="0" src="http://4.bp.blogspot.com/-1xCQlcZCiDk/VV5vv7vKcoI/AAAAAAAACGo/LYcTiWV1Thk/s1600/FracTree_SS.jpg" /></a></div><br /><br /><div class="separator"></div><span>&nbsp;</span><span>So what is a Fractal Tree Index? To quote the <a href="http://en.wikipedia.org/wiki/Fractal_tree_index" target="_blank">Wikipedia page</a>:</span><br /><blockquote class="tr_bq"><span><i>"a Fractal Tree index is a tree data structure that keeps data sorted and allows searches and sequential access in the same time as a B-tree but with insertions and deletions that are asymptotically faster than a B-tree."</i></span></blockquote><span>Fractal Tree Indexes are really cool, they enable the following capabilities in TokuDB and TokuMX[se]:</span><br /><ul><li><span>Great compression</span></li><li><span>High performance index maintenance</span></li><li><span>ACID, MVCC</span></li></ul><span><i>Lastly</i></span><i><span>, I think it's important to  disclose that I worked at Tokutek for 3.5 years (08/2011 -  01/2015) as VP/Engineering and I do not have any equity in Tokutek or  Percona.</span></i><br /><br /><br /><span><b><span>Thoughts on Percona + Fractal Tree Indexes</span></b></span><br /><span><b><span>&nbsp;</span></b> </span><br /><span>Files, Files, Files</span><br /><ul><li><span>Currently, each Fractal Tree Index is stored in it's own file. The benefit of this approach is that dropping an index instantly returns all space used by the index to the filesystem and the execution of the drop-index operation is very fast. The downside is the number of files on a server can become overwhelming with a large number of tables/collections.</span></li><li><span>I think it would be a great feature to allow users the choice of file-per-index, file-per-table, and tablespaces. </span></li><li><span>There is a <a href="https://tokutek.atlassian.net/browse/FT-605" target="_blank">Jira ticket</a> for the effort. </span></li></ul><span></span><span></span><span>Competition</span><br /><ul><li><span>Things are heating up in the write-optimized storage engine space.</span></li><ul><li><span>MySQL: <a href="http://deepis.com/insights/press-releases/deep-launches-deep-engine-leverages-machine-learning-usher-mysql-big-data" target="_blank">DeepDB launched at Percona Live 2015</a> and <a href="https://github.com/MySQLOnRocksDB/mysql-5.6/" target="_blank">RocksDB is working on a storage engine</a>.</span></li><li><span>MongoDB: <a href="http://www.zdnet.com/article/mongodb-cto-how-our-new-wiredtiger-storage-engine-will-earn-its-stripes/" target="_blank">WiredTiger contains a yet-to-be-supported LSM implementation</a> and <a href="http://blog.parse.com/announcements/mongodb-rocksdb-parse/" target="_blank">RocksDB is already in production at Parse</a>.</span></li></ul><li><span>This raises two concerns:</span></li><ul><li><span>Choice is great for the customer, but how much room is there for these technologies to differentiate from each other?</span></li><li><span>Facebook is backing RocksDB and MongoDB is backing WiredTiger, both of these companies have vast resources. It's as much about the marketing as it is the technology.</span></li></ul></ul><span>Online Backup</span><br /><ul><li><span>Tokutek's "hot backup" functionality is closed source and only provided with enterprise editions of TokuDB and TokuMX.</span></li><li><span>The other backup solution is file system snapshots. </span></li><li><span>I'm curious to see if Percona "open sources" hot backup or creates a different open source hot backup technology. </span></li></ul><span>Compression</span><br /><ul><li><span>As far as I know, the Fractal Tree Index currently supports quicklz, zlib, and lzma compression libraries.</span></li><li><span>There are likely benefits to be had with Snappy, especially on fast storage.</span></li><ul><li><span><a href="https://twitter.com/BohuTANG/status/562434664753664000" target="_blank">BohuTANG</a> from the community seems to have a it working.</span></li></ul><li><span>It might be interesting to experiment with index-prefix-compression, as <a href="http://www.mongodb.com/blog/post/whats-new-mongodb-30-part-3-performance-efficiency-gains-new-storage-architecture" target="_blank">WiredTiger has done</a>. WiredTiger claims both on-disk and in-memory space savings using this technique.</span></li></ul><span>Checkpointing</span><br /><ul><li><span>Simply put, a checkpoint is a process by which a database gets to a known state on disk. Should the database server crash (or lose power) the recovery process requires starting from the checkpoint and playing forward all subsequent committed transactions.</span></li><li><span>By default Fractal Tree Indexes checkpoint every 60 seconds.</span></li><li><span>A checkpoint affects the server's performance in that it requires CPU, RAM, and IO.</span></li><li><span>I assume some effort will go toward reducing the impact of a checkpoint on the running system.</span></li></ul><span>Code Complexity</span><br /><ul><li><span>I don't have specific numbers but I suspect that the Fractal Tree Index code base is more complicated than the other open source write-optimized storage engines.</span></li><li><span>I'm happy to be proven wrong about this if anyone wants to present their findings based on lines of code or other accepted code metrics.</span></li><li><span>On a side note WiredTiger has some <a href="http://source.wiredtiger.com/" target="_blank">very nice developer documentation</a>, I'm not sure about the RocksDB documentation.</span></li></ul><span>License</span><br /><ul><li><span>From the <a href="http://www.tokutek.com/resources/technology/" target="_blank">Tokutek website</a></span></li></ul><blockquote class="tr_bq"><span><i>Tokutek’s patented Fractal Tree indexing technology is a result of ten  years of research and development by experts in cache-oblivious  algorithmics and is protected by multiple patents.</i></span></blockquote><ul><li><span>The Fractal Tree Index is licensed as GNU GPL v2 <u>plus</u> a "Patent Rights Grant".</span></li><li><span>Does the modified GPLv2 license affects potential users or developers?</span></li></ul><span><span>Anti-use-cases</span></span><br /><ul><li><span><span>I'd like to highlight a three "soft spots" in Fractal Tree Indexing, as in areas where there is room for improvement or simply things to be aware of.</span></span></li><li><span><span>Leftmost deletion patterns</span></span></li><ul><li><span><span>Fractal Tree Indexes contain large message buffers in the upper levels. These buffers provide much allow for IO "avoidance" of certain operations.</span></span></li><li><span><span>Consider a workload where 100 million rows are inserted using an auto-incrementing primary key, then 50 million rows are deleted. </span></span></li><li><span><span>At this point the Fractal Tree Index will likely have "buffered" the deletes (deletes are just small messages), the inserted data is still in the leaf nodes.</span></span></li><li><span><span>Queries against this deleted data will perform poorly, the extreme case is to restart your server and "select min(pk) from foo;"&nbsp; </span></span></li><li><span><span>Partitioning is one way to deal with this pattern, rather than deleting the rows you'd merely drop them one partition at a time. </span></span></li></ul><li><span><span><span>Random primary key inserts</span></span></span></li><ul><li><span><span><span>Fractal Tree Indexes have a huge advantage, from an IO perspective, when maintaining non-unique indexes. Insert, update, and delete operations can be buffered.</span></span></span></li><li><span><span><span>However, unique indexes must be checked for uniqueness, and thus an IO is required unless the node holding the key is in memory.</span></span></span></li><li><span><span><span>Primary key indexes are always unique.</span></span></span></li><li><span><span><span>So Fractal Tree Indexes perform much like a standard B-tree when randomly inserting into a primary key index. When the data set is larger than RAM, each insert will require an IO to check for uniqueness.</span></span></span></li></ul><li><span><span>Latency</span></span></li><ul><li><span><span>Fractal Tree Indexes employ two techniques to achieve high compression</span></span></li><ul><li><span><span>Large block size - The default is 64KB, and can be set higher.</span></span></li><li><span><span>Algorithms - LZMA &gt; zlib &gt; quicklz (and Snappy is likely coming soon) </span></span></li></ul><li><span><span>Compression comes at a cost, latency.</span></span></li><li><span><span>The larger the node size, the longer the decompress operation. </span></span></li><li><span><span>The higher the compression, the longer the decompress operation.</span></span></li><li><span><span>Users are purchasing SSD/Flash for their IO performance, but they also want high compression because these devices are expensive.</span></span></li><li><span><span>At the moment it's complicated to determine the best combination of node size and compression algorithm, creating user-facing metrics will be helpful.</span></span></li></ul></ul><span>Human Resources</span><br /><ul><li><span>As with TokuDB and TokuMX[se], I'm curious to see how much Percona is&nbsp; looking to grow the team.  They've already posted on their <a href="http://www.percona.com/about-us/careers/open-positions">jobs page</a> for "</span><span>C/C++ Developers for TokuDB, TokuMX, and Tokutek Products".</span></li><li><span>Prioritizing resources between Fractal Tree Indexes, TokuDB, TokuMX, and TokuMXse will be tricky.</span></li></ul><span>&nbsp;</span><span>Please asks questions or comment below.</span>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.acmebenchmarking.com/2015/05/percona-acquires-tokutek-my-thoughts-3.html"> on May 21, 2015 08:52 PM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.acmebenchmarking.com/2015/04/percona-acquires-tokutek-my-thoughts-2.html" title="Acme Benchmarking">Percona Acquires Tokutek : My Thoughts #2 : TokuMX and TokuMXse</a></h2>

<h3>Tim Callaghan</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<span>A few days ago I wrote up <a href="http://www.acmebenchmarking.com/2015/04/percona-acquires-tokutek-my-thoughts-1.html?view=classic" target="_blank">my thoughts about the Percona acquisition of Tokutek with respect to TokuDB</a>. In this blog I'm going to do the same for TokuMX and TokuMXse. And in a few days I'll wrap up this trilogy by sharing my thoughts about Fractal Tree Indexes.</span><br /><span><br /></span><i><span>Again, when I'm writing up something that I was very involved with in the past I think it's important to disclose that I worked at Tokutek for 3.5 years (08/2011 -  01/2015) as VP/Engineering and I do not have any equity in Tokutek or  Percona.</span></i><br /><i><span></span></i><br /><i><b><span></span></b></i><span>Since much of the MySQL crowd might be hearing about Tokutek's "other products" for the first time I'll provide a little history of both of the products before I dive in deeper.</span><br /><br /><span><a href="http://www.tokutek.com/tokumx-for-mongodb/" target="_blank">TokuMX</a> is a fork of MongoDB that was <a href="http://www.tokutek.com/2013/06/announcing-tokumx-v1-0-tokumongo-you-can-have-it-all-2/" target="_blank">launched on June 19, 2013</a>. It began as an experiment in late 2012 by adding Fractal Tree Indexes to handle the secondary indexing work for stock MongoDB while leaving management of the collection data to MongoDB's MMAPv1 storage code. Needless to say the <a href="http://www.tokutek.com/2012/08/10x-insertion-performance-increase-for-mongodb-with-fractal-tree-indexes/" target="_blank">performance</a> <a href="http://www.tokutek.com/2012/08/268x-query-performance-increase-for-mongodb-with-fractal-tree-indexes-say-what/" target="_blank">gains</a> of the prototype were stunning, it provided <a href="http://www.tokutek.com/2013/02/mongodb-fractal-tree-indexes-high-compression/" target="_blank">amazing compression</a>, and even <a href="http://www.tokutek.com/2013/10/introducing-tokumx-transactions-for-mongodb-applications/" target="_blank">enabled transactions and snapshot queries</a>. As a fork it, TokuMX is a drop-in replacement for MongoDB: same wire protocol, same client libraries, same commands and query syntax, etc. TokuMX has released many GA versions since the v1.0 launch in June 2013; including 1.1, 1.2, 1.3, 1.4, 1.5, and 2.0.</span><br /><span><br /></span><span><a href="http://www.tokutek.com/2015/02/tokutek-take-mongodb-v3-0/" target="_blank">TokuMXse</a> is Tokutek's implementation of it's Fractal Tree Indexes in MongoDB v3.0, using the newly available Storage Engine API. One huge advantage of creating a storage engine version of TokuMX is to allow users to mix-and-match storage engines by adding a new server to a running MongoDB v3.0 replica set. A huge disadvantage is that most of the TokuMX goodness is unavailable, I'll get more into this later. Also worth noting is that&nbsp;<a href="http://www.tokutek.com/" target="_blank">Tokutek</a> has not yet created a GA version of TokuMXse, the most recent information I can see is from this <a href="https://groups.google.com/forum/?hl=en#!topic/tokumx-user/VOElRL1ok1Y" target="_blank">Google Groups post</a>. I'm curious as to how/if the Percona acquisition will affect the timing of a TokuMXse GA version.</span><br /><span></span><br /><br /><span><b><span>Thoughts on Percona + TokuMX</span></b></span><br /><span><b><span>&nbsp;</span></b> </span><br /><span>Features - TokuMX brings the following exclusive* features to MongoDB users</span><br /><ul><li><span>In one of the v1.x releases TokuMX introduced partitioning, specifically to deal with performance issues relating to capped collections (the oplog was public enemy #1 when it came to killing the performance of the server). This enabled light-weight deletion of large amounts of data, just what the oplog needed. The TokuMX oplog is bounded by time (hours or days), and not size (as MongoDB is). Any TokuMX collection can be partitioned if the collection is not sharded. I believe that sharded TokuMX collections can now be partitioned but there are some significant restrictions on them. If you are curious, <a href="https://twitter.com/zkasheff" target="_blank">Zardosht Kasheff</a> wrote a lot of great content on the subject, <a href="https://www.google.com/search?q=tokumx+partitioning" target="_blank">Google "tokumx partitioning"</a> for blogs and documentation.</span></li><li><span>Some Fractal Tree Indexes operations can be done blindly, that is they don't require a read before a write. TokuMX enables <a href="http://www.tokutek.com/2014/03/why-tokumx-replication-differs-from-mongodb-replication/" target="_blank">read-free replication to the secondaries</a> and performs <a href="http://www.tokutek.com/2014/09/fast-updates-coming-soon-in-tokumx-v2-0/" target="_blank">"fast-updates" ($inc, $set, ...)</a> in a read-free manner.</span></li><li><span>High-performance indexing, especially secondary indexes.</span></li><li><span>Document-level locking.</span></li><li><span>High compression.</span></li><li><span>Multi-statement transactions in unsharded environments.</span></li><li><span>Snapshot queries. Queries don't need to worry about dirty-reads, or documents missing from a query because they were deleted before the reader got to them.</span></li></ul><span>TokuMX in the age of MongoDB v3.0+</span><br /><ul><li><span>I used an asterisk after the term "exclusive" in the above section as alternative storage engines will surely enable some of the named features.</span></li><li><span><a href="http://www.mongodb.com/press/wired-tiger" target="_blank">MongoDB acquired WiredTiger in December 2014</a>, and is currently the only other "in-the-box" storage engine in MongoDB v3.0. The <a href="http://www.wiredtiger.com/" target="_blank">WiredTiger</a> storage engine provides document-level locking, compression (I haven't done much to measure it's compression vs. the Tokutek products, but I will), and will eventually provide a write-optimized options via it's <a href="http://en.wikipedia.org/wiki/Log-structured_merge-tree" target="_blank">LSM</a> implementation, but only it's B-trees are supported in MongoDB v3.0.</span></li><li><span><a href="http://www.rocksdb.org/" target="_blank">RocksDB</a> powers a MongoDB v3.0 storage engine (it can be used natively and also coming as a MySQL storage engine), and it is backed by <a href="http://www.facebook.com/" target="_blank">Facebook</a>. A few weeks ago <a href="https://twitter.com/mipsytipsy" target="_blank">Charity Majors</a> at <a href="https://parse.com/" target="_blank">Parse</a> <a href="http://blog.parse.com/announcements/mongodb-rocksdb-parse/" target="_blank">announced that Parse is running MongoDB + RocksDB and "seeing great results"</a>. RocksDB is also an LSM implementation with compression and write-optimizations, and yes AcmeBenchmarking will be getting hands-on time with it soon.</span></li></ul><span>Opportunities in 2015 (and beyond)?</span><br /><ul><li><span>The storage engine API is brand new in MongoDB v3.0. It appears that MongoDB is looking to enhance the API with each coming release and has announced a <a href="http://www.mongodb.com/blog/post/mongodb-storage-engine-summit-june-4th" target="_blank">MongoDB Storage Engine Summit</a> as part of <a href="http://mongodbworld.com/" target="_blank">MongoDB World 2015</a>. Storage engines and storage engine APIs are hard. It will take time for the API to improve enough to provide many of the features that TokuMX provides (since June 2013).</span></li><li><span>WiredTiger is new to MongoDB, so much of 2015 will likely be spent learning/tweaking/fixing. At some point, maybe in 2015, MongoDB will support WiredTiger's LSM implementation which should improve secondary indexing performance significantly, especially for people with slower IO subsystems. It will likely be helpful on capped collections (think oplog).</span></li><li><span>RocksDB is likely not tuned for many MongoDB workloads, so there will be plenty of learning to do here as well. Facebook is likely to work on it's internal use-cases before ones from the rest of the world. If it's not significantly better than WiredTiger and isn't "in-the-box" then there will be little reason for people to try it, but that's just my opinion.</span></li></ul><span>MongoDB Version Support</span><br /><ul><li><span>TokuMX is based on MongoDB v2.4.</span></li><li><span>MongoDB v2.6 was released in April 2014 and MongoDB v3.0 was released in February 2015.</span></li><li><span>So TokuMX is now two major releases behind MongoDB and needs to catch up soon.</span></li></ul><br /><span>Files, Files, Files</span><br /><ul><li><span>Just like TokuDB, TokuMX creates a lot of files. Two files per collection, plus another file for each secondary index.</span></li><li><span>On the contrary, MongoDB MMAPv1 uses "tablespaces" so there are far fewer files.</span></li><li><span>However, WiredTiger creates a similar number of files as TokuMX, so maybe this is a non-issue. I'd wager that there will be a "collapsing of files" effort at WiredTiger at some point.</span></li></ul><span>Hot Backup vs. MongoDB Backups</span><br /><ul><li><span>TokuMX's hot backup feature is  enterprise edition only (it is a paid feature) and is closed source.</span></li><li><span>Again, it feels weird that  Percona owns/offers a closed source technology, open sourcing the TokuMX hot backup would be nice to see.</span></li></ul><span>Other Enterprise Edition Features</span><br /><ul><li><span>TokuMX also offers audit capabilities and point in time recovery as paid closed-source features.</span></li><li><span>And again, it feels weird that  Percona owns/offers a closed source technology, open sourcing all TokuMX enterprise features would be nice to see.</span></li></ul><br /><span><span><b><span>Thoughts on Percona + TokuMXse</span></b></span> </span><br /><span><br /></span><span></span><br /><span><a href="http://en.wikipedia.org/wiki/Meh" target="_blank">Meh</a> Factor</span><br /><ul><li><span>Sorry, I couldn't think of a better heading for this section.</span></li><li><span>There are so many features in TokuMX that aren't in TokuMXse.</span></li><ul><li><span>Unless you need to inter-operate with MongoDB, use TokuMX and not TokuMXse (or if you need v2.6 or other v3.0 features)</span></li></ul></ul><span></span><br /><span></span><span>Release Schedule</span><br /><ul><li><span>As I mentioned above there have been several release candidates of TokuMXse, plus some chatter about an upcoming GA version, but nothing has materialized.</span></li><li><span>Also important is the definition of what exactly TokuMXse "is".</span></li><ul><li><span>Is it an "light" version of TokuMX, intended to give users a taste of the technology?</span></li><li><span>Will it be free or lower cost than TokuMX?</span></li></ul></ul><br /><span>Competition</span><br /><ul><li><span>TokuMXse has several advantages versus MMAPv1. </span></li><li><span>WiredTiger and RocksDB are good technology.</span></li><li><span>WiredTiger is backed by MongoDB, RocksDB is backed by Facebook. </span></li><li><span>How much better is TokuMXse performance and compression in real-world usage?</span></li></ul><br /><span>MongoDB Storage Engine API Additions</span><br /><ul><li><span>It is going to be interesting to see how the API evolves over time.&nbsp;</span></li><ul><li><span>Additions will surely come, but how will additions that enable a feature for a single storage engine be handled.</span></li></ul></ul><br /><span><span><b><span>General Thoughts on Percona + TokuMX[se</span></b></span>]</span><br /><br /><span>Human Resources</span><br /><ul><li><span>Percona has been performing MySQL  related engineering for quite a while now, but TokuMX[se] are very different efforts.</span></li><li><span>TokuMX is a fork of MongoDB, not a patch.</span></li><ul><li><span>Keeping up with new releases is a lot of work! </span></li></ul><li><span>As with TokuDB, I'm curious to see how much they  are looking to grow the team after adding TokuMX[se] to their product list.  They've already posted on their <a href="http://www.percona.com/about-us/careers/open-positions">jobs page</a> for "</span><span>C/C++ Developers for TokuDB, TokuMX, and Tokutek Products".</span></li><li><span>Prioritizing resources between TokuDB, TokuMX, and TokuMXse will be tricky.</span></li><li><span>Percona based support is attractive, IMHO.</span></li><ul><li><span>But their support team has no experience with TokuMX[se].</span></li></ul><li><span>Percona consulting also has no experience with TokuMX[se].</span></li></ul><br /><span>Awareness</span><br /><br /><ul><li><span>The Percona brand and marketing team will significantly raise awareness to the existence of TokuMX[se].</span></li><li><span>Existing Percona customers will be exposed to TokuMX[se], especially if they are "MongoDB curious".&nbsp;</span></li><li><span>The combination of these two should raise the number of TokuMX[se] customers and users.</span></li></ul><br /><span>Please asks questions or comment below.</span>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.acmebenchmarking.com/2015/04/percona-acquires-tokutek-my-thoughts-2.html"> on April 29, 2015 02:10 PM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.acmebenchmarking.com/2015/04/percona-acquires-tokutek-my-thoughts-1.html" title="Acme Benchmarking">Percona Acquires Tokutek : My Thoughts #1 : TokuDB</a></h2>

<h3>Tim Callaghan</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<span>Two weeks ago Percona announced it's acquisition of Tokutek (April 14, 2015). The analyst coverage was a bit fluffy for my liking, but I decided to give it some time and see if anything "meaty" would come along, and ... it hasn't. The sheer number of <a href="https://twitter.com/search?q=percona%20tokutek&src=typd">tweets on Twitter</a> was impressive, which makes me hopeful that the acquisition raised awareness to the Tokutek technologies and that the Tokutek products have a found a good home</span><br /><span><br /></span><span>I've been thinking a lot about the future of the Tokutek technologies over these same two weeks and want to share them publicly. I'm going to cover TokuDB in this blog post, TokuMX in a few days, and finally Fractal Tree Indexes a few days later. <i>[Full disclosure: I worked at Tokutek for 3.5 years (08/2011 - 01/2015) as VP/Engineering and I do not have any equity in Tokutek or Percona]</i></span><br /><br /><i><b><span><br /></span></b></i><b><span>Thoughts on Percona + TokuDB</span></b><br /><span><br /></span><span>Integration and Ease of Use</span><br /><ul><li><span>Percona will certainly spend the time to make using the TokuDB storage engine in Percona Server as easy and foolproof as possible. Prior to the acquisition, users needed to download and install an additional package to use the TokuDB storage engine in Percona Server (this was not the case for those downloading directly from the <a href="http://www.tokutek.com/" target="_blank">Tokutek web site</a> or downloading from <a href="https://downloads.mariadb.org/" target="_blank">MariaDB</a>). I hope that TokuDB becomes part of the base Percona Server package and that the plugin is installed by default. At that point the process of trying TokuDB is as easy as adding "engine=TokuDB" in a CREATE TABLE statement.</span></li><li><span>Memory usage (cache). InnoDB supports a user defined cache size, as does TokuDB. Users often allocate more than 50% to their cache to their storage engine and can easily over allocate the server's memory if using both engines. I'm not sure what the ideal solution is for this problem. I think InnoDB supports a dynamic cache sizing in MySQL 5.7, perhaps adding this feature to TokuDB and automatically changing the [over]allocation would work.</span></li></ul><span>Foreign Keys</span><br /><ul><li><span>Had MySQL implemented foreign key constraints above the storage engine this would have been a non-issue, but it didn't. They are implemented within InnoDB. Will foreign keys ever come to TokuDB? I'd argue that it's not worth the effort, and users needing foreign keys can always use InnoDB for those specific tables. But lack of foreign keys certainly complicates the user's experience.</span></li></ul><span>Files, Files, Files</span><br /><ul><li><span>Most people use InnoDB's file-per-table option, meaning a single file is creating in the file system for each table (I'm not going to count .frm files). In contrast, TokuDB creates 2 files for a table, and another file for each secondary index. A great benefit of this approach is that dropping an index is instantaneous, and all the space for that index is returned immediately. The downside is the sheer number of files, especially if you have a large number of tables. And a lot more files if you partition your tables (a full set of the before mentioned file for each partition).</span></li><li><span>All TokuDB files are kept in the root of the data folder (or they can be put in a single TokuDB defined data directory). Moving the files to the individual database folders would be a nice feature.</span></li></ul><span>Hot Backup vs. XtraBackup</span><br /><ul><li><span>Creating an online backup of TokuDB is significantly different than performing the same operation of InnoDB. Percona created XtraBackup to simplify the backup process for InnoDB and it is now a feature-rich backup technology (full backups, incremental backups, etc). XtraBackup <u><b>does not</b></u> work on TokuDB tables.</span></li><li><span>TokuDB's hot backup feature is enterprise edition only (it is the paid feature), closed source, and only supports the creation of a full backup. It does work on InnoDB tables, as long as <a href="http://dev.mysql.com/doc/innodb/1.1/en/innodb-performance-aio-linux.html" target="_blank">asynchronous IO</a> is not enabled.</span></li><li><span>What does the future hold? It would be great to see TokuDB's hot backup functionality merged into XtraBackup so a single backup technology existed that "just worked" for both storage engines.</span></li><li><span>At the moment it feels weird that Percona owns/offers a closed source technology, open sourcing the TokuDB hot backup would be nice to see.</span></li></ul><span>Instrumentation and Utilities</span><br /><ul><li><span>Percona Server is well known for the additional instrumentation it provides, it would be awesome if this operational "tooling" could also be applied to the TokuDB storage engine internals and exposed easy consumption.</span></li><li><span>It will also be interesting to see if TokuDB gets more attention in Percona's <a href="https://cloud.percona.com/" target="_blank">cloud tools</a>. Percona could collect and analyze information from servers using InnoDB and make the recommendation that TokuDB be adopted by analyzing the user's workload.</span></li><li><span>As Percona provides support to TokuDB customers and gathers feedback from the TokuDB community there will likely be features and new utilities added to the <a href="http://www.percona.com/software/percona-toolkit" target="_blank">Percona Toolkit</a>.</span></li></ul><span>Native Partitioning</span><br /><ul><li><span>InnoDB is adding native partitioning in MySQL 5.7. Partitioning is currently handled by what is essentially a "storage engine", which is pretty cool. A big downside to this implementation is that queries needing data from multiple partitions query each partition in order, and can take a long time when the number of partitions is large. I assume that InnoDB's long term plans for native partitioning is to support concurrent queries on multiple partitions, we shall see. Percona will need to invest in TokuDB to bring native partitioning to it as well.</span></li></ul><span>MySQL and MariaDB Support</span><br /><ul><li><span>Will Percona assist MariaDB with the engineering/QA/packaging of TokuDB?</span></li><li><span>Will Percona offer a MySQL version of TokuDB as Tokutek has in the past?&nbsp;</span></li><li><span>Time will tell.</span></li></ul><span>Human Resources</span><br /><ul><li><span>Percona has been performing MySQL related engineering for quite a while now, but TokuDB is not exactly the same effort as XtraDB and XtraBackup. I'm curious to see how much they are looking to grow the team after adding TokuDB to their product list. They've already posted on their <a href="http://www.percona.com/about-us/careers/open-positions">jobs page</a> for "</span><span>C/C++ Developers for TokuDB, TokuMX, and Tokutek Products".</span></li><li><span>Adding Percona based support for TokuDB is a huge win for current and future TokuDB customers.</span></li><li><span>Percona consulting will quickly learn the best workloads for TokuDB which should grow the user base (both paid and community).</span></li><li><span>I'm excited about all of these possibilities for TokuDB.</span></li></ul><span>I will probably come up with more thoughts over time, but this feels like a good place to stop for now. I'll post my TokuMX and TokuMXse thoughts in a few days.&nbsp;</span><br /><br /><span>Please asks questions or comment below.</span>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.acmebenchmarking.com/2015/04/percona-acquires-tokutek-my-thoughts-1.html"> on April 29, 2015 08:38 AM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.acmebenchmarking.com/2015/04/how-to-purchase-benchmarking-hardware.html" title="Acme Benchmarking">How to Purchase [Benchmarking] Hardware on a Budget</a></h2>

<h3>Tim Callaghan</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<span>One of my goals at Acmebenchmarking is make sure I'm running on hardware that is representative of real-world infrastructure, while at the same time doing it as inexpensively as possible.</span><br /><br /><span>To date I've been running on two custom built "desktops" (for lack of a better term). Both have an Intel Core i7 4790K processor (quad core plus hyperthreading, 4Ghz), 32GB RAM (dual channel), and a quality SSD. They are named acmebench01 and acmebench02.</span><br /><br /><span>Alas, it is time to expand. <i><b>MUST...PURCHASE...MORE...HARDWARE!</b></i></span><br /><span><br />In order to maintain the inexpensive theme I tend to buy used hardware, my goal on this purchase was to achieve many more cores and greater memory bandwidth than my existing machines can provide. Keep in mind that used hardware is great for benchmarking (and likely development and QA environments) but you might want to avoid it for production. For years now I've been purchasing used hardware from <a href="http://neweraserver.com/">NES International</a>, so I brought up their configurator and ordered the following.</span><br /><span><br /></span><b><span>Dell Workstation T7500</span></b><br /><span>- (2) Intel Xeon 5560 CPU</span><br /><span>- 48GB RAM</span><br /><span>- 250GB SATA</span><br /><span><br /></span><span>The "server" purchase included free shipping (at 70 pounds that is no small benefit) and landed on my doorstep for <u><b>$819</b></u>. I replaced the cheap SATA drive with a quality SSD and now have a pretty nice rig for testing workloads with high thread counts or the need for a significant bump in memory bandwidth. Again, I can't say enough about the folks at NES International (they even have an <a href="http://www.ebay.com/usr/neweraserver">eBay Store</a>). Be careful, you might spend a lot of time looking over their stuff as they currently have over 20,000 used Dell servers for sale.</span><br /><span><br /></span><span>Say hello to acmebench03, (12) 2.8Ghz cores plus hyper-threading and 3 memory channels per socket. We shall see which databases and storage engines can actually harness such power!</span><br /><span><br /></span><div class="separator"><a href="http://2.bp.blogspot.com/-KQCerHPvyaE/VSwDMzyTahI/AAAAAAAACEA/rbdOqDMVHz0/s1600/20150410_01_acmebench03-cpuinfo.jpg"><img border="0" src="http://2.bp.blogspot.com/-KQCerHPvyaE/VSwDMzyTahI/AAAAAAAACEA/rbdOqDMVHz0/s1600/20150410_01_acmebench03-cpuinfo.jpg" height="320" width="314" /></a></div><span><br /></span><br /><span><br /></span><span>Here are a few more pictures for anyone into the hardware "unboxing" scene.</span><br /><br /><div><span>Unopened.</span></div><span></span><br /><span> </span><br /><div class="separator"><a href="http://1.bp.blogspot.com/-nPIR4prQDBs/VSwDSOqPYoI/AAAAAAAACEI/TE4lZnsCPYA/s1600/20150410_02_acmebench03-unopened.png"><img border="0" src="http://1.bp.blogspot.com/-nPIR4prQDBs/VSwDSOqPYoI/AAAAAAAACEI/TE4lZnsCPYA/s1600/20150410_02_acmebench03-unopened.png" height="320" width="236" /></a></div><br /><div><span>Amazing packing job.</span></div><div><span></span></div><div></div><div class="separator"><a href="http://2.bp.blogspot.com/-Whx46Hx001o/VSwDjXXB9NI/AAAAAAAACEQ/Ao1V96N7wEM/s1600/20150410_03_acmebench03-packing-job.png"><img border="0" src="http://2.bp.blogspot.com/-Whx46Hx001o/VSwDjXXB9NI/AAAAAAAACEQ/Ao1V96N7wEM/s1600/20150410_03_acmebench03-packing-job.png" height="320" width="236" /></a></div><div><span></span></div><div><span><br /></span></div><div><span>Outside.</span></div><div><span></span></div><div></div><div class="separator"><a href="http://1.bp.blogspot.com/-V4UQYVpsK8g/VSwDm23odCI/AAAAAAAACEY/Lo-UgVYePqY/s1600/20150410_04_acmebench03-outside.png"><img border="0" src="http://1.bp.blogspot.com/-V4UQYVpsK8g/VSwDm23odCI/AAAAAAAACEY/Lo-UgVYePqY/s1600/20150410_04_acmebench03-outside.png" height="320" width="236" /></a></div><div><span><br /></span></div><div><span>Inside.</span></div><div class="separator"><a href="http://4.bp.blogspot.com/-8qZo2nv0kKY/VSwDpqs6WkI/AAAAAAAACEg/xE5imlfu9uE/s1600/20150410_05_acmebench03-inside.png"><img border="0" src="http://4.bp.blogspot.com/-8qZo2nv0kKY/VSwDpqs6WkI/AAAAAAAACEg/xE5imlfu9uE/s1600/20150410_05_acmebench03-inside.png" height="320" width="236" /></a></div><div><span> </span></div><div><span><br /></span></div><span><br /></span><br /><br /><br /><br /><br />			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.acmebenchmarking.com/2015/04/how-to-purchase-benchmarking-hardware.html"> on April 13, 2015 03:01 PM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.xaprb.com/blog/2015/04/02/state-of-the-storage-engine/" title="">State Of The Storage Engine - DZone</a></h2>

<h3>Baron Schwartz</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<p>I contributed an article on <a href="http://www.dzone.com/articles/state-storage-engine">modern database storage
engines</a> to the recent
<a href="http://dzone.com/research/guide-to-databases">DZone Guide To Database and Persistence
Management</a>. I&rsquo;m cross-posting the
article below with DZone&rsquo;s permission.</p>

<p><img src="http://www.xaprb.com/media/2015/04/boardwalk.jpg" alt="Boardwalk" />
</p>

<p>Readers of this guide already know the database world is undergoing rapid change. From relational-only, to NoSQL and Big Data, the technologies we use for data storage and retrieval today are much different from even five years ago.</p>

<p>Today’s datasets are so large, and the workloads so demanding, that one-size-fits-all databases rarely make much sense. When a small inefficiency is multiplied by a huge dataset, the opportunity to use a specialized database to save money, improve performance, and optimize for developer productivity and happiness can be very large. And today’s solid-state storage is vastly different from spinning disks, too. These factors are forcing fundamental changes for database internals: the underlying algorithms, file formats, and data structures. As a result, modern applications are often backed by as many as a dozen distinct types of databases (polyglot persistence). These trends signal significant, long-term change in how databases are built, chosen, and managed.</p>

<blockquote>
<p>Most companies can afford only one or two proper in-depth evaluations for a new database.</p>
</blockquote>

<h3 id="textbook-architectures-lose-relevance">Textbook Architectures Lose Relevance</h3>

<p>Many of today’s mature relational databases, such as MySQL, Oracle, SQL Server, and PostgreSQL, base much of their architecture and design on decades-old research into transactional storage and relational models that stem from two classic textbooks in the field—known simply as <a href="http://www.amazon.com/dp/1558601902">Gray &amp; Reuters</a> and <a href="http://www.amazon.com/dp/1558605088">Weikum &amp; Vossen</a>. This “textbook architecture” can be described briefly as having:</p>

<ul>
<li>Row-based storage with fixed schemas</li>
<li>B-Tree primary and secondary indexes</li>
<li>ACID transaction support</li>
<li>Row-based locking</li>
<li>MVCC (multi-version concurrency control) implemented by keeping old row versions</li>
</ul>

<p>But this textbook architecture has been increasingly questioned, not only by newcomers but by leading database architects such as <a href="http://slideshot.epfl.ch/play/suri_stonebraker">Michael Stonebraker</a>. Some new databases depart significantly from the textbook architecture with concepts such as wide-row and columnar storage, no support for concurrency at all, and eventual consistency. It’s worth noting that although NoSQL databases represent obvious changes in the data model and language—how developers access the database—not all NoSQL databases innovate architecturally. Coping with today’s data storage challenges often requires breaking from tradition architecturally, especially in the storage engine.</p>

<h3 id="log-structured-merge-trees">Log-Structured Merge Trees</h3>

<p>One of the more interesting trends in storage engines is the emergence of log-structured merge trees (LSM trees) as a replacement for the venerable B-Tree index. LSM trees are now about two decades old, and LevelDB is perhaps the most popular implementation. Databases such as Apache HBase, Hyperdex, Apache Cassandra, RocksDB, WiredTiger, and Riak use various types of LSM trees.</p>

<p>LSM trees work by recording data, and changes to the data, in immutable segments or runs. The segments are usually organized into levels or generations. There are several strategies, but the first level commonly contains the most recent and active data, and lower levels usually have progressively larger and/or older data, depending on the leveling strategy. As data is inserted or changed, the top level fills up and its data is copied into a segment in the second level. Background processes merge segments in each level together, pruning out obsolete data and building lower-level segments in batches. Some LSM tree implementations add other features such as automatic compression, too. There are several benefits to this approach as compared to the classic B-Tree approach:</p>

<ul>
<li>Immutable storage segments are easily cached and backed up</li>
<li>Writes can be performed without reading first, greatly speeding them up</li>
<li>Some difficult problems such as fragmentation are avoided or replaced by simpler problems</li>
<li>Some workloads can experience fewer random-access I/O operations, which are slow</li>
<li>There may be less wear on solid-state storage, which can’t update data in-place</li>
<li>It can be possible to eliminate the B-Tree “write cliff,” which happens when the working set no longer fits in memory and writes slow down drastically</li>
</ul>

<p>Although many of the problems with B-Tree indexes can be avoided, mitigated, or transformed, LSM tree indexes aren’t a panacea. There are always trade-offs and implementation details. The main set of trade-offs for LSM trees are usually explained in terms of amplification along several dimensions. The amplification is the average ratio of the database’s physical behavior to the logical behavior of the user’s request, over the long-term. It’s usually a ratio of bytes to bytes, but can also be expressed in terms of operations, e.g. number of physical I/O operations performed per logical user request.</p>

<ul>
<li><strong>Write amplification</strong> is the multiple of bytes written by the database to bytes changed by the user. Since some LSM trees rewrite unchanging data over time, write amplification can be high in LSM trees.</li>
<li><strong>Read amplification</strong> is how many bytes the database has to physically read to return values to the user, compared to the bytes returned. Since LSM trees may have to look in several places to find data, or to determine what the data’s most recent value is, read amplification can be high.</li>
<li><strong>Space amplification</strong> is how many bytes of data are stored on disk, relative to how many logical bytes the database contains. Since LSM trees don’t update in place, values that are updated often can cause space amplification.</li>
</ul>

<p>In addition to amplification, LSM trees can have other performance problems, such as read and write bursts and stalls. It’s important to note that amplification and other issues are heavily dependent on workload, configuration of the engine, and the specific implementation. Unlike B-Tree indexes, which have essentially a single canonical implementation, LSM trees are a group of related algorithms and implementations that vary widely.</p>

<p>There are other interesting technologies to consider besides LSM trees. One is <a href="https://symas.com/getting-down-and-dirty-with-lmdb-qa-with-symas-corporations-howard-chu-about-symass-lightning-memory-mapped-database/">Howard Chu</a>’s LMDB (Lightning Memory-Mapped Database), which is a copy-on-write B-Tree. It is widely used and has inspired clones such as <a href="https://github.com/boltdb/bolt">BoltDB</a>, which is the storage engine behind the up-and-coming <a href="http://influxdb.com/">InfluxDB</a> time-series database. Another LSM alternative is <a href="http://www.tokutek.com/">Tokutek’s</a> fractal trees, which form the basis of high-performance write and space-optimized alternatives to MySQL and MongoDB.</p>

<h3 id="evaluating-databases-with-log-structured-merge-trees">Evaluating Databases With Log-Structured Merge Trees</h3>

<p>No matter what underlying storage you use, there’s always a trade-off. The iron triangle of storage engines is this:</p>

<p>You can have <strong>sequential reads without amplification, sequential writes without amplification, or an immutable write-once design</strong>—<i>pick any two</i>.</p>

<p>Today’s emerging Big Data use cases, in which massive datasets are kept in raw form for a long time instead of being summarized and discarded, represent some of the classes of workloads that can potentially be addressed well with LSM tree storage (time-series data is a good example). However, knowledge of the specific LSM implementation must be combined with a deep understanding of the workload, hardware, and application.</p>

<blockquote>
<p>&hellip;although NoSQL databases represent obvious changes in the data model and language, not all NoSQL databases innovate architecturally.</p>
</blockquote>

<p>Sometimes companies don’t find a database that’s optimized for their exact use case, so they build their own, often borrowing concepts from various databases and newer storage engines to achieve the efficiency and performance they need. An alternative is to adapt an efficient and trusted technology that’s almost good enough. At VividCortex, we ignore the relational features of MySQL and use it as a thin wrapper around InnoDB to store our large-scale, high-velocity time-series data.</p>

<p>Whatever road you take, a good deal of creativity and experience is required from architects who are looking to overhaul their application’s capabilities. You can’t just assume you’ll plug in a database that will immediately fit your use case. You’ll need to take a much deeper look at the storage engine and the paradigms it is based on.</p>

<blockquote>
<p><strong>Baron Schwartz</strong> is the founder and CEO of <a href="https://vividcortex.com">VividCortex</a>, the best way to see what your production database servers are doing. He is the author of High Performance MySQL and many open-source tools for MySQL administration. He’s also an Oracle ACE and frequent participant in the PostgreSQL community.</p>
</blockquote>

<p>To read the full report free of charge, download the
<a href="http://dzone.com/research/guide-to-databases">DZone Guide To Database and Persistence
Management</a>.</p>

<p>Cropped boardwalk image by <a href="https://unsplash.com/nmsilva">Nuno Silva</a>.</p>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.xaprb.com/blog/2015/04/02/state-of-the-storage-engine/"> on April 02, 2015 08:51 AM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.acmebenchmarking.com/2015/03/benchmarketing-charts-hurt-us-all.html" title="Acme Benchmarking">Bad Benchmarketing and the Bar Chart</a></h2>

<h3>Tim Callaghan</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<span>Technical conferences are flooded with visual [mis]representations of a particular product's performance, compression, cost effectiveness, micro-transactions per flux-capacitor, or whatever two-axis comparison someone dreams up. </span><span>Lets be honest, benchmarketers like to believe we all suffer from <a href="http://www.merriam-webster.com/dictionary/innumeracy">innumeracy</a>. </span><br /><span><br /></span><span>The <a href="http://www.merriam-webster.com/">Merriam-Webster dictionary</a> defines innumeracy as follows:</span><br /><blockquote class="tr_bq"><span><i><span class="ssens">innumeracy (noun): marked by an ignorance of mathematics and the scientific approach </span></i></span></blockquote><span><a href="http://smalldatum.blogspot.com/">Mark Callaghan</a> has been a long time advocate of <a href="http://smalldatum.blogspot.com/2014/06/benchmarketing.html">explaining benchmark results</a>, but that's not the point of the bar chart. Oh no, the bar chart only exists to catch your eye and draw you into the booth for further conversation.</span><br /><span><br /></span><span>I was attending a large name-brand conference in 2014. A well-known hardware vendor was presenting one of the keynotes. A few slides into the deck and it was "Benchmark Time!", so up came the following bar chart.</span><br /><div class="separator"><span></span></div><div class="separator"><a href="http://2.bp.blogspot.com/-keoyMEm27u4/VPRm_h33YJI/AAAAAAAAB-E/Q89pNWP-CuE/s1600/benchmarketing-graphs-01-original.png"><img border="0" src="http://2.bp.blogspot.com/-keoyMEm27u4/VPRm_h33YJI/AAAAAAAAB-E/Q89pNWP-CuE/s1600/benchmarketing-graphs-01-original.png" /></a></div><span>The visualization of their data is quite dramatic. Their product, the "us" bar, delivering a substantially higher "% Improvement" over their competitions, the "them" bar. On a quick glance your mind tells you, "wow, their product is almost 3x better than the competition". And a quick glance is all you get because presenters typically spend less than 60 seconds per slide, even less in a keynote. I've been waiting to catch this type of benchmarketer in the wild, so I quickly pulled out my phone and took a picture.</span><br /><span><br /></span><span>Lets break down the events that led to the above bar chart. Long before the keynote the vendor in question asked someone on their technical staff to create a scenario (benchmark) comparing their product to the competition. The request usually includes something like the following, "Make sure we are measurably better than them, but not so much that people won't believe it". So the technical resource goes away, creates the benchmark, executes it, and presents the following data to the Marketing department.</span><br /><br /><pre><code>us     58.5<br />them   49.0</code></pre><span><br /></span><span>Now keep in mind, the "us" number of 58.5 <i><b>is only 19.3% higher</b></i> than the "them" number of 49.0. A 20% improvement in an important system metric might be huge for certain use-cases, but its not that compelling for general consumption, especially during a keynote. So marketing gets to work with the "data", <i>which almost seems silly given that the data consists of exactly two numbers</i>.</span><br /><span><br /></span><span>Now any good Marketer will generally fire up Microsoft Excel and see what they can do with this data. <i>Indeed it is almost comical to call it data, this scenario is actually just 2 values.</i></span><br /><span><br /></span><span>First up is what I call the purely scientific graph. Setting the y-axis range to the possible values (lets use 0 to 100 for this scenario) creates the following graph.</span><br /><span></span><br /><div class="separator"><a href="http://3.bp.blogspot.com/-LC5J9w8kREw/VPRsIIDsIaI/AAAAAAAAB-U/RfjhYNNHQiY/s1600/benchmarketing-graphs-04-0-to-100.png"><img border="0" src="http://3.bp.blogspot.com/-LC5J9w8kREw/VPRsIIDsIaI/AAAAAAAAB-U/RfjhYNNHQiY/s1600/benchmarketing-graphs-04-0-to-100.png" /></a></div><span>Needless to say, this chart doesn't make the cut. There is too little visual difference between the two bars.</span><br /><span><br /></span><span>So next up is an attempt to re-chart the data with a still scientific approach, what I like to call the "we are the best" chart.</span><br /><span></span><br /><div class="separator"><a href="http://2.bp.blogspot.com/-BiDfiuIf9lQ/VPRsvT78dII/AAAAAAAAB-c/ahUCfGgqIqQ/s1600/benchmarketing-graphs-03-0-to-65.png"><img border="0" src="http://2.bp.blogspot.com/-BiDfiuIf9lQ/VPRsvT78dII/AAAAAAAAB-c/ahUCfGgqIqQ/s1600/benchmarketing-graphs-03-0-to-65.png" /></a></div><span>The goal of this chart is to set something slightly larger than our value as the maximum y value but keep the minimum value at 0, thus making the difference between "us" and "them" more apparent. As with the prior graph, this one is rejected as our awesomeness is not properly conveyed.</span><br /><span><br /></span><span>So it's time to get extreme, and create the "world domination" graph. I've never seen one of these in the wild, but it's just a matter of time.</span><br /><span></span><br /><div class="separator"><a href="http://1.bp.blogspot.com/-B4mai6IhVwg/VPRtXt6bY4I/AAAAAAAAB-k/IeM-bhsssBw/s1600/benchmarketing-graphs-02-48-to-59.png"><img border="0" src="http://1.bp.blogspot.com/-B4mai6IhVwg/VPRtXt6bY4I/AAAAAAAAB-k/IeM-bhsssBw/s1600/benchmarketing-graphs-02-48-to-59.png" /></a></div><span>This graph uses a value slightly larger than the "us" as the maximum y-axis value and something slightly smaller than the "them" as the minimum y-axis value. The results are stunning, we're talking order-of-magnitude improvements now. <b>Well done!</b></span><br /><span><br /></span><span>At this point the presentation starts coming together with with the above slide. Inevitably someone in engineering walks by a printer, sees the chart, and freaks out. Engineering and Marketing negotiate a peaceful settlement and we end up with the chart at the top of this blog. Not ideal, but certainly better than what might have been presented. Benchmarketing for-the-win!</span><br /><br /><span>So that's it, hopefully that explains the process. Perhaps you're now a little better prepared to question what you see, and question you should. Don't be innumerate.</span><br /><br /><span>I want to create a page on AcmeBenchmarking with a Benchmarketing Hall of Fame, so please send along any pictures or URLs of the bad benchmarketing </span><span>you've seen.&nbsp;</span><br /><br /><span>Also get involved in the comments. Any Marketers have a contrary opinion?</span>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.acmebenchmarking.com/2015/03/benchmarketing-charts-hurt-us-all.html"> on March 02, 2015 01:00 PM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.acmebenchmarking.com/2015/02/how-to-benchmark-mongodb.html" title="Acme Benchmarking">How to benchmark MongoDB</a></h2>

<h3>Tim Callaghan</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<span>There are generally three components to any benchmark project:</span><br /><ol><li><span>Create the benchmark application</span></li><li><span>Execute it</span></li><li><span>Publish your results</span></li></ol><span>I assume many people think they want to run more benchmarks but give up since step 2 is extremely consuming as you expand the number of different configurations/scenarios.</span><br /><br /><span>I'm hoping that this blog post will encourage more people to dive-in and participate, as I'll be sharing the bash script I used to test the <a href="http://www.acmebenchmarking.com/2015/02/mongodb-v30-compression-benchmarks.html" target="_blank">various compression options coming in the MongoDB 3.0 storage engines</a>. It enabled me to run a few different tests against 8 different configurations, recording insertion speed and size-on-disk for each one.</span><br /><br /><span>If you're into this sort of thing, please read on and provide any feedback or improvements you can think of. You also might want to grab a Snickers, as there is a lot to cover. I've commented along the way so hopefully it is an interesting read. Also, links to the full script and configuration files are at the bottom of the blog. Lets get started!</span><br /><br /><pre><code>#!/bin/bash<br /><br /># remember the directory we are starting from<br />#   the script expects the MongoDB configuration files<br />export homeDirectory=$PWD<br /><br /># directory where MongoDB/TokuMX tarballs are located<br />export tarDirectory=${BACKUP_DIR}/mongodb<br /><br /># directory used for MongoDB server binaries and data folder<br />export MONGO_DIR=~/temp<br /><br /># perform some sanity checks<br /><br /># check that $MONGO_DIR is defined<br />if [ -z "$MONGO_DIR" ]; then<br />    echo "Need to set MONGO_DIR"<br />    exit 1<br />fi<br /><br /># check that $MONGO_DIR exists<br />if [ ! -d "$MONGO_DIR" ]; then<br />    echo "Need to create directory $MONGO_DIR"<br />    exit 1<br />fi<br /><br /># check that $MONGO_DIR is empty<br />#   force manual cleanup before starting<br />if [ "$(ls -A ${MONGO_DIR})" ]; then<br />   echo "Directory $MONGO_DIR must be empty before starting"<br />   exit 1<br />fi</code></pre><span><br /></span><span>I'm a big fan of two things at the top of all my scripts: directory locations and sanity checks. The three directories needed for this particular benchmark run are as follows:</span><br /><ul><li><span>homeDirectory = The directory from where we are executing the script.</span></li><li><span>tarDirectory = The directory where the tar files exist for the various MongoDB flavors/versions that we are benchmarking. You'll likely need to change this for your benchmarks.</span></li><li><span>MONGO_DIR = The directory where we'll be unpacking the tar files (to execute the mongod binary) as well as creating a directory for storing the data for the benchmark. Make sure this is on decent storage is you are running a performance benchmark, a single SATA drive isn't fast. You'll likely need to change this for your benchmarks.</span></li></ul><span>The sanity checks follow, we want to make sure that $MONGO_DIR is defined (just in case), the the $MONGO_DIR directory exists, and that the $MONGO_DIR directory is empty. The empty check is something I think is important, you might have something interesting in that directory and should manually clear it out before starting the benchmark.</span><span></span><br /><span><br /></span><br /><pre><code># decide which tarballs and configurations we want to benchmark<br />#   use semi-colon list of "tarball;id;config;mongo_type"<br />#     tarball     : MongoDB or TokuMX tarball<br />#     id          : Short hand description of this particular benchmark run, ends up in the log file and the summary log<br />#     config      : YAML configuration file to use for the this benchmark run<br />#     mongo_type  : Identifies which "type" of MongoDB, tokumx|mxse|wt|mongo<br />export benchmarkList=""<br />export benchmarkList="${benchmarkList} mongodb-linux-x86_64-tokumxse-1.0.0-rc.2.tgz;mxse_100rc2_none;tokumxse-uncompressed.conf;mxse"<br />export benchmarkList="${benchmarkList} mongodb-linux-x86_64-tokumxse-1.0.0-rc.2.tgz;mxse_100rc2_quicklz;tokumxse-quicklz.conf;mxse"<br />export benchmarkList="${benchmarkList} mongodb-linux-x86_64-tokumxse-1.0.0-rc.2.tgz;mxse_100rc2_zlib;tokumxse-zlib.conf;mxse"<br />export benchmarkList="${benchmarkList} mongodb-linux-x86_64-tokumxse-1.0.0-rc.2.tgz;mxse_100rc2_lzma;tokumxse-lzma.conf;mxse"<br />export benchmarkList="${benchmarkList} mongodb-linux-x86_64-3.0.0-rc8.tgz;mmapv1_300rc8;mmapv1.conf;mongo"<br />export benchmarkList="${benchmarkList} mongodb-linux-x86_64-3.0.0-rc8.tgz;wt_300rc8_none;wiredtiger-uncompressed.conf;wt"<br />export benchmarkList="${benchmarkList} mongodb-linux-x86_64-3.0.0-rc8.tgz;wt_300rc8_snappy;wiredtiger-snappy.conf;wt"<br />export benchmarkList="${benchmarkList} mongodb-linux-x86_64-3.0.0-rc8.tgz;wt_300rc8_zlib;wiredtiger-zlib.conf;wt"</code></pre><span><br /></span><span>Benchmarking is usually a single test run against multiple scenarios, and this is the section where we define those scenarios. The benchmarkList variable starts empty and is then appended with one or more scenarios. The scenario information is broken down into 4 segments, each delimited by a semi-colon. The comment above it is self-explanatory but worth explaining is the fourth segment, mongo_type. This script doesn't care what specific "MongoDB" you are running, but others I've created do so I always define it should I want it somewhere else.</span><br /><br /><pre><code># make sure we have valid tarballs and config scripts for this benchmark run<br />echo "checking that all needed tarballs exist."<br />for thisBenchmark in ${benchmarkList}; do<br />    TARBALL=$(echo "${thisBenchmark}" | cut -d';' -f1)<br />    MONGOD_CONFIG=$(echo "${thisBenchmark}" | cut -d';' -f3)<br /><br />    if [ -e ${tarDirectory}/${TARBALL} ]; then<br />        echo "  located ${tarDirectory}/${TARBALL}"<br />    else<br />        echo "  unable to locate ${tarDirectory}/${TARBALL}, exiting."<br />        exit 1<br />    fi<br /><br />    if [ -e ${MONGOD_CONFIG} ]; then<br />        echo "  located ${MONGOD_CONFIG}"<br />    else<br />        echo "  unable to locate ${MONGOD_CONFIG}, exiting."<br />        exit 1<br />    fi<br />done</code></pre><br /><span>More sanity checking here. Before running any benchmarks we want to make sure that all the tar files and configuration files actually exist on the server. Nothing is more disappointing than starting a long running series of benchmarks only to come back in a day and find that some of them failed because of a type or missing file.</span> <br /><br /><pre><code>export DB_NAME=test<br />export NUM_CLIENTS=2<br />export DOCS_PER_CLIENT=$((512 * 80000))<br />export NUM_INSERTS=$((NUM_CLIENTS * DOCS_PER_CLIENT))<br />export SUMMARY_LOG_NAME=summary.log<br />rm -f ${SUMMARY_LOG_NAME}</code></pre><span><br /></span><span>This section allows some control over the benchmark itself, plus gives us information needed for interpreting some of the results.</span><br /><ul><li><span>DB_NAME = The MongoDB database we'll be inserting into.</span></li><li><span>NUM_CLIENTS = The number of simultaneous insert clients. You can set this to any value &gt;= 1, if you set it to &lt; 1 you'll still get a single insert client.</span></li><li><span>DOCS_PER_CLIENT = The number of documents a single client will insert. This is multiplied by NUM_CLIENTS to find the total number of inserts (NUM_INSERTS), and is needed to calculate inserts per second later in the script. This value of 512 * 80000 is taken directly from the Javascript code, I'd normally inject it for the benchmark but didn't due to a lack of time.</span></li><li><span>NUM_INSERTS = Total number of inserts for the benchmark, a cooler way to do this would be to get a count from the collection itself, but that might take a while if an exact count is important and the particular storage engine supports document level locking. And remember, benchmarking isn't always about being cool, efficiency counts too.</span></li><li><span>SUMMARY_LOG_NAME = A single log file that will contain all results, summarized. And yes, delete it if it exists.</span></li></ul><br /><pre><code>for thisBenchmark in ${benchmarkList}; do<br />    export TARBALL=$(echo "${thisBenchmark}" | cut -d';' -f1)<br />    export MINI_BENCH_ID=$(echo "${thisBenchmark}" | cut -d';' -f2)<br />    export MONGOD_CONFIG=$(echo "${thisBenchmark}" | cut -d';' -f3)<br />    export MONGO_TYPE=$(echo "${thisBenchmark}" | cut -d';' -f4)<br /><br />    echo "benchmarking tarball = ${TARBALL}"</code></pre><br /><span>Start the loop where we benchmark each scenario by grabbing each one and cutting it into the four components. Give the user a heads up as to which TARBALL we're benchmarking this time.</span><br /><br /><pre><code>    # clean up + start the new server<br />    <br />    pushd ${MONGO_DIR}<br />    if [ "$?" -eq 1 ]; then<br />        echo "Unable to pushd $MONGO_DIR, exiting."<br /> exit 1<br />    fi<br />    <br />    # erase any files from the previous run<br />    rm -rf *<br />    <br />    # untar server binaries to here<br />    tar xzvf ${tarDirectory}/${TARBALL} --strip 1<br />    <br />    # create the "data" directory<br />    mkdir data<br />    bin/mongod --config ${homeDirectory}/${MONGOD_CONFIG}<br />    popd</code></pre><br /><span>Did I mention how defensive I try to write these benchmarking scripts? Maybe paranoid is a better term. Earlier we confirmed that MONGO_DIR is defined, exists as a directory, and is empty. Guess what? Something might go terribly wrong during the benchmark and that might no longer be the case. So right after changing to the MONGO_DIR directory using pushd, check that pushd succeeded. Erase any existing files in the directory, untar the current benchmark's tarball, create a data folder, start MongoDB with the current scenario's configuration file, and popd back to our starting directory.</span><br /><br /><pre><code>    # wait for mongo to start<br />    while [ 1 ]; do<br />        $MONGO_DIR/bin/mongostat -n 1 &gt; /dev/null 2&gt;&amp;1<br />        if [ "$?" -eq 0 ]; then<br />            break<br />        fi    <br />        sleep 5<br />    done<br />    sleep 5</code></pre><span><br /></span><span>We are starting mongod forked, so the MongoDB server isn't yet available. This code executes until the mongostat utility returns data, letting us know that the server is running.</span><br /><span><br /></span><span>Any ideas on a cleaner way to do this?</span><br /><br /><pre><code>    # log for this run<br />    export LOG_NAME=${MINI_BENCH_ID}-${NUM_CLIENTS}-${NUM_INSERTS}.log<br />    rm -f ${LOG_NAME}</code></pre><span><br /></span><span>Create a custom log file for this particular scenario.</span><br /><br /><pre><code>    # TODO : log server performance with mongostat<br /></code></pre><span><br /></span><span>If you've ever attending one of my benchmark presentations you've likely heard me say that benchmarking is never done, there is always more to measure and analyze. This script currently records overall (cumulative) inserts per second, catching mongostat output along the way would allow for creating a pretty graph over time. I highly recommend picking a way to add "to-do" tasks to your scripts and code, mine is as simple as "TODO : ".</span><br /><br /><pre><code>    # start the first inserter<br />    T="$(date +%s)"<br />    echo "`date` | starting insert client 1" | tee -a ${LOG_NAME}<br />    $MONGO_DIR/bin/mongo ${DB_NAME} --eval 'load("./compress_test.js")' &amp;<br />    sleep 5</code></pre><span><br /></span><span>This particular benchmark is simple Javascript, so we execute it using the mongo shell. Prior to starting the client we grab the current time (probably the number of seconds since the epoch) so we can calculate the total inserts per second. I include a "sleep 5" after this first client since it might take a bit of time for the collection to get created, I've found it's always safest to let the first insert client get started on it's own.</span><br /><br /><span>Again, thanks to <a href="https://twitter.com/comerford">Adam</a> <a href="http://comerford.cc/">Comerford</a> for sharing <a href="https://comerford.cc/2015/02/04/mongodb-3-0-testing-compression/">this benchmark</a>. </span><br /><br /><pre><code>    # start the additional insert clients<br />    clientNumber=2<br />    while [ ${clientNumber} -le ${NUM_CLIENTS} ]; do<br />        echo "`date` | starting insert client ${clientNumber}" | tee -a ${LOG_NAME}<br />        $MONGO_DIR/bin/mongo ${DB_NAME} --eval 'load("./compress_test.js")' &amp;<br />        let clientNumber=clientNumber+1<br />    done</code></pre><br /><span>If we are running 2 or more insert clients then each gets started with this loop.</span><br /><br /><pre><code>    # wait for all of the client(s) to finish<br />    wait</code></pre><span><br /></span><span>I only learned about the wait command a few months ago, and it is extremely useful. It causes our script to pause (wait) until any children processes we created are finished. So for this example each of the insert clients will finish before the script continues.</span><br /><br /><pre><code>    # report insert performance<br />    T="$(($(date +%s)-T))"<br />    printf "`date` | insert duration = %02d:%02d:%02d:%02d\n" "$((T/86400))" "$((T/3600%24))" "$((T/60%60))" "$((T%60))" | tee -a ${LOG_NAME}<br />    DOCS_PER_SEC=`echo "scale=0; ${NUM_INSERTS}/${T}" | bc `<br />    echo "`date` | inserts per second = ${DOCS_PER_SEC}" | tee -a ${LOG_NAME}</code></pre><span><br /></span><span>Now that the inserts are finished we find the number of elapsed seconds by subtracting the current seconds (from the epoch) from our starting time. Calculating inserts per second is a simple as dividing the number of inserts by the number of seconds.</span><br /><br /><pre><code>    # stop the server<br />    T="$(date +%s)"<br />    echo "`date` | shutting down the server" | tee -a ${LOG_NAME}<br />    $MONGO_DIR/bin/mongo admin --eval "db.shutdownServer({force: true})"<br /><br />    # wait for the MongoDB server to shutdown<br />    while [ 1 ]; do<br />        pgrep -U $USER mongod &gt; /dev/null 2&gt;&amp;1<br />        if [ "$?" -eq 1 ]; then<br />            break<br />        fi    <br />        sleep 5<br />    done<br />    T="$(($(date +%s)-T))"<br />    printf "`date` | shutdown duration = %02d:%02d:%02d:%02d\n" "$((T/86400))" "$((T/3600%24))" "$((T/60%60))" "$((T%60))" | tee -a ${LOG_NAME}</code></pre><span><br /></span><span>Prior to calculating size on disk I like to stop the server, since that allows each storage engine to perform cleanup, flush old log files, and shut down cleanly. I also like to time the operation. It's always bothered me that the MongoDB server shutdown process is asynchronous, the client requesting the shutdown is immediately disconnected with an unfriendly warning message (which one might mistake for an error).</span><br /><span><br /></span><span>In any event, the loop immediately following the db.shutdownServer() call is there to wait for the mongod process to disappear. Until it does, MongoDB is not really stopped.</span><br /><span><br /></span><span>Any ideas on how to improve this?</span><br /><br /><pre><code>    # report size on disk<br />    SIZE_BYTES=`du -c --block-size=1 ${MONGO_DIR}/data | tail -n 1 | cut -f1`<br />    SIZE_MB=`echo "scale=2; ${SIZE_BYTES}/(1024*1024)" | bc `<br />    echo "`date` | post-load sizing (SizeMB) = ${SIZE_MB}" | tee -a ${LOG_NAME}</code></pre><span><br /></span><span>Find and report the total megabytes of the data directory (dbPath). I usually only report on the specific collection and it's indexes, this is simpler in that it includes the entire data directory.</span><br /><br /><pre><code>    # put all the information into the summary log file<br />    echo "`date` | tech = ${MINI_BENCH_ID} | ips = ${DOCS_PER_SEC} | sizeMB = ${SIZE_MB}" | tee -a ${SUMMARY_LOG_NAME}<br />done</code></pre><span><br /></span><span>Having all the results go to a single summary log file make it easy to interpret and graph your results.</span><br /><br /><br /><br /><span>So there you have it. Download the script and configuration files, make some changes, and run a few tests for yourself. Oh, give me some feedback if you can think of areas I can improve the above.</span><br /><br /><span><span>You are well on your way to your benchmarking black belt!</span></span><br /><br /><br /><i><b><span><span>Links to everything you'll need to try this at home.</span></span></b></i><br /><ul><li><span><span><a href="https://gist.github.com/tmcallaghan/4d15c18f9e34e1ea8a21">run.benchmark.bash</a> (the script we picked apart in this blog)</span></span></li><li><a href="https://gist.github.com/tmcallaghan/d6690529c62756ce32d7"><span><span>mmapv1.conf</span></span></a></li><li><a href="https://gist.github.com/tmcallaghan/68aa2ab4fb07cd138537"><span><span>wiredtiger-uncompressed.conf</span></span></a></li><li><a href="https://gist.github.com/tmcallaghan/3591fa29f8d9ce1e0178"><span><span>wiredtiger-snappy.conf</span></span></a></li><li><a href="https://gist.github.com/tmcallaghan/f068e414f6018185e92b"><span><span>wiredtiger-zlib.conf</span></span></a></li><li><a href="https://gist.github.com/tmcallaghan/e3fe0584263585cfaeac"><span><span>tokumxse-uncompressed.conf</span></span></a></li><li><a href="https://gist.github.com/tmcallaghan/81fef36f8ad94e462803"><span><span>tokumxse-quicklz.conf</span></span></a></li><li><a href="https://gist.github.com/tmcallaghan/a2aba6930e88e70a3ef6"><span><span>tokumxse-zlib.conf</span></span></a></li><li><a href="https://gist.github.com/tmcallaghan/d3964a28a775c57d3720"><span><span>tokumxse-lzma.conf</span></span></a></li><li><a href="https://gist.github.com/comerford/e5417b57d8b4691dc55c"><span><span>compress_test.js</span></span></a></li></ul>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.acmebenchmarking.com/2015/02/how-to-benchmark-mongodb.html"> on February 23, 2015 02:05 PM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.acmebenchmarking.com/2015/02/mongodb-v30-compression-benchmarks.html" title="Acme Benchmarking">MongoDB v3.0 Compression Benchmarks</a></h2>

<h3>Tim Callaghan</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<span>In my <a href="http://www.acmebenchmarking.com/2015/02/mongodb-storage-engine-shootout-round-1.html" target="_blank">last</a> <a href="http://www.acmebenchmarking.com/2015/02/mongodb-v3-se-shootout-1a.html" target="_blank">two</a> blogs, I compared the indexed insertion performance of the various <a href="http://www.mongodb.com/" target="_blank">MongoDB</a> v3.0 storage engines. It was interesting to see how they stacked up against each other, especially looking at the performance variability in each. Based on those results I expect the <a href="http://www.wiredtiger.com/" target="_blank">WiredTiger</a> and <a href="http://www.tokutek.com/" target="_blank">Tokutek</a> developers to continue improving their respective technologies throughout 2015, there is much to be done.</span><br /><div><span><br /></span></div><div><span>But enough about those benchmarks, it's time for a good-old-fashioned compression test. I enjoy testing compression since it is extremely scientific. You simply insert a bunch of data then measure the size on disk. I decided to do two separate compression tests: one from <a href="http://comerford.cc/" target="_blank">Adam</a> <a href="https://twitter.com/comerford" target="_blank">Comerford</a> of MongoDB and the other a simple mongoimport test of an easily accessible data set.</span></div><div><span><br /></span></div><div><span>First up is a benchmark that was originally created and presented on <a href="http://comerford.cc/wordpress/2015/02/04/mongodb-3-0-testing-compression/" target="_blank">Adam Comerford's blog</a>. <i>I owe Adam a word for thanks as I didn't realize that MongoDB supports YAML based configuration files until I read his blog, this made my testing so much simpler.</i>&nbsp;Adam created a <a href="https://gist.github.com/comerford/e5417b57d8b4691dc55c" target="_blank">small amount of Javascript</a> to insert the data and recorded both size on disk and overall insert performance for his tests.</span></div><div><span><br /></span></div><div><span>Here are the results executed on my benchmark machine, now including the results for the TokuMXse storage engine as well.</span></div><div><span><br /></span></div><div class="separator"><a href="http://4.bp.blogspot.com/-vCqIGlS1Ar4/VNkC2IeMsJI/AAAAAAAAB78/7E-Ulo6b_WM/s1600/se-shootout-02-adamc-compression-size.png"><img border="0" src="http://4.bp.blogspot.com/-vCqIGlS1Ar4/VNkC2IeMsJI/AAAAAAAAB78/7E-Ulo6b_WM/s1600/se-shootout-02-adamc-compression-size.png" height="426" width="640" /></a></div><div><span><br /></span></div><div><span>Size-wise, TokuMXse is 8.2% smaller than WiredTiger using zlib compression. This is interesting, especially since WiredTiger is using zlib level 6, versus TokuMXse using zlib level 5. The higher the number, and it goes up to 9, the more aggressive the compression. TokuMXse with lzma compression is far and away the winner here, as the on-disk size is 31.1% smaller than WiredTiger/zlib.</span></div><div><span><br /></span></div><blockquote class="tr_bq"><span><i><b>Note</b>: it's important to keep in mind the balance between compression and performance, especially when it comes to query performance. The more aggressive the compression, like lzma versus zlib, or the more aggressive the zlib level, the longer it takes to decompress the data for reads/updates.</i></span></blockquote><div><span>And for that same benchmark, here are the insert performance numbers measured in documents inserted per second.</span></div><div><span><br /></span><br /><div class="separator"><a href="http://1.bp.blogspot.com/-37s7efPfxVs/VNn_WRixrxI/AAAAAAAAB8s/lYij_8g_gLs/s1600/se-shootout-02-adamc-compression-speed.png"><img border="0" src="http://1.bp.blogspot.com/-37s7efPfxVs/VNn_WRixrxI/AAAAAAAAB8s/lYij_8g_gLs/s1600/se-shootout-02-adamc-compression-speed.png" height="426" width="640" /></a></div><span><br /></span></div><div class="separator"></div><div><span>The only surprise here is that WiredTiger with zlib is so much slower than WiredTiger with snappy, specifically 35.1%. I'm curious to understand why that is the case. Anyone?</span><br /><br /><blockquote class="tr_bq"><i><b><span>Edit : 2015-02-10 : As Adam Comerford pointed out in Twitter and in a comment to this blog, the performance issue is in WiredTiger's zlib compression on the journal. A repeat test without journal compression showed good insert performance (similar to none and snappy).</span></b></i></blockquote></div><div><span><br /></span></div><div><span>My second compression benchmark uses a data set available from <a href="https://twitter.com/andy_pavlo" target="_blank">Andy Pavlo's</a> <a href="http://www.cs.cmu.edu/~pavlo/datasets/index.html" target="_blank">Collected Data Sets</a>, specifically the <a href="http://www.cs.brown.edu/~pavlo/torrent/peersnapshots-01.csv.gz" target="_blank">BitTorrent Peer Snapshot Part 1</a>. For each test I start with an empty MongoDB server with the appropriate storage engine and, if applicable, compression settings. I think this data set is more real-world than Adam's Javascript data generator.</span></div><div><span><br /></span></div><div><span>First up are the compression numbers.</span></div><div><span><br /></span></div><div class="separator"><a href="http://1.bp.blogspot.com/-AP6ez18phq0/VNkFtBph-9I/AAAAAAAAB8U/51I0Xn3pDqs/s1600/se-shootout-02-andyp-compression-size.png"><img border="0" src="http://1.bp.blogspot.com/-AP6ez18phq0/VNkFtBph-9I/AAAAAAAAB8U/51I0Xn3pDqs/s1600/se-shootout-02-andyp-compression-size.png" height="426" width="640" /></a></div><div><span><br /></span></div><div><span>Again, the TokuMXse/zlib size on disk was smaller than WiredTiger/zlib, this time at 7%. TokuMXse/lzma was 28% smaller than WiredTiger/zlib.</span></div><div><span><br /></span></div><div><span>Most interesting to me were the performance results for this test, again measured in number of documents inserted per second.</span></div><div><span><br /></span></div><div class="separator"></div><div><div class="separator"><a href="http://1.bp.blogspot.com/-O9C4tLKdaBY/VNn_fjNMa6I/AAAAAAAAB80/8LrxBIF6TTg/s1600/se-shootout-02-andyp-compression-speed.png"><img border="0" src="http://1.bp.blogspot.com/-O9C4tLKdaBY/VNn_fjNMa6I/AAAAAAAAB80/8LrxBIF6TTg/s1600/se-shootout-02-andyp-compression-speed.png" height="426" width="640" /></a></div><span><br /></span></div><div><span>In this test WiredTiger was 55% slower using zlib than snappy. TokuMXse performance was relatively unchanged throughout the test.</span></div><div><span><br /></span></div><div><span>To recap from these benchmarks:</span></div><div><ul><li><span>TokuMXse is around 7% smaller than WiredTiger with zlib, and substantially smaller with lzma.</span></li><li><span>WiredTiger insertion performance is significantly slower with zlib than snappy. Side note, I see nothing in the <a href="https://jira.mongodb.org/" target="_blank">MongoDB Jira</a> in regards to this issue.</span></li><li><span>MMAPv1 insert performance was the best of the bunch, but it's size-on-disk is just awful.</span></li></ul><div><span>Next week I'm going to publish and explain the <i><u>simple</u></i> bash scripts I created for this round of benchmarks.</span></div></div>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.acmebenchmarking.com/2015/02/mongodb-v30-compression-benchmarks.html"> on February 10, 2015 03:30 PM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.acmebenchmarking.com/2015/02/mongodb-v3-se-shootout-1a.html" title="Acme Benchmarking">MongoDB v3.0 Storage Engine Shootout : Round 1a : WiredTiger and directIO</a></h2>

<h3>Tim Callaghan</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<span>In <a href="http://www.acmebenchmarking.com/2015/02/mongodb-storage-engine-shootout-round-1.html" target="_blank">round 1</a> of the MongoDB v3.0 Storage Engine Shootout I noticed that <a href="http://www.wiredtiger.com/" target="_blank">WiredTiger</a> did something totally unexpected, it performed better with the OpLog enabled than it did with the OpLog disabled. This is peculiar, as enabling the OpLog forces MongoDB to maintain an additional collection of all insert/update/delete operations on the server, plus the collection is capped which is a no simple feat for WiredTiger's B-tree (it's a similarly painful for <a href="http://www.tokutek.com/" target="_blank">Tokutek's</a> <a href="http://en.wikipedia.org/wiki/Fractal_tree_index" target="_blank">Fractal Tree</a>).</span><br /><span><br /></span><span>My benchmark server has 32GB of RAM. Since these first few benchmark rounds are not intended to be in-memory benchmarks I need to make sure the working set of data far exceeds the cache size so I can induce IO. The easiest way to do that on storage engines that support a defined cache size is to use directIO. With directIO the filesystem writes are not cached in the OS buffers, so setting a small cache for the particular storage engine works fine. In my testing I use 8GB as the cache size. Unfortunately the MMAPv1 storage engine uses memory mapped files and doesn't have a specific parameter to limit the amount of memory it uses, it just uses all available memory in your server. I get around this by executing a program prior to my benchmark that exclusively grabs a set amount of memory on the server, leaving just 12GB available to the MMAPv1 engine. My long term plan is to measure the performance loss when using something like <a href="http://www.docker.com/" target="_blank">Docker</a> and run the servers in containers.</span><br /><span><br /></span><span>The WiredTiger storage engine in MongoDB v3.0 is highly configurable. A small number of these configuration options have been exposed and are easily set via the command line when starting up the server. Enabling directIO requires passing a specific configuration string that is not exposed as one of these options, specifically "--wiredTigerEngineConfigString direct_io=[data]".</span><br /><blockquote class="tr_bq"><i><span><b>My current advice to everyone is that if you can't set a WiredTiger parameter with a simple defined command line argument, then don't do it. <u>They are hard to set for a reason</u>, leave the string based arguments alone!</b></span></i></blockquote><span></span><br /><span>I've changed my benchmarking methodology. Going forward I'm booting my server with the "mem=" argument, thus limiting the server to 12GB of total memory. Also, all go-forward benchmarks with WiredTiger will be using bufferedIO. At some point the MongoDB/WiredTiger team will improve the directIO implementation and I'll rerun my tests and share the results.</span><br /><span><br /></span><span>So here are the results of the rerun. First up is WiredTiger with the OpLog on and off. Results are now more explainable than before, but still odd. With directIO, WiredTiger was faster with the OpLog on than off. Now the performance is similar with the OpLog enabled and disabled. Again, not exactly what I expected but better than before.</span><br /><span><br /></span><br /><div class="separator"><a href="http://4.bp.blogspot.com/-vNBhpsRRNxY/VNN1NA_wEuI/AAAAAAAAB7M/zI-R_oYnCy8/s1600/iibench-tps-se-shootout-01a-wiredtiger-12-0-all.png"><img border="0" src="http://4.bp.blogspot.com/-vNBhpsRRNxY/VNN1NA_wEuI/AAAAAAAAB7M/zI-R_oYnCy8/s1600/iibench-tps-se-shootout-01a-wiredtiger-12-0-all.png" height="480" width="640" /></a></div><span><br /></span><span>So replacing the prior bufferedIO run we have a new comparison graph showing TokuMXse vs. WiredTiger vs. MMAPv1.</span><br /><span><br /></span><br /><div class="separator"><a href="http://4.bp.blogspot.com/-VYYXN9KNzjI/VNN1cV6K3II/AAAAAAAAB7U/A8uDCLMYTY4/s1600/iibench-tps-se-shootout-01a-all.png"><img border="0" src="http://4.bp.blogspot.com/-VYYXN9KNzjI/VNN1cV6K3II/AAAAAAAAB7U/A8uDCLMYTY4/s1600/iibench-tps-se-shootout-01a-all.png" height="480" width="640" /></a></div><span><br /></span><span>Still a convincing victory for TokuMXse. The low points in TokuMXse are above the peak performance of WiredTiger, and MMAPv1 is just a mess (there are 10 second intervals where MMAPv1 inserted less than 100 documents).</span><br /><span><br /></span><span>A <a href="http://comerford.cc/wordpress/2015/02/04/mongodb-3-0-testing-compression/" target="_blank">recent blog on compression size and performance</a> by <a href="https://twitter.com/comerford" target="_blank">Adam Comerford</a> of MongoDB has distracted me, so round 2 will commence on Monday and focus on compression.</span><br /><span><br /></span>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.acmebenchmarking.com/2015/02/mongodb-v3-se-shootout-1a.html"> on February 05, 2015 10:57 AM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.acmebenchmarking.com/2015/02/mongodb-storage-engine-shootout-round-1.html" title="Acme Benchmarking">MongoDB Storage Engine Shootout : Round 1 : Indexed Insertion</a></h2>

<h3>Tim Callaghan</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<div class="separator"><span><a href="http://1.bp.blogspot.com/-CgPt9dgZ9LA/VNDVgbvT4sI/AAAAAAAAB50/QcWVFjcVZgk/s1600/storage-engine-wars.png"><img border="0" src="http://1.bp.blogspot.com/-CgPt9dgZ9LA/VNDVgbvT4sI/AAAAAAAAB50/QcWVFjcVZgk/s1600/storage-engine-wars.png" height="163" width="320" /></a></span></div><span>The next release of MongoDB includes the ability to select a storage engine, the goal being that different storage engines will have different capabilities/advantages, and user's can select the one most beneficial to their particular use-case. <i><b>Storage engines are cool.</b></i> MySQL has offered them for quite a while. One very big difference between the MySQL and MongoDB implementations is that in MySQL the user gets to select a particular storage engine for each table, whereas in MongoDB it's a choice made at server startup. You get a single storage engine for everything on the particular mongod instance. I see pros and cons to each decision, but that's a blog for another day.</span><br /><br /><span>In <a href="http://www.mongodb.com/blog/post/renaming-our-upcoming-release-mongodb-30">MongoDB 3.0</a> (yes it was going to be 2.8 but now it's 3.0, <i>get over it</i>) the existing storage engine technology, named MMAPv1, is the default but MongoDB will support the <a href="http://www.mongodb.com/press/wired-tiger">acquired WiredTiger</a> storage engine to a limited degree. As I understand it the B-tree implementation is supported, but LSM is not. Eliot has publicly stated that the long term goal is for WiredTiger to be fully supported and the default, and eventually MMAPv1 will be deprecated. The only other currently viable storage engine is <a href="http://www.tokutek.com/">Tokutek's</a> <a href="http://www.tokutek.com/2015/01/announcing-tokumxse-v1-0-0-rc-0/">TokuMXse</a>, which must be obtained directly from Tokutek. The <a href="https://github.com/facebook/rocksdb">RocksDB</a> team at <a href="http://www.facebook.com/">Facebook</a> is also working on a storage engine, but the release date doesn't seem to be any time soon.</span><br /><span></span><br /><span><br /></span><span>But enough of this overview, this series of blogs is all about picking a benchmark, explaining what it's doing, running it on all storage engines, and selecting a winner for the "round".</span><br /><span><br /></span><span>First up is iiBench, the indexed insertion benchmark. It's easy to grab and run the code for yourself via <a href="https://github.com/tmcallaghan/iibench-mongodb">GitHub</a>. The benchmark itself was created to show how well systems perform in a pure insertion workload to a single collection with 3 secondary indexes, all of which are on fields with random insertion patterns.</span><br /><span><br /></span><span>I ran iiBench with defaults except for the following, all of which can be easily modified via the run.simple.bash script:</span><br /><ul><li><span>4 concurrent insert threads (WiredTiger and TokuMXse support document level locking so they should have an advantage here)</span></li><li><span>250 documents per inserted batch</span></li><li><span>benchmark duration = 60 minutes</span></li><li><span>1 additional character field, 1000 bytes, 75% compressible</span></li><li><span>Every 10 second interval the number of completed inserts is reported </span></li></ul><span>All benchmarks were run on AcmeBenchmarking server #1 (ab01), I'll be creating a "server details" page soon, but for the time being:</span><br /><ul><li><span><span>Intel 4790K CPU (quad core + hyperthreading)</span></span></li><li><span>32GB DDR3 RAM</span></li><li><span>LSI 9280 RAID : 512MB cache, Samsung 830 256GB SSD</span></li><ul><li><span>Lots of IOPs and bandwidth </span></li></ul><li><span>Ubuntu 14.04 </span></li></ul><span>The goal is to show how the system behave long-term, not just for a short amount of time. The easiest way to do that is to get data &gt; RAM, so...</span><br /><ul><li><span>For WiredTiger and TokuMXse I configured cache size to 8GB and forced directIO.</span></li><li><span>For MMAPv1 I run an additional process on the server that uses/locks all but 12GB of RAM, so the mongod server doesn't have access to the entire 32GB. I plan on running via docker at some point so I can simply expose less memory to the server, but this technique has served me well so far.</span></li></ul><span>&nbsp;And lastly, the MongoDB versions:</span><br /><ul><li><span>mongodb-linux-x86_64-3.0.0-rc7.tgz</span></li><li><span>mongodb-linux-x86_64-tokumxse-20150123e.tgz</span></li></ul><b><span>First benchmark run, OpLog is on.</span></b><br /><br /><div class="separator"><a href="http://1.bp.blogspot.com/-aLNTFWf1y_g/VNDZQy2ynTI/AAAAAAAAB6A/ZJOZmTO2fiE/s1600/iibench-tps-se-shootout-01-all-oplog-on.png"><img border="0" src="http://1.bp.blogspot.com/-aLNTFWf1y_g/VNDZQy2ynTI/AAAAAAAAB6A/ZJOZmTO2fiE/s1600/iibench-tps-se-shootout-01-all-oplog-on.png" height="480" width="640" /></a></div><br /><span>A couple of interesting things to note here:</span><br /><ul><li><span>The WiredTiger B-tree becomes IO bound for secondary index maintenance quickly.</span></li><li><span>MMAPv1 comes close to the 0-line 30 minutes in. The 0-line is bad, it means there were no inserts completed in the measured 10 second interval.</span></li></ul><span>After looking at this graph I wanted to understand the performance impact of the OpLog. For those new to MongoDB the OpLog is where completed operations are logged for replay into secondaries (replication). The OpLog can also be tailed or queried just like any other collection in MongoDB. The OpLog is a capped collection, meaning it's kept at a fixed size, which is hard for B-tree/Fractal Tree backed engined like WiredTiger and TokuMXse to maintain. So here is the rerun of the benchark with the OpLog disabled (off).</span><br /><br /><span><b><span>Second benchmark run, OpLog is off.</span></b> </span><br /><br /><div class="separator"><a href="http://1.bp.blogspot.com/-ubuAWs8tXg0/VNDeYmaDNaI/AAAAAAAAB6Q/loXS8BXv2dU/s1600/iibench-tps-se-shootout-01-all-oplog-off.png"><img border="0" src="http://1.bp.blogspot.com/-ubuAWs8tXg0/VNDeYmaDNaI/AAAAAAAAB6Q/loXS8BXv2dU/s1600/iibench-tps-se-shootout-01-all-oplog-off.png" height="480" width="640" /></a></div><br /><span>The performance changes are interesting, and worth looking at in isolation of each engine. I've regraphed each result, showing the particular engine with the OpLog on and off, and started the graph at the 1200 second mark to eliminate the early in-memory noise.</span><br /><span><br /></span><span><b>MMAPv1</b>&nbsp;</span><br /><span>The 0-line issues went away after disabling the OpLog. This is nice to see, but running a server without an OpLog is a really bad idea if you care about your data. With the OpLog enabled it's performance variability is concerning.</span><br /><span><br /></span><br /><div class="separator"><a href="http://1.bp.blogspot.com/-_UtRI9S1A4I/VNDfkG10UoI/AAAAAAAAB6c/xnxGtGMvSDc/s1600/iibench-tps-se-shootout-01-mmapv1.png"><img border="0" src="http://1.bp.blogspot.com/-_UtRI9S1A4I/VNDfkG10UoI/AAAAAAAAB6c/xnxGtGMvSDc/s1600/iibench-tps-se-shootout-01-mmapv1.png" height="480" width="640" /></a></div><span><br /></span><span><b>TokuMXse</b>&nbsp;</span><br /><span>As with MMAPv1, performance improved after disabling the OpLog.</span><br /><span><br /></span><br /><div class="separator"><a href="http://4.bp.blogspot.com/-wneYOSNRtvY/VNDhDqgDyOI/AAAAAAAAB64/aLJaSTWeAzc/s1600/iibench-tps-se-shootout-01-tokumxse(1).png"><img border="0" src="http://4.bp.blogspot.com/-wneYOSNRtvY/VNDhDqgDyOI/AAAAAAAAB64/aLJaSTWeAzc/s1600/iibench-tps-se-shootout-01-tokumxse(1).png" height="480" width="640" /></a></div><div class="separator"><a href="http://3.bp.blogspot.com/-0wNEr4NtTOw/VNDfsOPEGHI/AAAAAAAAB6k/b2InpKon2I0/s1600/iibench-tps-se-shootout-01-tokumxse.png"><br /></a></div><span></span><br /><span><b>WiredTiger</b>&nbsp;</span><br /><span>This is a real head-scratcher. Performance peaks were actually higher (almost 2x) with the OpLog enabled, even though the server needs to maintain an additional collection. Ideas?</span><br /><span><br /></span><br /><div class="separator"><a href="http://2.bp.blogspot.com/-oQxcZu5vczo/VNDfwelfcQI/AAAAAAAAB6s/vi1ZSRs5uQk/s1600/iibench-tps-se-shootout-01-wiredtiger.png"><img border="0" src="http://2.bp.blogspot.com/-oQxcZu5vczo/VNDfwelfcQI/AAAAAAAAB6s/vi1ZSRs5uQk/s1600/iibench-tps-se-shootout-01-wiredtiger.png" height="480" width="640" /></a></div><span><br /></span><b><span><span>Round 1 Recap</span></span></b><br /><span></span><br /><br /><span>Round 1 goes</span><span> to TokuMXse, it's performance is substantially higher in both the OpLog enabled and OpLog disabled tests. <i>I'm hoping that someone can explain to me the reason WiredTiger performed better with the OpLog enabled.</i></span><br /><span><br /></span><span>In Round 2 I'll add queries to the above workload to see how that impacts the insertion performance.</span>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.acmebenchmarking.com/2015/02/mongodb-storage-engine-shootout-round-1.html"> on February 03, 2015 03:12 PM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.acmebenchmarking.com/2015/01/todo-so-long-and-thanks-for-all-help.html" title="Acme Benchmarking">So long, and thanks for all the help.</a></h2>

<h3>Tim Callaghan</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<br /><div class="separator"><a href="http://3.bp.blogspot.com/-xUqhWzPeRZI/VLe8vRP8GqI/AAAAAAAAB5U/MhWGyiTVIFo/s1600/fish.jpg"><img border="0" src="http://3.bp.blogspot.com/-xUqhWzPeRZI/VLe8vRP8GqI/AAAAAAAAB5U/MhWGyiTVIFo/s1600/fish.jpg" height="320" width="213" /></a></div><div><span><br /></span></div><div><span>Today is my last day at <a href="http://www.tokutek.com/" target="_blank">Tokutek</a>. On Monday I'm starting a new opportunity as VP/Technology at <a href="http://www.crunchtime.com/" target="_blank">CrunchTime!</a>. If you are a web developer, database developer, or quality assurance engineer in the Boston area and looking for a new opportunity please contact me or visit the <a href="http://www.crunchtime.com/about-us/careers/" target="_blank">CrunchTime! career page</a>.</span><br /><span><br /></span><span>I've really enjoyed my time at VoltDB and Tokutek. Working for <a href="http://en.wikipedia.org/wiki/Michael_Stonebraker">Mike Stonebraker</a>&nbsp;(at VoltDB) was on my career "bucket list" and in these past 3.5 years at Tokutek I've experienced the awesomeness of the MySQL ecosystem and the surging NoSQL database market.</span><br /><span><br /></span><span>But now I'm ready to going back to consuming databases, not creating them. It's probably a good idea for anyone in technology to work on the other side of the producer/consumer line, nothing beats "real-world" experience.</span><br /><span><br /></span><span>If you like what I've been writing here then keep this blog in your RSS reader and/or follow me on twitter using </span><a href="https://twitter.com/tmcallaghan" target="_blank">@tmcallaghan</a><span> and </span><a href="https://twitter.com/acmebench" target="_blank">@acmebench</a><span>. My goal is to keep blogging twice a month.&nbsp;</span><br /><blockquote class="tr_bq"><span><i>I hope there will be less concern about my benchmarking objectivity, as I'll no longer have a stake in the benchmarked technologies.</i></span></blockquote></div><div><span>Here are some topics I've already started working on:</span><br /><ul><li><span>Creating a brand new benchmark that has more "real world" appeal than existing ones. I've stared work on an "email server" workload with operations like send, read, label, search, forward, reply, etc. MongoDB has <a href="http://www.mongodb.com/presentations/mythbusting-understanding-how-we-measure-performance-mongodb-0" target="_blank">hinted</a> that they are creating an "inbox fan in/out" benchmark which might be similar (I wish they'd make it public), mine will be available on <a href="https://github.com/tmcallaghan" target="_blank">my GitHub</a> and will likely support multiple database technologies.</span></li><li><span>A lot of MongoDB 2.8 benchmarks comparing the MMAPV1, <a href="http://www.wiredtiger.com/" target="_blank">WiredTiger</a>, and <a href="http://www.tokutek.com/" target="_blank">TokuMXse</a> Storage Engines as well as <a href="http://www.tokutek.com/tokumx-for-mongodb/" target="_blank">TokuMX</a>.</span></li><li><span><a href="http://www.mysql.com/" target="_blank">MySQL</a> benchmarks including TokuDB plus the 5.7 enhancements.</span></li><li><span>Analysis of good and bad benchmarks that I've seen. Please email me or comment on this blog if you've found an interesting new benchmark that could use review.</span></li><li><span>And, of course, blogs about the practice of benchmarking itself.</span></li></ul></div><div><span>Lastly, I'd like to give a shout out to many people and companies I've worked with over the past 6+ years. You've all been welcoming and supportive, and really made my job fun. Specifically,</span></div><ul><li><span>My <a href="http://www.dbms2.com/2010/05/25/voltdb-finally-launches/" target="_blank">"greater-known but nonethemore smart"</a> brother, <a href="http://smalldatum.blogspot.com/" target="_blank">Mark Callaghan</a>.</span></li><li><span>My amazing team at <a href="http://www.tokutek.com/" target="_blank">Tokutek</a>, a true bunch of wall-breakers.&nbsp;I'll miss Rich, Zardosht, Leif, John, Christian, Joel, Joe, and Abdelhak.</span></li><li><a href="https://www.linkedin.com/in/gnarvaja" target="_blank">Gerry</a><span> and </span><a href="https://www.linkedin.com/in/sheeri" target="_blank">Sheeri</a><span> for letting me sing the jingle on the </span><a href="http://www.oursql.com/" target="_blank">OurSQL Podcast</a><span>.</span></li><li><span>The entire MySQL ecosystem.</span></li><li><span><a href="http://www.percona.com/about-us/our-team/vadim-tkachenko">Vadim</a>, <a href="http://www.percona.com/about-us/our-team/peter-zaitsev">Peter</a>, and <a href="https://www.linkedin.com/in/jrobyoung">Rob</a> at Percona,&nbsp;<a href="https://www.linkedin.com/in/amrith" target="_blank">Amrith</a> at <a href="http://tesora.com/" target="_blank">Tesora</a>, <a href="https://www.linkedin.com/pub/robert-hodges/3/568/a64" target="_blank">Robert</a> at <a href="http://continuent.com/" target="_blank">Continuent</a> [now VMware], <a href="https://www.linkedin.com/in/xaprb" target="_blank">Baron</a> at <a href="http://www.vividcortex.com/" target="_blank">VividCortex</a>, <a href="https://www.linkedin.com/in/shlominoach">Shlomi</a> at <a href="http://www.booking.com/">Booking.com</a>, and <a href="https://www.linkedin.com/pub/henrik-ingo/3/232/8a7">Henrik</a> at <a href="http://www.mongodb.com/">MongoDB</a>.</span></li><li><span>Too many to people to name from&nbsp;</span><a href="http://www.percona.com/" target="_blank">Percona</a><span>,&nbsp;</span><a href="http://www.mysql.com/" target="_blank">Oracle/MySQL</a><span>,&nbsp;</span><a href="http://dbhangops.github.io/" target="_blank">DbHangOps</a><span>, &nbsp;</span><a href="http://voltdb.com/" target="_blank">VoltDB</a><span>, and&nbsp;</span><a href="https://mariadb.com/" target="_blank">MariaDB</a><span>.</span></li><li><span>And lastly thanks to everyone who has attended one of my webinars or presentations, commented on my blogs, or used TokuDB/TokuMX (commercially or community).</span></li></ul><div><span>My personal email address is available by clicking the disclaimer widget on the right hand side of the screen.</span><br /><span><br /></span><span>So long... and stay tuned.&nbsp;</span></div>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.acmebenchmarking.com/2015/01/todo-so-long-and-thanks-for-all-help.html"> on January 23, 2015 04:01 PM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.acmebenchmarking.com/2015/01/should-vegetarians-open-steakhouse.html" title="Acme Benchmarking">Should vegetarians open steakhouse restaurants?</a></h2>

<h3>Tim Callaghan</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<i><span>"Should vegetarians open steakhouse restaurants?"</span></i><br /><span><br /></span><span>Though someone will probably give me several examples of why they should, I'll argue that they absolutely should not. How can someone who doesn't eat steak convince others to eat at their "steak-only" restaurant?</span><br /><span><br /></span><span>But this is something a "professional technology benchmarker" (PTB) struggles with on a regular basis. Hello, I'm Tim Callaghan, and I'm a PTB.</span><br /><blockquote class="tr_bq"><span><b>professional technology benchmarker</b>, or PTB (noun) : One who compares two technologies as part of their job. One of these technologies is usually the product of the PTB's employer, the other is almost always not.</span></blockquote><span>In a past experience I was tasked with comparing the performance of a fully in-memory database with Oracle and MySQL on a "TPC-C like" workload. At the time I was an Oracle expert and working for the in-memory database company, but had never started a single MySQL server in my life. At Tokutek I've run numerous comparisons of TokuDB and TokuMX against InnoDB and MongoDB. In fact it's a large part of my job, and something I really enjoy.</span><br /><span><br /></span><span>In benchmarking competing technologies I <b><u>always</u></b> follow the same exact process:</span><br /><br /><ol><li><span>Decide which competitive advantage to showcase (keep it simple).</span></li><li><span>Build the benchmark (borrow from existing apps).</span></li><li><span>Execute the benchmark (record everything).</span></li><li><span>Publish and explain the results (blog and encourage feedback).</span></li></ol><div><span>Step 3 is where I'm always overly cautious. Here's a punch list of rules I follow:</span></div><div><ul><li><span>To the best extent possible, make sure that the benchmark environment is fair to everyone.</span></li><ul><li><span>Nothing invalidates results faster than a misconfigured system.</span></li></ul><li><span>Capture all details about the environment and publish them in your results.</span></li><ul><li><span>Hardware, operating system, configuration parameters.</span></li></ul><li><span>Get advice from the experts on any technology you aren't an expert in.</span></li><ul><li><span>Minimally, show them the results of your benchmark and ask for feedback prior to publishing.</span></li></ul></ul><div><span>In my opinion, this last bullet is the most important one. When I first started at Tokutek I was asked to improve the benchmarking. Tokutek's only product at the time was TokuDB, a MySQL storage engine competing with InnoDB. There were several resources at Tokutek to help me configure TokuDB, but InnoDB was another story. I needed to configure a few brand new servers and get benchmarking immediately.&nbsp;</span></div><div><span><br /></span></div><div><span>Did I tear open the server boxes and run benchmarks? Nope. Rather, I called <a href="http://smalldatum.blogspot.com/" target="_blank">my brother</a>. He told me to reach out to <a href="https://twitter.com/datacharmer" target="_blank">Giuseppe Maxia</a> (<a href="http://datacharmer.blogspot.com/" target="_blank">The Data Charmer</a>) about optimally configuring CentOS servers and <a href="https://twitter.com/vadimtk" target="_blank">Vadim Tkachenko</a> (<a href="http://www.percona.com/blog/author/vadim/" target="_blank">MySQL Performance Expert</a>) about configuring InnoDB for performance.</span></div></div><div><span><br /></span></div><div><span>Prior to reaching out to Giuseppe and Vadim, I did my homework by reading as much of their web based content as possible. I then sent them emails asking for assistance, and was amazed at how much they were willing to help. That was over 3 years ago and they are still helpful whenever I have a question.</span></div><div><span><br /></span></div><div><span>So where am I going with this?</span></div><div><span><br /></span></div><div><span>I recently wrote a <a href="http://www.acmebenchmarking.com/2015/01/can-we-improve-current-state-of.html" target="_blank">blog</a>&nbsp;titled "Can we improve the current state of benchmarking?". In it I proposed ways to improve the process of technology benchmarking, primarily peer review. I discussed a mistake in the implementation of the <a href="http://stssoft.com/" target="_blank">STSsoft</a> <a href="http://stssoft.com/products/stsdb-4-0/benchmark/" target="_blank">Database Benchmark</a>, specifically how it was incorrectly checking size for TokuDB. The benchmark code was checking uncompressed size, not compressed. A simple error, and one that could have easily been reviewed and discussed prior to the putting marketing claims around compression on their website.</span></div><div><span><br /></span></div><div><span>Equally concerning to me in the <a href="http://stssoft.com/products/stsdb-4-0/benchmark/#tests-on-hdd-drives" target="_blank">benchmark results</a> was the insertion performance of TokuDB. The <a href="http://stssoft.com/products/stsdb-4-0/" target="_blank">STSdb product page</a> claims a "10x performance improvement" over Fractal Trees. Even though the particular benchmark workload was a random insertion pattern, the TokuDB "REPLACE INTO" optimization should have handled it with ease. Granted, the hardware for the test was not ideal as an Intel Celeron processor and single 500G 7.2K SATA hard drive.&nbsp;</span></div><div><span><br /></span></div><div><span>So I dug in and read the benchmark code some more...</span></div><div><span><br /></span></div><div><span>In their performance chart it shows STSdb 4.0 inserting at a very high rate of speed, the exit throughput looks to be just above 50000 inserts per second. The TokuDB insert performance is horribly low, it's hard to read on the graph but I'd estimate it to be around 1500 inserts per second.</span></div><div><span><br /></span></div><div><span>In reading the benchmark code I found the bottleneck for TokuDB's performance was the IO performance. In my test a single SATA drive showed nearly 100% IO utilization. By default, TokuDB runs fully durable meaning that every commit is followed by an fsync() operation. I'm not sure what the STSdb durability guarantee is (I'm the vegetarian in their steakhouse), but given that their documentation states that ACID is on the road-map I find it hard to believe they are performing fsync() for each commit, nor do I understand what an STSdb commit even is. I'm confident that a consumer grade SATA drive isn't going to perform more than ~100 IOPs.</span></div><div><span><br /></span></div><div><span>So I ran two tests. One was to shutoff fsync-on-commit behavior in TokuDB. And the benchmark ran much faster. But I like the D in ACID, so I modified the benchmark application to perform 10000 inserts per batch instead of 1000, which reduces the number of fsync() operations by 90%. The results are dramatic.</span></div><blockquote class="tr_bq"><span><i>Note that I'm running on TokuDB v7.5.3 for MySQL 5.5.40, stock defaults (no TokuDB variables defined in my.cnf other than a 256M cache and directIO), on an Ubuntu 14.04 desktop with a Core i7-4790K, 32GB RAM, and an Intel 480GB SSD. The benchmark client is running in a Windows 7 Virtual Machine (VMware Workstation 11.0) on the Ubuntu desktop.</i></span></blockquote><div><span>Insert performance, 100 million rows, random keys, <b>1000</b> inserts per batch.</span></div><div><span><br /></span></div><div class="separator"><a href="http://3.bp.blogspot.com/-JwTRrOAWxpA/VK6NxRSfQII/AAAAAAAAB4U/Ey6XAx7_jAw/s1600/1000-per-batch.png"><img border="0" src="http://3.bp.blogspot.com/-JwTRrOAWxpA/VK6NxRSfQII/AAAAAAAAB4U/Ey6XAx7_jAw/s1600/1000-per-batch.png" height="166" width="640" /></a></div><div><span><br /></span></div><div><span>Insert performance, 100 million rows, random keys, <b>10000</b> inserts per batch.</span></div><div><span><br /></span></div><div class="separator"><a href="http://3.bp.blogspot.com/-SZAVqIOyIVU/VK6N4KDfdWI/AAAAAAAAB4c/WqwkWjbbWKE/s1600/10000-per-batch.png"><img border="0" src="http://3.bp.blogspot.com/-SZAVqIOyIVU/VK6N4KDfdWI/AAAAAAAAB4c/WqwkWjbbWKE/s1600/10000-per-batch.png" height="166" width="640" /></a></div><div><span><br /></span></div><div><span>Increasing the batch size from 1000 to 10000 improved TokuDB insert throughput over 3x. This is largely explained by the fact that a single SATA disk offers low IOPs, so the fsync operations were gating performance with smaller batches. Disabling fsync-on-commit makes it run even faster.</span></div><blockquote class="tr_bq"><span><i><b>Note</b>: I can't explain why my insert performance was far higher than theirs, as I only changed the stock TokuDB configuration to be a 256M cache and directIO (to make sure this isn't an in-memory test). I'd guess it's their CPU and hard drive, but I'm not sure. And yes, I'd be happy to help figure it out.</i></span></blockquote><div><span>So I'm back to where I started. How can <b><i><u>I</u></i></b> improve things? I'm not an expert in every competing technology I benchmark against. Yet a</span><span>s a professional technical benchmarker I want people to trust my results.</span></div><div><span><br /></span></div><div><span>For now I can only wait for others to question my results, configurations, and benchmark applications. While I'm waiting I'll continue questioning the results of my peers.&nbsp;</span><span>And it doesn't have to be all doom-and-gloom. I'll also be pointing out when I find a great benchmark, or benchmarker, or benchmarketer.</span></div>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.acmebenchmarking.com/2015/01/should-vegetarians-open-steakhouse.html"> on January 08, 2015 10:39 AM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.acmebenchmarking.com/2015/01/can-we-improve-current-state-of.html" title="Acme Benchmarking">Can we improve the current state of benchmarketing?</a></h2>

<h3>Tim Callaghan</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<span>I'm starting off 2015 with the following New Year's Resolution, to improve the state of benchmarking.&nbsp;</span><span>About a month ago I noticed the following </span><a href="https://twitter.com/iamic/status/541908179982884864" target="_blank">tweet</a><span>:</span><br /><blockquote class="tr_bq"><span>Hey <b>@tokutek</b>, please look at this: <a href="http://stssoft.com/products/stsdb-4-0/benchmark">http://stssoft.com/products/stsdb-4-0/benchmark</a> …. Are the benchmarks rigged or correctly done? I'm curious to know!</span></blockquote><span>While I've never met Ian Campbell (<a href="https://twitter.com/iamic" target="_blank">@iamic</a>) he certainly knew how to call me to action. I immediately checked out the <a href="http://stssoft.com/" target="_blank">STSsoft website</a>, the <a href="http://stssoft.com/products/stsdb-4-0/benchmark/" target="_blank">benchmark results page</a>, and the <a href="http://stssoft.com/products/database-benchmark/" target="_blank">benchmark code itself</a>. My first reaction was that something had to be wrong, as the benchmark results showed TokuDB and MyISAM requiring the same amount of disk space. FYI, MyISAM does not compress at all unless it's in a <a href="http://dev.mysql.com/doc/refman/5.5/en/myisam-storage-engine.html#idm140705558016896" target="_blank">compressed read-only mode</a>.</span><br /><span><br /></span><span>Needless to say it was time to figure out what was really going on with this benchmark. As was the case with the <a href="http://www.acmebenchmarking.com/2014/10/dissecting-enterprisedb-nosql-benchmark.html" target="_blank">EnterpriseDB NoSQL Benchmark</a> that I reviewed back in October, I decided to dig in, review the <a href="http://www.urbandictionary.com/define.php?term=benchmarketing" target="_blank">benchmarketing</a>, and dissect the benchmark itself.</span><br /><br /><br /><span><b>Step 1: Benchmarketing Review</b></span><br /><span><br /></span><span>To keep my process bounded, I decided to only review the compression claims as stated on&nbsp;the <a href="http://stssoft.com/products/stsdb-4-0/" target="_blank">STSdb v4.0 product page</a>. <i>The page also makes serious performance claims versus Fractal Tree indexing technology that I'll likely test and blog in the future.</i> The page clearly states:</span><br /><blockquote class="tr_bq"><span>"<b>Up to 3x</b> more compact than TokuDB."</span></blockquote><span>Underneath the claim is a link to the <a href="http://stssoft.com/products/stsdb-4-0/benchmark/" target="_blank">benchmark results page</a>. In the size chart at the bottom of that page it shows STSdb 4.0 at 5365 MB, MyISAM at 7051 MB, and TokuDB at 7051 MB. So I was left wondering...</span><br /><br /><ul><li><span>On what planet is 5365 3x smaller than 7051?</span></li><ul><li><span>For the claim to be true the STSdb size would need to be 2350 MB.</span></li></ul><li><span>Why is the MyISAM size exactly the same as the TokuDB size?</span></li></ul><div><span>The answer to the second question causes a serious benchmarketing issue for the vendor. The SQL that the benchmark uses to determine size was generic for both MyISAM and TokuDB. In the TokuDB case it is calculating uncompressed size, which explains why it was the same as MyISAM. This could also have been checked by looking at the size of the files on disk.&nbsp;</span><span><i>At the bottom of this blog I've included the benchmark code change required to properly determine the size for TokuDB.</i></span></div><br /><span><br /></span><span><br /></span><b>Step 2: Run the benchmark (including the fix)</b><br /><span><br /></span><span>Here are my results for MyISAM and TokuDB. I'm including MyISAM results to show that I'm running the benchmark properly (comparing to the posted results).</span><br /><blockquote class="tr_bq"><span><i>Note that I'm running on TokuDB v7.5.3 for MySQL 5.5.40, stock defaults (no TokuDB variables defined in my.cnf), on an Ubuntu 14.04 desktop with a Core i7-4790K, 32GB RAM, and an Intel 480GB SSD. The benchmark client is running in a Windows 7 Virtual Machine (VMware Workstation 11.0) on the Ubuntu desktop.</i></span></blockquote><span><br /></span><div class="separator"><a href="http://3.bp.blogspot.com/-8cgg-Yj3alc/VKvmiywVKSI/AAAAAAAAB34/K4JntIEZ48I/s1600/100mm-myisam.png"><img border="0" src="http://3.bp.blogspot.com/-8cgg-Yj3alc/VKvmiywVKSI/AAAAAAAAB34/K4JntIEZ48I/s1600/100mm-myisam.png" height="363" width="640" /></a></div><span><br /></span><div class="separator"><a href="http://4.bp.blogspot.com/-C5T5_W0K28Y/VKvmo1Zgo8I/AAAAAAAAB4A/B9S4WOdutmk/s1600/100mm-tokudb-zlib.png"><img border="0" src="http://4.bp.blogspot.com/-C5T5_W0K28Y/VKvmo1Zgo8I/AAAAAAAAB4A/B9S4WOdutmk/s1600/100mm-tokudb-zlib.png" height="364" width="640" /></a></div><br /><b><br /></b><b>Step 3: Analyze the results</b><br /><b><br /></b><span>TokuDB is <b>18% smaller</b> than STSdb (4413 MB vs. 5365 MB).</span><br /><span><br /></span><span><br /></span><b>Where do we go from here?</b><br /><b><br /></b><span>Can we do better than this? I think we can. I propose the following:</span><br /><br /><ul><li><span>Do everything possible to make sure you publish accurate results.</span></li><ul><li><span>If something looks too good to be true, it probably is.</span></li></ul><li><span>Review the benchmark efforts of others.</span></li><ul><li><span>Even if it's not comparing to your technology, peer review is needed.</span></li></ul><li><span>Challenge incorrect results.</span></li><ul><li><span>I welcome others to review my benchmarks and my results as it only makes the benchmark more trustworthy.</span></li></ul></ul><br /><span><br /></span><span><br /></span><span>The correct SQL to determine TokuDB size:</span><br /><span><br /></span><span>try</span><br /><span>{</span><br /><span>&nbsp; string tables = String.Join(" OR ", Enumerable.Range(0, connections.Length).Select(x =&gt; String.Format("table_name = '{0}'", GetTableName(x))));</span><br /><span>&nbsp; string query = "";</span><br /><span><br /></span><span>&nbsp; if (StorageEngine == MySQLStorageEngine.TokuDB)</span><br /><span>&nbsp; {</span><br /><span>&nbsp; &nbsp; query = String.Format("select sum(bt_size_allocated) from information_schema.TokuDB_fractal_tree_info where table_schema='{0}' and ({1});", conn.Database, tables);</span><br /><span>&nbsp; }</span><br /><span>&nbsp; else</span><br /><span>&nbsp; {</span><br /><span>&nbsp; &nbsp; query = String.Format("SELECT SUM(Data_length + Index_length) FROM INFORMATION_SCHEMA.TABLES WHERE table_schema = '{0}' and ({1});", conn.Database, tables);</span><br /><span>&nbsp; }</span><br /><span>&nbsp; IDataReader reader = conn.ExecuteQuery(query);</span><br /><span><br /></span><span>&nbsp; long size = 0;</span><br /><span>&nbsp; if (reader.Read())</span><br /><span>&nbsp; &nbsp; size = reader.GetInt64(0);</span><br /><span><br /></span><span>&nbsp; reader.Close();</span><br /><span><br /></span><span>&nbsp; return size;</span><br /><br /><span>}</span>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.acmebenchmarking.com/2015/01/can-we-improve-current-state-of.html"> on January 06, 2015 11:28 AM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.acmebenchmarking.com/2014/12/benchmarking-mongodb-28-mmapv1.html" title="Acme Benchmarking">Benchmarking MongoDB 2.8 MMAPV1 Collection Level Locking</a></h2>

<h3>Tim Callaghan</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<span>While MongoDB 2.8 introduces a formal <a href="http://docs.mongodb.org/manual/release-notes/2.8/#storage-engines-improved-concurrency-document-level-locking-with-compression" target="_blank">storage engine</a> API and brings with it the new <a href="http://www.wiredtiger.com/" target="_blank">WiredTiger</a> storage engine, it also adds collection level locking to the existing memory mapped engine (MMAPV1) which will remain the default engine until MongoDB 3.0, <a href="http://www.zdnet.com/article/mongodb-cto-how-our-new-wiredtiger-storage-engine-will-earn-its-stripes/" target="_blank">so says Eliot</a>.</span><br /><span><br /></span><span>The MongoDB community has been waiting a long time for collection level locking, the <a href="https://jira.mongodb.org/browse/SERVER-1240" target="_blank">Jira ticket</a> was created on June 15, 2010. When I saw the following Facebook post I got excited to give it a spin, but unfortunately the results were extremely poor using MongoDB 2.7.8 (I assume there were other bottlenecks that hadn't yet been removed, and understand that testing early software can be hit-or-miss).</span><br /><span><br /></span><br /><div class="separator"><a href="http://3.bp.blogspot.com/-1NVbbS9XpUc/VI7of1yw8GI/AAAAAAAAB3A/9mYWJ2X18Nc/s1600/mongodb-cll.png"><span><img border="0" src="http://3.bp.blogspot.com/-1NVbbS9XpUc/VI7of1yw8GI/AAAAAAAAB3A/9mYWJ2X18Nc/s1600/mongodb-cll.png" /></span></a></div><span><br /></span><span>Don't get me wrong, I'm a big fireworks fan. With this sort of announcement I expected to see some sort of measurement validating the improvements of going from database level locking to a collection level locking. I've been unable to see any benchmark results, so I decided to run my standard set of 5 benchmarks to compare the MMAPV1 technology in MongoDB 2.6.5 versus MongoDB 2.8.0.RC2 to see how much improvement the lock refinement has made.</span><br /><span><br /></span><span>Rather than go into the technical details of locks I'll assume that a user's expectation of lock refinement is as follows:</span><br /><span><br /></span><br /><blockquote class="tr_bq"><span><i>With database level locking I can only write into a single collection at a time within a single database, regardless of the number of clients. With collection level locking I'll achieve additional throughput (insert/update/delete) by concurrently operating on multiple collections.</i></span></blockquote><span><br /></span><span>With most technologies the performance improvement with concurrent clients generally scales through a particular number of them, levels off at some point, then often times gets worse with additional client load.</span><br /><span><br /></span><span>Having said all of that, here are my results. In future blogs I'll be drilling deeper into specifics, but for now thought the results were interesting enough to share as is. All benchmark code is available in my GitHub: <a href="https://github.com/tmcallaghan/iibench-mongodb" target="_blank">iibench</a>, <a href="https://github.com/tmcallaghan/sysbench-mongodb" target="_blank">sysbench</a>. The benchmarks were run on a dual socket Xeon 5520 (8 hardware threads plus hyperthreading), 8 drive 10K SAS RAID10 array, 48GB RAM.</span><br /><span><br /></span><span><br /></span><b>Benchmark 1 : iiBench : 1 insert thread</b><br /><span><br /></span><span>This benchmark measures the sequential insertion performance into a single collection with 3 secondary indexes using a single insert client.</span><br /><span><br /></span><span>2.6.5 = 3190 inserts per second</span><br /><span>2.8.0.RC2 = 3414 inserts per second (7% faster)</span><br /><span><br /></span><span>Since this test is single threaded I didn't expect to see any performance improvement. Having said that, measurable performance increases are always nice.</span><br /><br /><b><br /></b><b>Benchmark 2 : iiBench : 4 insert threads</b><br /><span><br /></span><span>This benchmark measures the sequential insertion performance into a single collection with 3 secondary indexes using four concurrent insert clients.</span><br /><span><br /></span><span>2.6.5 = 3177 inserts per second</span><br /><span>2.8.0.RC2 = 3233 inserts per second (2% faster)</span><br /><span><br /></span><span>This test uses 4 insert threads, but all of them are inserting into the same collection so collection level locking provides no benefit.</span><br /><span><br /></span><b><br /></b><b>Benchmark 3 : iiBench : 1 insert threads plus 1 query thread</b><br /><span><br /></span><span>This benchmark measures the sequential insertion performance into a single collection with 3 secondary indexes using a single insert client while simultaneously querying the collection via a second client.</span><br /><span><br /></span><span>2.6.5 = 2700 inserts per second</span><br /><span>2.8.0.RC2 = 3317 inserts per second (23% faster)</span><br /><span><br /></span><span>Nothing about collection level locking should have made this benchmark faster, but I measured a 23% improvement. Good stuff.</span><br /><div><span><br /></span></div><div><b><br /></b><b>Benchmark 4 : Sysbench : Greater than RAM</b><br /><span><br /></span><span>Sysbench is an interesting workload as it tests point queries, range queries, aggregation, inserts, updates, and deletes. This test was run with 16 collections so the opportunity for parallelism enabled by collection level locking was certainly present.</span><br /><span><br /></span><span>First the 16 collections are loaded with data (insert only), with 8 concurrent loader threads each loading it's own collection. This experiment loads each collection with 10 million documents, so the amount of data exceeds the available RAM in the server (and requires IO).</span><br /><span><br /></span><span>Load phase (journal on, oplog off):</span><br /><span><br /></span><span>2.6.5 = 8539 inserts per second</span><br /><span>2.8.0.RC2 = 13954 inserts per second (63% faster)</span><br /><span><br /></span><span>I'd usually report a 63% improvement as fantastic, but with 8 concurrent loaders the opportunity for improvement was significantly higher. <b><i>This was the test that should have shown the most dramatic performance improvement.</i></b></span><br /><span><br /></span><span>Execute phase (journal on, oplog on):</span><br /><span><br /></span><br /><div class="separator"><a href="http://2.bp.blogspot.com/-MWS-SIeOEnE/VI75Z3R7PDI/AAAAAAAAB3Q/RjwHnnQqHwM/s1600/sysbench-gt-ram.png"><img border="0" src="http://2.bp.blogspot.com/-MWS-SIeOEnE/VI75Z3R7PDI/AAAAAAAAB3Q/RjwHnnQqHwM/s1600/sysbench-gt-ram.png" height="355" width="640" /></a></div><div class="separator"><br /></div><span>The largest gain was at 64 client threads, where performance improved 136%. Nice improvement, but I expected more.</span><br /><span><br /></span><b><br /></b><b>Benchmark 5 : Sysbench : In-Memory</b><br /><span><br /></span><span>Again, the same Sysbench workload and schema but only 1 million documents per collection so the entire workload easily fits in RAM.</span><br /><span><br /></span><span>Load phase (journal on, oplog off):</span><br /><span><br /></span><span>2.6.5 = 27347 inserts per second</span><br /><span>2.8.0.RC2 = 43045 inserts per second (72% faster)</span><br /><span><br /></span><span>Again, I'd usually report a 72% improvement as fantastic, but with 8 concurrent loaders the opportunity for improvement was significantly higher.</span><br /><span><br /></span><span>Execute phase (journal on, oplog on):</span><br /><span><br /></span><br /><div class="separator"><a href="http://3.bp.blogspot.com/-nALt_oTjJcc/VI77Yh--B3I/AAAAAAAAB3c/Yh-pl36yoMw/s1600/sysbench-in-ram.png"><img border="0" src="http://3.bp.blogspot.com/-nALt_oTjJcc/VI77Yh--B3I/AAAAAAAAB3c/Yh-pl36yoMw/s1600/sysbench-in-ram.png" height="353" width="640" /></a></div><div class="separator"><br /></div><span>The largest gain this time was at 1024 client threads, where performance improved 118%. As with the greater than RAM Sysbench, nice improvement, but I expected more.</span><br /><span><br /></span><span><br /></span><span><b>Conclusion</b></span><br /><span><br /></span><span>While there are some measurable performance improvements in the collection level locking feature in MongoDB 2.8, I wonder how much additional time and effort will go into further improving the performance. Given that document level locking is a feature not coming to the MMAPV1 storage engine at all, I assume there won't be much effort into further performance improvements. Please comment with your own tests, I'd like to understand workloads that provide more benefit from this effort.</span></div>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.acmebenchmarking.com/2014/12/benchmarking-mongodb-28-mmapv1.html"> on December 15, 2014 12:25 PM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.acmebenchmarking.com/2014/11/announcing-iibench-for-mysql-in-java.html" title="Acme Benchmarking">Announcing iiBench for MySQL in Java</a></h2>

<h3>Tim Callaghan</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<span>I just pushed the new Java based iiBench for MySQL (and Percona Server and MariaDB), the code and documentation are available now in the <a href="https://github.com/tmcallaghan/iibench-mysql" target="_blank">iibench-mysql Github repo</a>. Pull request are welcome!</span><br /><span><br /></span><span>The <a href="http://www.tokutek.com/resources/technology/iibench/" target="_blank">history of iiBench</a> goes back to the early days of <a href="http://www.tokutek.com/" target="_blank">Tokutek</a>. Since "indexed insertion" is a strength of <a href="http://en.wikipedia.org/wiki/Fractal_tree_index" target="_blank">Fractal Tree indexes</a>, the first iiBench was created by Tokutek in C++ back in 2008. <a href="http://smalldatum.blogspot.com/" target="_blank">Mark Callaghan</a> rewrote iiBench in Python, adding several features along the way. His version of iiBench is available in <a href="https://code.launchpad.net/~mdcallag/mysql-patch/mytools" target="_blank">Launchpad</a>.</span><br /><span><br /></span><span>So why did I create a new iiBench in Java?</span><br /><br /><ul><li><span>Raw Speed</span></li><ul><li><span>By <b><i>eliminating the insert calls</i></b> from each version I tested how many inserts per second each version. Any time spent waiting for the next batch of inserts is time that could be put toward inserting rows (and yes I understand that concurrent clients can reduce this concern).</span></li><li><span>All tests were run on my desktop (Intel i7-4790K). As the below graph shows, the 1 thread version of the Java iiBench is almost 4x faster than the 4 threaded Python iiBench, and the 4 thread Java version scales quite nicely.</span></li></ul></ul><div class="separator"><a href="http://1.bp.blogspot.com/-jGHWnU9Clmw/VHNPMlISYqI/AAAAAAAAB2g/qifnHqTxggw/s1600/20141124-iibench-raw-inserts-per-second.png"><img border="0" src="http://1.bp.blogspot.com/-jGHWnU9Clmw/VHNPMlISYqI/AAAAAAAAB2g/qifnHqTxggw/s1600/20141124-iibench-raw-inserts-per-second.png" height="400" width="640" /></a></div><ul><li><span>Capability</span></li><ul><li><span>Because of the Python's <a href="https://wiki.python.org/moin/GlobalInterpreterLock" target="_blank">Global Interpreter Lock</a>, I need to run 4 copies of the Python iiBench to create 4 loader "threads". Each of these benchmark clients creates it's own log files that need to be aggregated to show cumulative insertion performance. Java threading handles it cleanly, allowing a single client application to run regardless of how many client threads are needed.</span></li></ul><li><span>Miscellaneous</span></li><ul><li><span>Not really relevant to the version or language, but I cringe every time I need to get software from Launchpad. I am not a fan of <a href="http://bazaar.canonical.com/en/" target="_blank">Bazaar</a>.</span></li></ul></ul><div><span>Again, contributions/comments/complaints are always welcome, here or in github!</span></div>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.acmebenchmarking.com/2014/11/announcing-iibench-for-mysql-in-java.html"> on November 24, 2014 11:31 AM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.acmebenchmarking.com/2014/11/prediction-mongodb-28-storage-engines.html" title="Acme Benchmarking">Prediction: MongoDB 2.8 storage engines and the rise of the MongoDBA</a></h2>

<h3>Tim Callaghan</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<span>MongoDB has always been about ease of use. With nothing more than the mongod binary, starting a MongoDB server is as simple as:</span><br /><blockquote class="tr_bq"><span>./mongod --dbpath=/path/to/data</span></blockquote><span>As a long time user of Oracle and MySQL I'm extremely impressed by just how simple this is. It certainly encourages new users to try it out.</span><br /><span><br /></span><span>In MongoDB 2.6 and earlier there has only been a single "storage engine" available in the server. That storage engine has very few tunable parameters, so the defaults are fine for most users. If you don't like the defaults you can probably change them with a little review of the documentation.</span><br /><span><br /></span><span>MongoDB 2.8 adds the ability to support an unlimited number of storage engines via a storage engine API. Using the alternative <a href="http://www.wiredtiger.com/" target="_blank">WiredTiger</a> storage engine is as simple as asking for it on the command line:</span><br /><blockquote class="tr_bq"><span>./mongod --dbpath=/path/to/data --storageEngine wiredtiger</span></blockquote><span>Simple, right? It is if the defaults work for your use-case and infrastructure. If not, the <a href="http://docs.mongodb.org/manual/release-notes/2.8/?_ga=1.161065725.190157924.1335295563" target="_blank">MongoDB 2.8 release notes</a> point you to the <a href="http://source.wiredtiger.com/2.4.1/group__wt.html#ga9e6adae3fc6964ef837a62795c7840ed" target="_blank">WiredTiger configuration documentation</a>. My guess is that most MongoDB users who visit that page will immediately Google for help or advice (look for yourself, there are a lot of complicated parameters, many of which need to be tuned in combination). I assume that MongoDB will reduce the parameters and provide more helpful documentation in the future, but that's an interesting challenge to solve.</span><br /><span><br /></span><span>So back to my prediction. MongoDB's simplicity will [no, must] change over time. Picking the correct storage engine is only the first step. It must then be tuned according to the workload and hardware. Indexing and queries continue to grow in complexity, as does the MongoDB optimizer. There have even been hints of schema enforcement in past presentations. Sounds a lot like a DBA to me, a MongoDBA.</span>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.acmebenchmarking.com/2014/11/prediction-mongodb-28-storage-engines.html"> on November 17, 2014 11:55 AM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.acmebenchmarking.com/2014/10/dissecting-enterprisedb-nosql-benchmark.html" title="Acme Benchmarking">Dissecting the EnterpriseDB NoSQL Benchmark</a></h2>

<h3>Tim Callaghan</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<span>In my last <a href="http://www.acmebenchmarking.com/2014/10/tokumx-vs-postgresql-in-enterprisedbs.html" target="_blank">blog</a> I analyzed the compression portion of the&nbsp;<a href="http://blogs.enterprisedb.com/2014/09/24/postgres-outperforms-mongodb-and-ushers-in-new-developer-reality/" target="_blank">EnterpriseDB NoSQL Benchmark</a>. I concluded that the data set was far too easily compressed. This, in my opinion, invalidates any conclusions originally published comparisons attempt to portray.</span><br /><span><br /></span><span>In this blog I want to describe each section of the original benchmark. My goal is not to rerun and compare results with Postgres, MongoDB, or TokuMX. Rather, I want to point out exactly what each individual benchmark is doing and what could have been done differently or better.</span><br /><span><br /></span><span>Before I begin I want to point out that in no way does this specific benchmark disqualify Postgres as a potential NoSQL solution. Also, EnterpriseDB benchmarked both products with stock default configurations. I'm not sure why <u>not following</u> the most basic "best practices" is a good idea. An example of this is <a href="http://docs.mongodb.org/manual/administration/production-notes/#recommended-configuration" target="_blank">turning down the readahead</a> for MongoDB.</span><br /><span><br /></span><span><b>Benchmark - Prepare</b></span><br /><span>For a given number of documents to insert, a two large flat files are created:</span><br /><br /><ul><li><span>File 1 is pure JSON, in "batches" of 11 base documents, within which a small percentage of the document is variable and the rest is fixed. This file is appropriate for bulk loading. It is important to note that the "description" field of each document is several kilobytes in length and unchanged in each batch. Bottom line, these documents are highly compressible.</span></li><li><span>File 2 is a single document insert statement appropriate to the destination platform (Postgres or MongoDB). While the format is different than file 1, the content of the rows is much the same and highly compressible. Note that neither Postgres nor MongoDB is generating extended inserts, just a single insert per statement.</span></li></ul><br /><b>Benchmark - Data Load</b><br /><span>The data load benchmark is simply timing the process of bulk loading file 1 from the prepare phase. No indexes exist prior to loading, so this is a primary key only test. And since it's bulk loading a single file with a single loader, there is no concurrency for this test.</span><br /><span><br /></span><span><b>Benchmark - Index???</b></span><br /><span>You won't find this benchmark result listed because for some reason the "create index" step after bulk loading isn't timed. I'm not going to run this test myself to see which product won, but it seems crazy to me that this step wasn't timed. I'd like to understand this oversight, but the comment capability on the EnterpriseDB blog is now disabled. In any event, 3 indexes are created each on a different field: name, type, and brand.</span><br /><span><br /></span><span><b>Benchmark - Select</b></span><br /><span>I believe that a query workload should be part of any new benchmark, so it's nice to see that queries are included here. In a single thread four large queries are executed in order, each of which is matching 9% to 18% of the data set. It would be nice to see a more selective set of queries, plus some concurrency on this test.</span><br /><span><br /></span><span><b>Benchmark - Insert</b></span><br /><span>After the select benchmark completes, the table/collection is dropped and reloaded using file 2 from the prepare phase. This benchmark is different from the data load benchmark in that it does not utilize the servers bulk load functionality. I assume this is intended to show how the server will perform long term (after the initial data load completes), but a lack of indexes and no concurrency make this step largely uninteresting.</span><br /><span><br /></span><b>Benchmark - Size</b><br /><span>After the data was bulk loaded (Data Load) a command is executed to determine how much disk space the unindexed form is consuming.</span><br /><span><b><br /></b></span><b><span>Overall Thoughts</span></b><br /><span>While I don't think this benchmark will become a standard any time soon, there are some simple improvements to make the results more interesting:</span><br /><br /><ul><li><span>Bulk loading without indexes is not necessarily a bad idea, but the time required to build indexes after the load completes should be measured and reported.</span></li><li><span>Size on disk should include secondary indexes.</span></li><li><span>Some of the select workload should be randomly selecting more specific data. Put it in a loop and make it concurrent.</span></li><li><span>Prior to starting the insert benchmark, create the secondary indexes. Also, make this test concurrent by instantiating several insert clients.</span></li></ul>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.acmebenchmarking.com/2014/10/dissecting-enterprisedb-nosql-benchmark.html"> on October 27, 2014 09:45 AM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.acmebenchmarking.com/2014/10/tokumx-vs-postgresql-in-enterprisedbs.html" title="Acme Benchmarking">TokuMX vs. PostgreSQL in EnterpriseDB's NoSQL Compression Benchmark</a></h2>

<h3>Tim Callaghan</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<span>Since this is my first blog I feel it's necessary to introduce myself. I'm Tim Callaghan, I work at <a href="http://www.tokutek.com/" target="_blank">Tokutek</a> (makers of <a href="http://www.tokutek.com/tokudb-for-mysql/" target="_blank">TokuDB</a> and <a href="http://www.tokutek.com/tokumx-for-mongodb/" target="_blank">TokuMX</a>), and I love benchmarking. While some of the content on this blog will certainly be about Tokutek technologies, I plan on exploring a wide variety of others as well. These are strictly my own personal views and opinions, and comments/feedback are always welcome. Lets get started...</span><br /><span><br /></span><span>A few weeks ago I noticed an <a href="http://blogs.enterprisedb.com/2014/09/24/postgres-outperforms-mongodb-and-ushers-in-new-developer-reality/" target="_blank">EnterpriseDB NoSQL Benchmark</a> that measured Data Load, Insert, Select, and Size. It wasn't just a NoSQL benchmark, it was specifically calling out <a href="http://www.mongodb.com/" target="_blank">MongoDB</a> by declaring "Postgres Outperforms MongoDB and Ushers in New Developer Reality". Now the <a href="http://blogs.enterprisedb.com/author/marc_linster/" target="_blank">blogger</a> had my attention, I needed to learn more, so I dug in.</span><br /><span><br /></span><span>I was concerned with how little the blog explained about the benchmark itself, the results were only presented as "Relative Performance Comparisons". Was this an example of what <a href="http://smalldatum.blogspot.com/" target="_blank">Mark Callaghan</a> describes as <a href="http://smalldatum.blogspot.com/2014/06/benchmarketing.html" target="_blank">benchmarketing</a>?</span><br /><span></span><br /><span>I've created far too many benchmark applications in my life, and analyzed countless more. To EnterpriseDB's credit, they published their <a href="https://github.com/EnterpriseDB/pg_nosql_benchmark" target="_blank">benchmark code</a> on GitHub. After downloading the code and opening my editor I saw something I never would have imagined, a benchmark written completely in bash. <i>Yes, bash.</i> I used to think I was a bash expert, but now I know differently.</span><br /><span><br /></span><span>The compression portion of the benchmark is simple. An extremely large JSON data set is created as a flat file, the flat file is loaded into MongoDB and Postgres, and the on-disk size is recorded for the compression comparison. Since the TokuMX binaries use the same names as MongoDB, I was able to easily run the test comparing TokuMX and Postgres. The blog stated that benchmark was intended to run without additional tuning, so I setup TokuMX to start with it's default configuration.&nbsp;</span><br /><span><br /></span><span><b>Results</b></span><br /><span><br /></span><br /><div class="separator"><a href="http://1.bp.blogspot.com/-r7jAN2KZ2PI/VET_UZOA-gI/AAAAAAAAB1c/QhAmRNFkzEk/s1600/20141020-ab-nosql-compression.png"><img border="0" src="http://1.bp.blogspot.com/-r7jAN2KZ2PI/VET_UZOA-gI/AAAAAAAAB1c/QhAmRNFkzEk/s1600/20141020-ab-nosql-compression.png" height="400" width="640" /></a></div><span><br /></span><span><b>Details</b></span><br /><span><br /></span><span>The 10 million document raw file 25,287 MB. Each technology is compared here:</span><br /><ul><li><span>Postgres 9.4 beta2 required 14,204 MB (56.17% raw size).</span></li></ul><ul><li><span>Postgres 9.4 beta3 <u>also</u> required 14,204 MB (56.17% raw size). This surprised me, as I tested this newer beta of Postgres specifically because of the following release note:</span></li></ul><blockquote class="tr_bq"><b><i>The JSON data set produced by this benchmark application contains very large amounts of redundant text. As a matter of fact, there are 11 "base" documents in total, each of which is only slightly modified with a random handful of numeric characters for each batch of 11 documents. So each batch of 11 documents is 99% identical to every other batch, and each batch is about 29KB in total.</i></b></blockquote><ul><li><span>MongoDB 2.6.4 required 41,035 MB (162.28% raw size). This is a well known behavior of MongoDB, as the default for <a href="http://docs.mongodb.org/manual/core/storage/#power-of-2-sized-allocations" target="_blank">Power of 2 Sized Allocations</a> was changed to "enabled in MongoDB 2.6. This sizing strategy is meant to improve the chances that a document update can be done in-place, and thus reduce index updates.</span></li></ul><ul><li><span>Disabling Power of 2 Sized Allocations in MongoDB 2.6.4 reduced the required space to 27,871 MB (110.22% of raw size).</span></li></ul><ul><li><span>TokuMX 2.0.0 required 1,199 MB (4.74% raw size).</span></li></ul><div><span><b><br /></b></span></div><div><span><b>Thoughts and Analysis</b></span></div><div><span><br /></span></div><div><span>TokuMX compression was the winner, hands down. Compared to Postgres, TokuMX required 91.56% less space on disk (1,199 MB vs. 14,204 MB). Nothing more to say here, it wasn't even close.</span></div><div><br /></div><div><span>But lets be honest, <b><i>this&nbsp;</i></b></span><span><b><i>compression benchmark is not interesting!</i></b>&nbsp;The data is simply too easily compressed. It shows that you should never accept benchmark results without understanding the benchmark itself. Plus you should make sure that the results are interesting to your specific use-case.</span></div><span><br /></span><span>I plan on reviewing load and query aspects of the benchmark in an upcoming blog. </span>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.acmebenchmarking.com/2014/10/tokumx-vs-postgresql-in-enterprisedbs.html"> on October 20, 2014 10:52 AM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.xaprb.com/blog/2013/10/01/mysql-isnt-limited-to-nested-loop-joins/" title="">MySQL isn't limited to nested-loop joins</a></h2>

<h3>Baron Schwartz</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<p>I have followed the &ldquo;Use the Index, Luke!&rdquo; blog for a while. Today Marcus <a href="http://use-the-index-luke.com/blog/2013-10-01/mysql-is-to-sql-like-mongodb-to-nosql">wrote</a> that (I&rsquo;ll paraphrase) MongoDB disgraces NoSQL the same way that MySQL disgraces SQL. I agree with a lot of this, actually, although I&rsquo;m not sure I&rsquo;d put it so strongly. People often like products for good reasons, and to think that legions of developers are stupid or ill-educated is suspect, in my opinion.</p>

<p>But that wasn&rsquo;t what I meant to write about. I wanted to point out something about the blog post that&rsquo;s a little outdated. He wrote, and this time I&rsquo;ll quote, &ldquo;MySQL is rather poor at joining because is only supports nested loops joins. Most other SQL database implement the hash join and sort/merge join algorithms too.&rdquo;</p>

<p>It&rsquo;s no longer true that MySQL doesn&rsquo;t support these, and hasn&rsquo;t been for a while, depending on which version of MySQL you look at. What&rsquo;s slightly unfortunate, in my opinion, is that MySQL doesn&rsquo;t call out in the documentation that they&rsquo;re actually implemented. MySQL documentation talks about Multi-Range Read, Block Nested-Loop, and Batched Key Access join &ldquo;optimizations.&rdquo;</p>

<p>Functionally, these are closely related to combinations of hash and sort-merge join algorithms, and really represent mixtures of features from them combined in different ways, depending on the exact query. Most &ldquo;sophisticated&rdquo; RDBMSs also implement a lot of subtle variations &ndash; edge-case optimizations are really worthwhile. It is rarely as cut-and-dried as pure hash-join or sort-merge join. And in the end, there is always &ndash; always &ndash; iteration over rows to match them up, regardless of the data structure used, regardless of the RDBMS. MySQL happens to call these variations &ldquo;nested loop join optimizations&rdquo; and similar phrases, but that&rsquo;s what they are in other RDBMSs too.</p>

<p>MySQL does very well on many types of joins for which sort-merge and hash-join algorithms are designed. See, for example, <a href="http://www.mysqlperformanceblog.com/2012/03/21/multi-range-read-mrr-in-mysql-5-6-and-mariadb-5-5/">this blog post</a> and <a href="http://www.mysqlperformanceblog.com/2012/03/12/index-condition-pushdown-in-mysql-5-6-and-mariadb-5-5-and-its-performance-impact/">this one</a> and also <a href="http://www.mysqlperformanceblog.com/2012/05/31/a-case-for-mariadbs-hash-joins/">this one on MariaDB&rsquo;s further optimizations</a>.</p>

<p>I think the MySQL documentation could help a little by calling things names that normal users understand. The names we see in the documentation are really reflective of how the optimizer internals gurus think about the algorithms, in my opinion. I think the names describe the implementation, not the end result. I&rsquo;d suggest phrasing it differently for general consumption by the DBA public. Perhaps something like &ldquo;sort-merge join implemented with a _____ algorithm.&rdquo; Or perhaps &ndash; and I will admit I don&rsquo;t keep the details fresh in my mind so I&rsquo;m not the one to ask for the right answer &ndash; perhaps the algorithms MySQL uses really aren&rsquo;t as related or comparable as I think they are, and a different type of explanation is in order. But I bet a lot of DBAs from SQL Server and Oracle Database backgrounds would find it helpful to have an explanation in familiar terms. (This concludes my free and probably unwanted advice to the MySQL docs team!)</p>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.xaprb.com/blog/2013/10/01/mysql-isnt-limited-to-nested-loop-joins/"> on October 01, 2013 12:00 AM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.xaprb.com/blog/2013/04/29/what-tokudb-might-mean-for-mongodb/" title="">What TokuDB might mean for MongoDB</a></h2>

<h3>Baron Schwartz</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<p>Last week <a href="http://www.tokutek.com/">Tokutek</a> <a href="http://www.tokutek.com/2013/04/announcing-tokudb-v7-open-source-and-more/">announced</a> that they&rsquo;re open-sourcing their TokuDB storage engine for MySQL. If you&rsquo;re not familiar with TokuDB, it&rsquo;s an ACID-compliant storage engine with a high-performance index technology known as fractal tree indexing. Fractal trees have a number of nice characteristics, but perhaps the most interesting is that they deliver consistently high performance under varying conditions, such as when data grows much larger than memory or is updated frequently. B-tree indexes tend to get fragmented over time, and exhibit a performance cliff when data doesn&rsquo;t fit in memory anymore.</p>

<p>The MySQL community is excited about having access to TokuDB&rsquo;s source code, and rightly so. TokuDB is, broadly speaking, aimed at the same category of use cases as Oracle&rsquo;s InnoDB, which has been MySQL&rsquo;s leading storage engine for a long time.</p>

<p>MySQL&rsquo;s market size is large for an opensource product (roughly $500M to $1B USD, depending on who you talk to), and in a big pond, a stone causes wide ripples. I think the most significant implications, though, are for MongoDB. Tokutek has published a series of <a href="http://www.tokutek.com/tag/mongodb/">benchmarks of MongoDB performance with TokuDB</a> as the storage engine instead of MongoDB&rsquo;s default storage engine. The results are compelling.</p>

<p>I think TokuDB will rapidly become the storage engine of choice for MongoDB, and could catapult MongoDB into the lead in the NoSQL database arena. This would have profound implications for opensource databases of all flavors, not just NoSQL databases.</p>

<p>It&rsquo;s worth revisiting a bit of ancient history for some context.</p>

<p>Way back in the olden days, MySQL&rsquo;s main storage engine was MyISAM. MyISAM is non-transactional and has table-level locking, meaning that a write (update, insert, delete, or similar) blocked all concurrent access to the table. This is okay for some uses, and can even be very good in special cases, but in the general case it is a disaster. MyISAM introduced some special workarounds for common cases (such as permitting nonblocking inserts to occur at the end of the table), but in the end, you can&rsquo;t fix table-level locking. A mixed workload needs storage that&rsquo;s designed for high read and write concurrency without blocking.</p>

<p>MyISAM had other problems, such as lacking transactions, being prone to data corruption, and long repair times after a crash.</p>

<p>As a result, MySQL as a whole was only interesting to a minority of users. For demanding applications it was little more than a curiosity.</p>

<p>Then came InnoDB. InnoDB introduced ACID transactions, automatic crash recovery, and most importantly, row-based locking and MVCC, which allowed highly concurrent access to rows, so readers and writers don&rsquo;t block each other. InnoDB was the magic that made MySQL a credible choice for a wide range of use cases.</p>

<p>Most of the interesting chapters in MySQL&rsquo;s history have involved InnoDB in one way or another. To list some highlights: Oracle bought InnoDB&rsquo;s creator Innobase Oy, MySQL scrambled to find a replacement (Maria, Falcon, PBXT), Sun&rsquo;s decision to acquire MySQL was said to be influenced by Falcon, Percona created XtraDB, and Oracle acquired Sun. Things are settling down now, but it&rsquo;s easy to forget how much of a soap opera the MySQL world has lived through because of InnoDB not being owned by MySQL.</p>

<p>And in the middle of all this came NoSQL databases. In the past half-dozen years, more databases have been invented, popularized, and forgotten than I care to think about. In many cases, though, these databases were criticized as ignoring or reinventing (badly) decades of learning in relational database technology, and even computer science in general. I know I&rsquo;ve looked at my share of face-palm code.</p>

<p>Databases, by and large, depend on reliable, high-performance storage and retrieval subsystems more than anything else. Many of the NoSQL databases have interesting ideas built on top of bad, bad, bad storage code.</p>

<p>MongoDB is a case in point. MongoDB reinvented some of the worst parts of MySQL all over again. Storage was initially little more than mmap over a file. I think Mark Callaghan put it best in 2009, when he said &ldquo;Reinventing MyISAM is not a feature.&rdquo; MongoDB&rsquo;s storage at that time really was MyISAM-like. It&rsquo;s improved somewhat since then, but it hasn&rsquo;t had the wholesale rip-and-replace improvement that I think is needed. Not only that, but MongoDB as a whole is still (predictably) built around the limitations of the underlying storage, with coarse-grained locking.</p>

<p>But MongoDB, like MySQL, has been relevant in spite of these shortcomings. Form your own opinion about why this is, but from my point of view there are two main reasons:</p>

<ul>
<li>MongoDB was born in an era when the popular databases were frustratingly slow and clunky to work with, and innovation was stalled due to the political drama surrounding them.</li>
<li>MongoDB simply feels nice to developers. If you&rsquo;re not a developer, this is a little hard to explain, but it just feels good, like your favorite pair of jeans. Like a hug from a good friend. Like a hammock and a summer day. The difference between an SQL database and MongoDB for many developers is like the difference between an iPod and a cheap knockoff MP3 player. I could go on and on.</li>
</ul>

<p>It&rsquo;s difficult to overstate the importance of this, because it means that MongoDB may well become an enterprise database, despite what bad opinions you may have about it now. Why is this? It&rsquo;s because developers are king in the modern IT enterprise. Developers determine what technologies get adopted in IT. CTOs like to think the decisions come from the top down, but I&rsquo;ve seen it work the other way time and time again. Developers start to use something that frustrates them less than the alternatives, and a groundswell begins that&rsquo;s impossible to stop. Someday the CTO discovers that the question of whether to use technology X was decided by a junior developer long ago and deployed to production, and now it&rsquo;s too late.</p>

<p>I&rsquo;ve done it myself. At Crutchfield I hijacked the company-wide policy that migration from legacy VB6 to .NET would proceed along the lines of a transition to VB.NET. I was fighting through awful code day in and day out, and I knew that a more restrictive language would prevent a lot of bad practices. So I wrote several major systems in C# without asking permission. It&rsquo;s a lot easier to get forgiveness than permission. Then I showed off what I&rsquo;d done. When I left Crutchfield, the IT department had chosen C#, not VB.NET, as its language of the future (even though there were, and probably still are, major VB.NET applications).</p>

<p>Similarly, at Crutchfield I was provided a 15-inch CRT monitor to work on. This was 2003, you understand. Even at that time, it was awful. How can you expect a developer to work on a flickering, small monitor? I bought my own large-screen LCD and put it in my cubicle. Management ordered me to remove it because it was causing a flood of &ldquo;hey, how did Baron get a nice monitor?&rdquo; questions, but the camel already had a nose under the tent. I took my monitor home, but not too long after that we all started to get nicer monitors. I brought my own nice chair to work, too. All told I probably forced Crutchfield to spend thousands of dollars upgrading equipment. You have to be careful about headstrong kids like me &ndash; don&rsquo;t turn your backs on us for a moment.</p>

<p>This story illustrates why MongoDB is likely to become a major database: because developers enjoy working with it. It feels pleasant and elegant. Remember, most technology decisions are based on how people feel, not on facts. We&rsquo;re not rational beings, so don&rsquo;t expect the best solution to win. Expect people to choose what makes them happy.</p>

<p>And with the availability of TokuDB, MongoDB is lovable by a lot more people. With reliable storage and transactions, uncool kids can like it too.</p>

<p>It goes further than just the storage engine. The kernel of MongoDB has code that needs to be fixed, such as the coarse-grained locking code. Tokutek basically forked MongoDB in order to insert TokuDB into it. They had to, in order to get all that locking out of the way and allow MongoDB to shine with TokuDB on the backend.</p>

<p>I&rsquo;m not sure exactly how this will play out &ndash; will Tokutek start offering a competitive product? Will there be opensource community-based forks of MongoDB that integrate TokuDB? Will 10gen do the engineering to offer TokuDB as a backend? Will 10gen and Tokutek partner to do the engineering and provide support? Will 10gen acquire Tokutek? Will a large company acquire both? You decide.</p>

<p>But I believe that a few things are inevitable, and don&rsquo;t require a crystal ball to guess.</p>

<p>Anyone who cares about MongoDB is going to be using TokuDB as their storage backend within a matter of months. It&rsquo;s happened before &ndash; look at what happened to MySQL and InnoDB. Look at Riak; people dropped Bitcask like a hot potato when LevelDB storage arrived (although it hasn&rsquo;t been a perfect solution).</p>

<p>Just to be clear, I do not think that MongoDB&rsquo;s parallels with MySQL&rsquo;s history must inevitably repeat in all aspects of the story. The world of databases today (big data, cloud, mobile) is not in the same situation it was when MySQL was creeping into general awareness (web, gaming, social, general lack of good alternatives to commercial databases), and the reasons people use MongoDB now are different from the reasons people chose MySQL back in the day. Still, there&rsquo;s a good chance that MySQL&rsquo;s past can teach us about MongoDB&rsquo;s future, and for some use cases, MongoDB deployments will soon accelerate rapidly. I expect a larger commercial ecosystem to emerge, too; right now the MongoDB market is worth tens of millions, and I&rsquo;d guess in a few years we&rsquo;ll look back and see a sharp inflection point in 2013 and 2014. TokuDB could help propel MongoDB&rsquo;s market size into hundreds of millions of dollars, which is a position occupied uniquely by MySQL today in the opensource database world.</p>

<p><a href="http://www.youtube.com/watch?v=2UFc1pr2yUU">It&rsquo;s getting real</a> in the MongoDB world &ndash; this is going to be interesting to watch.</p>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.xaprb.com/blog/2013/04/29/what-tokudb-might-mean-for-mongodb/"> on April 29, 2013 12:00 AM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.xaprb.com/blog/2013/01/10/bold-predictions-on-which-nosql-databases-will-survive/" title="">Bold predictions on which NoSQL databases will survive</a></h2>

<h3>Baron Schwartz</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<p>In case you&rsquo;ve been <a href="http://www.youtube.com/watch?v=cvXqm0RdJms">living under a rock</a> for the last 5 years, the NoSQL movement has changed. There was a time when everyone &ndash; EVERYONE &ndash; was dumping on relational databases, and MySQL in particular. Nonsense like &ldquo;SQL itself is inherently unscalable&rdquo; routinely came out of the mouths of otherwise usually sensible people. But that&rsquo;s cooled off a little bit, thank heavens.</p>

<p>And what&rsquo;s the new hotness? Well, Big Data, of course! But I digress. In the world of databases, it&rsquo;s move over NoSQL, heeeeeere&rsquo;s NewSQL. I&rsquo;m talkin&rsquo; NuoDB, Clustrix, MySQL Cluster (NDB), and so forth. A lot of people now recognize that it wasn&rsquo;t SQL or the relational model that was a problem &ndash; it was the implementations that had some issues. The pendulum has swung a little away from vilifying SQL, and we don&rsquo;t talk about NoSQL as much as we talk about document-oriented or key-value or whatever.</p>

<p>Does that spell death for NoSQL databases? Not in my opinion. But I am just in the mood to stick my neck out a bit today, so I&rsquo;m going to do something I don&rsquo;t normally do &ndash; predict the future. Here&rsquo;s my prediction: <strong>there may be many NoSQL databases that live long and healthy lives, but among them will probably be MongoDB, Redis, and Riak</strong>.</p>

<p>Discuss!</p>

<p>PS: this prediction is about what I think will happen. If I get one out of three right, I&rsquo;ll be happy. It&rsquo;s not an endorsement of any database, dismissal of any other database, or an opinion about what I think <em>should</em> happen. Limitations and exclusions apply. Subject to credit approval, see store for details.</p>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.xaprb.com/blog/2013/01/10/bold-predictions-on-which-nosql-databases-will-survive/"> on January 10, 2013 12:00 AM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.xaprb.com/blog/2012/08/23/avoiding-statement-based-replication-warnings/" title="">Avoiding statement-based replication warnings</a></h2>

<h3>Baron Schwartz</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<p>Although not perfect, MySQL replication was probably the killer feature that made MySQL the default database for web applications some time ago. Since then, MySQL replication has been improved greatly, with such notable changes as row-based replication. At the same time, the replication engineering team has made MySQL replication more conservative and less forgiving of foot-gun errors. These have gone a long way towards helping users avoid some of the problems that made replication sometimes drift out of sync with the master copy, sometimes silently.</p>

<p>In some cases I think the strictness has gone a little too far. One example is the server&rsquo;s identification of statements that are unsafe for replication because they are nondeterministic. Here is a statement in an application I manage, which is designed to claim some work from a queue. After running this statement, the application checks if any rows were affected, and if so, it then fetches and processes the rows:</p>

<pre>update pending_jobs set token = ?
where token is null
  and (owner_pid is null or owner_pid <pre> ?)
order by id
limit 1;</pre>

<p>MySQL will write to the server&rsquo;s error log when this statement is issued and binlog_format=STATEMENT, because of the presence of a LIMIT in the statement: <em>120823 20:59:12 [Warning] Unsafe statement written to the binary log using statement format since BINLOG_FORMAT = STATEMENT. The statement is unsafe because it uses a LIMIT clause. This is unsafe because the set of rows included cannot be predicted. Statement: [statement follows]</em></p>

<p>This becomes a problem very quickly, because in fact the statement is deterministic and the rows to be affected can be predicted perfectly. The server is just being overly strict. The general technique illustrated here is a superior alternative to some other ways of <a href="http://www.engineyard.com/blog/2011/5-subtle-ways-youre-using-mysql-as-a-queue-and-why-itll-bite-you/">implementing a queue in a database table</a>. But if a superior alternative floods the error log with spurious messages, it must be avoided anyway.</p>

<p>The solution I chose in this case is a blend of SQL and application code. Part of the logic &ndash; the limit &ndash; must be handled in the application code, and pulled out of the UPDATE statement so the server will consider it to be deterministic. Here is pseudocode for the result:</p>

<pre>
function claim_a_job() {
   $pid   = get_pid();
   $token = md5(rand(), time(), $pid);
   @jobs  = query(
            "select id from pending_jobs
             where token is null and (owner_pid is null or owner_pid <pre> ?)
             order by id", $pid);
   foreach ( $job in @jobs ) {
      next unless query("update pending_jobs set token=?
                         where token is null and id=?", $token, $job);
      return $job;
   }
   return null;
}
</pre>

<p>This code finds all unclaimed rows and tries to claim each one in turn. If there&rsquo;s a race condition and another worker has claimed the job in the meantime, no rows will be updated. If the UPDATE affects a row, then the function claimed the job successfully, and the job&rsquo;s ID is returned. The most important thing, however, is that the SQL lacks any constructs such as LIMIT that might cause errors to be spewed into the log. I want my logs to be silent so that I can detect when something really important actually happens.</p>

<p>Percona Server has a feature to disable logging this warning, which is a mixed blessing. I want to find all such queries and examine them, because some of them might be a legitimate risk to replication integrity. If I disable the logging, it becomes much harder, though I can potentially do it by inspecting TCP traffic instead. I do wish that official MySQL supported the ability to silence warnings selectively, however.</p>

<p>Another possible solution would be to switch to row-based binary logging, which comes with many other benefits as well. But such a change is not to be taken lightly; it requires a careful assessment of the server and its workload, lest there be unintended consequences.</p>

<p>An even better solution would be to implement some additional features in the server. Many of the features that developers like the most about NoSQL databases such as MongoDB and Redis (or even PostgreSQL) are special-case behaviors to simplify things that are awkward to do in most databases. Examples include atomically adding and removing from a queue, and features to avoid polling, such as LISTEN and NOTIFY.</p></pre></pre>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.xaprb.com/blog/2012/08/23/avoiding-statement-based-replication-warnings/"> on August 23, 2012 12:00 AM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.xaprb.com/blog/2012/04/09/automated-integrated-sharding-the-new-killer-database-feature/" title="">Automated, integrated sharding: the new killer database feature</a></h2>

<h3>Baron Schwartz</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<p>MySQL became wildly successful in part because it had built-in, simple replication. Sure, it had lots of interesting failure scenarios and was not great at first &mdash; it is much better these days &mdash; but it was nevertheless successful because there was a single, out-of-the-box, not-very-complex way to do replication. I have opined many times before that this was one of the killer features missing from PostgreSQL. I think that can large explain why MySQL became more popular more quickly.</p>

<p>The new killer feature is automatic sharding, in my opinion. If you&rsquo;re not accustomed to the word, &ldquo;sharding&rdquo; means partitioning of a large dataset across many servers.</p>

<p>It is easy to poke fun at <a href="http://www.mongodb.org/">MongoDB&rsquo;s</a> current limitations, but for all that, it has a story to tell about sharding. There is One Right Way To Do It in MongoDB, and it&rsquo;s a part of the product.</p>

<p>I don&rsquo;t see sharding being added into the core of MySQL itself, but there are some very interesting efforts headed towards MySQL. There are at least the following companies providing sharding via a proxy or middleware solution, with a lot of other features also available in some products:</p>

<ul>
<li><a href="http://www.scalebase.com/">Scalebase</a></li>
<li><a href="http://www.scalearc.com/">ScaleArc</a></li>
<li><a href="http://www.dbshards.com/">dbShards</a></li>
<li><a href="http://www.parelastic.com/">ParElastic</a></li>
</ul>

<p>In addition, there are community-based efforts, such as <a href="http://code.google.com/p/shard-query/">Shard-Query</a> and the <a href="http://spiderformysql.com/">Spider</a> storage engine. And there&rsquo;s <a href="http://mysql.com/products/cluster/">MySQL (NDB) Cluster</a>, and commercial rip-out-and-plug-in replacements for MySQL such as <a href="http://www.clustrix.com/">Clustrix</a>.</p>

<p>Am I missing any? I probably am. You can see and talk to many of these companies at this week&rsquo;s <a href="http://www.percona.com/live/mysql-conference-2012/">MySQL conference</a>, by the way.</p>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.xaprb.com/blog/2012/04/09/automated-integrated-sharding-the-new-killer-database-feature/"> on April 09, 2012 12:00 AM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.xaprb.com/blog/2011/01/29/my-sessions-at-the-oreilly-mysql-conference-2011/" title="">My sessions at the O'Reilly MySQL Conference 2011</a></h2>

<h3>Baron Schwartz</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<p>I&rsquo;ll be presenting several sessions at the <a href="http://en.oreilly.com/mysql2011/">O&rsquo;Reilly MySQL Conference &amp; Expo 2011</a>, which is April 11-14 in Santa Clara, California. I recommend this conference to anyone interested in open-source databases including MySQL, PostgreSQL, CouchDB, MongoDB, and others. There is very good coverage of a diverse list of open-source databases.</p>

<p>My sessions are as follows:</p>

<ul>
<li><a href="http://en.oreilly.com/mysql2011/public/schedule/detail/17808">Building on Strengths, Learning from Differences</a>: a keynote address on the open-source database ecosystem, how we got here, and what we can do to make the most of the future.</li>
<li><a href="http://en.oreilly.com/mysql2011/public/schedule/detail/17153">Forecasting MySQL Performance and Scalability</a>: mathematical models for forecasting performance and scalability that actually work and are not hard to do. (<a href="http://www.mysqlperformanceblog.com/2011/01/26/modeling-innodb-scalability-on-multi-core-servers/">Example</a>)</li>
<li><a href="http://en.oreilly.com/mysql2011/public/schedule/detail/17129">The Aspersa System Administrator&rsquo;s Toolkit</a>: this is an under-appreciated toolkit at the moment, but it could be the next Maatkit.</li>
</ul>

<p>In addition, I am listed as presenting <a href="http://en.oreilly.com/mysql2011/public/schedule/detail/17142">Diagnosing and Fixing MySQL Performance Problems</a>, a 3-hour tutorial on how to find and solve performance problems with swift and definite results. However, I actually have a scheduling conflict and a couple of my colleagues will present this instead.</p>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.xaprb.com/blog/2011/01/29/my-sessions-at-the-oreilly-mysql-conference-2011/"> on January 29, 2011 12:00 AM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.xaprb.com/blog/2011/01/22/version-1-1-8-of-better-cacti-templates-released/" title="">Version 1.1.8 of Better Cacti Templates released</a></h2>

<h3>Baron Schwartz</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<p>I&rsquo;ve released version 1.1.8 of the <a href="http://code.google.com/p/mysql-cacti-templates/">Better Cacti Templates</a> project. This release includes a bunch of bug fixes and several important new graphs. There are <a href="http://code.google.com/p/mysql-cacti-templates/wiki/MySQLTemplates#MySQL_Query_Response_Time_%28Microseconds%29">graphs</a> for the new <a href="http://www.percona.com/docs/wiki/percona-server:features:response_time_distribution">response-time statistics exposed in Percona Server</a>, and a new set of graphs for <a href="http://code.google.com/p/mysql-cacti-templates/wiki/MongoDBTemplates">MongoDB</a>.</p>

<blockquote>
<p><a href="https://vividcortex.com/">VividCortex</a> is the startup I founded in 2012. It&rsquo;s the easiest way to monitor what
your servers are doing in production and I consider it far superior to Cacti. VividCortex offers <a href="https://vividcortex.com/monitoring/mysql/">MySQL performance
monitoring</a> and <a href="https://vividcortex.com/monitoring/postgres/">PostgreSQL
performance management</a> among many
other features.</p>
</blockquote>

<p>There are <a href="http://code.google.com/p/mysql-cacti-templates/wiki/UpgradingTemplates">upgrade instructions</a> on the project wiki for this and all releases. There is also a comprehensive tutorial on <a href="http://code.google.com/p/mysql-cacti-templates/wiki/CreatingGraphs">how to create your own graphs and templates</a> with this project. Use the <a href="http://code.google.com/p/mysql-cacti-templates/issues/list">project issue tracker</a> (<strong>not the comments on this post!</strong>) to view and report issues, and use the <a href="http://groups.google.com/group/better-cacti-templates">project mailing list</a> to discuss the templates and scripts.</p>

<p>The full changelog follows.</p>

<pre>
2011-01-22: version 1.1.8

  * The cache file names could conflict due to omitting --port (issue 171).
  * Load-average parsing did not work correctly at high load (issue 170).
  * The --mpds option to make-template.pl did not create new inputs (issue 133).
  * The url and port were reversed in the Nginx commandline (issue 149).
  * Added $nc_cmd to ss_get_by_ssh.php (issue 154, issue 152).
  * InnoDB Transactions and other graphs showed NaN instead of 0 (issue 159).
  * Added graphs for Percona Server response-time distribution (issue 158).
  * Added graphs for MongoDB (issue 136).
  * Added a minimum option to the template construction logic (issue 169).
  * Added memtotal for Memory (issue 146).
  * make-template.pl sanity checks were too strict (issue 168).
</pre>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.xaprb.com/blog/2011/01/22/version-1-1-8-of-better-cacti-templates-released/"> on January 22, 2011 12:00 AM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.xaprb.com/blog/2010/12/19/schedule-for-mysql-and-beyond-conference-is-live/" title="">Schedule for MySQL-and-beyond conference is live</a></h2>

<h3>Baron Schwartz</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<p><a href="http://en.oreilly.com/mysql2011/">O&rsquo;Reilly&rsquo;s 2011 edition of the MySQL conference</a> has an expanded agenda, with good representation from Postgres, CouchDB, MongoDB, and others. Take a look at <a href="http://en.oreilly.com/mysql2011/public/schedule/full">the full schedule listing</a>, which is being filled out as talks are approved and the speakers verify that they&rsquo;ll give the session.</p>

<p>I am certainly looking forward to this year&rsquo;s event. A tremendous amount of progress has landed in GA versions of open-source databases this year. To name just a couple, there&rsquo;s a new version of Postgres (9.0) with built-in replication and many more improvements; there&rsquo;s MySQL 5.5 GA; there&rsquo;s the HandlerSocket NoSQL interface to MySQL; Drizzle has a beta release; and the list goes on. I believe that this conference will have balanced and representative coverage of what&rsquo;s really important to users. It isn&rsquo;t dominated by any vendor this year; O&rsquo;Reilly is running the conference independently, and the committee members represent a broad spectrum of databases themselves.</p>

<p>In short, I am happier than I&rsquo;ve ever been about this great and unique conference. It&rsquo;s definitely going to be the best year so far. Thank you O&rsquo;Reilly for holding it, and thank you to all the great speakers, and thanks to all the companies who sponsor the event.</p>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.xaprb.com/blog/2010/12/19/schedule-for-mysql-and-beyond-conference-is-live/"> on December 19, 2010 12:00 AM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.xaprb.com/blog/2010/12/14/a-review-of-mongodb-the-definitive-guide-by-chodorow-and-dirolf/" title="">A review of MongoDB, the Definitive Guide by Chodorow and Dirolf</a></h2>

<h3>Baron Schwartz</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<p>
  <div id="attachment_2114" class="wp-caption alignleft">
    <a href="http://www.amazon.com/dp/1449381561?tag=xaprb-20"><img src="http://www.xaprb.com/media/2010/12/mongodb-definitive-guide.gif" alt="MongoDB, the Definitive Guide" title="MongoDB, the Definitive Guide" width="180" height="236" class="size-full wp-image-2114" /></a><p class="wp-caption-text">
      MongoDB, the Definitive Guide
    </p>
  </div>
</p>

<p><a href="http://www.amazon.com/dp/1449381561?tag=xaprb-20">MongoDB, the Definitive Guide</a>, by Kristina Chodorow and Michael Dirolf, 2010. About 200 pages. (Here&rsquo;s a <a href="http://oreilly.com/catalog/0636920001096">link to the publisher&rsquo;s site</a>.)</p>

<p>This is a good introduction to MongoDB, mostly from the application developer&rsquo;s point of view. After reading through this, I felt that I understood the concepts well, although I am not a MongoDB expert, so I can&rsquo;t pretend to be a fact-checker. The topics are clearly and logically presented for the most part; there is a small amount of repetition in one of the appendixes, but I don&rsquo;t mind that. The writing and editing is top-notch, as I&rsquo;ve come to expect from O&rsquo;Reilly.</p>

<p>Read this book if you want to learn what MongoDB is, what it does, and how to use it. Don&rsquo;t expect that you will learn everything there is to know about topics such as administration and tuning, although it&rsquo;ll be a good start. (The MongoDB documentation is an excellent reference to continue your education in those areas.)</p>

<p>You might be pleasantly surprised at the lack of hype in this book. It wasn&rsquo;t written by wide-eyed fanboys, and it does mention the weaknesses of MongoDB, although it understandably doesn&rsquo;t spend any time bashing MongoDB for having shortcomings. I think you&rsquo;ll get a balanced view of the database&rsquo;s strengths and weaknesses, certainly enough to make a responsible decision about whether it&rsquo;s worth investigating more deeply.</p>

<p>To sum up, as I wrote to the authors, &ldquo;Nice book. Very well written, very clear and objective.&rdquo;</p>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.xaprb.com/blog/2010/12/14/a-review-of-mongodb-the-definitive-guide-by-chodorow-and-dirolf/"> on December 14, 2010 12:00 AM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.xaprb.com/blog/2010/06/29/theres-a-european-opensql-camp-coming-up/" title="">There's a European OpenSQL Camp coming up</a></h2>

<h3>Baron Schwartz</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<p>In addition to the Boston edition, there&rsquo;s an <a href="http://opensqlcamp.org/Events/FrOSCon2010/">OpenSQL Camp at the same time and place as FrOSCon</a> mid-August in Germany. The call for papers is open until July 11th. As always, the conference is about all kinds of open-source databases: MySQL and PostgreSQL are only two of the obvious ones; MongoDB and Cassandra featured prominently at the last one I attended, and SQLite was well represented at the first one.</p>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.xaprb.com/blog/2010/06/29/theres-a-european-opensql-camp-coming-up/"> on June 29, 2010 12:00 AM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.xaprb.com/blog/2010/06/25/opensql-camp-boston-2010/" title="">OpenSQL Camp Boston 2010</a></h2>

<h3>Baron Schwartz</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<p>Sheeri and others are organizing another incarnation of <a href="http://opensqlcamp.org/Events/Boston2010/">OpenSQL Camp in October in Boston</a>. You ought to go! It&rsquo;s relevant to MySQL, PostgreSQL, SQLite, and lots of the newer generation of databases &ndash; MongoDB, Cassandra, and so on.</p>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.xaprb.com/blog/2010/06/25/opensql-camp-boston-2010/"> on June 25, 2010 12:00 AM</a></span>
		</div>
	</div><!-- .post -->

	<div class="hentry post">
		<div class="postHeader">
	<img class="face" src="images/nobody.png" alt="">

			<h2 class="entry-title"><a href="http://www.xaprb.com/blog/2009/11/17/recap-of-portland-opensql-camp-2009/" title="">Recap of Portland OpenSQL Camp 2009</a></h2>

<h3>Baron Schwartz</h3>

			<div class="clearBoth"></div>
		</div>
		<div class="entry-content">
<p>I was at <a href="http://opensqlcamp.org/Events/Portland2009/">OpenSQL Camp 2009 in Portland</a> last weekend. I thought the event was very well done. On Friday we had a pizza party at Old Town Pizza, which was awesome. Saturday and Sunday were breakfast, sessions, lunch (yum), and sessions and hacking. These were held at <a href="http://www.soukllc.com/">souk</a>, a co-working space. After 5PM, people got together for dinner, beer, etc.</p>

<p>I presented on mk-query-digest &ndash; a live demo of features requested by the audience. Sessions from others that I thought were particularly good included ones on CouchDB and MongoDB. I mixed up the time and missed the session from Tokutek on how fractal tree indexes work. I&rsquo;ll try to watch the video if that one was taped.</p>

<p>During the hackathons, Daniel and I worked on Maatkit. We are laying groundwork for a more powerful mk-query-digest.</p>

<p>As you may know, I created OpenSQL Camp. But I was not involved in organizing this or the previous event in Germany, which I think is great. I talked briefly with Eric and Selena about seeing if we could put together a recipe to make the process easy for folks to organize their own. We should be able to lay out checklists and timelines of major things &ndash; location, shirts, sponsorship, budgeting, food. Eric and Selena got great food, much better than the Panera catering I had for the first event. Those kinds of decisions and results should be recorded. It would be great to be able to treat it like a franchise so anyone could just add water and make their own.</p>

<p>I also might be willing to help organize another on the East Coast, perhaps as soon as next year if I can reduce my workload enough to have the time. I&rsquo;d probably want to do something in or near Washington DC, which is a more convenient location with better public transport than my hometown of Charlottesville.</p>

<p>It all started out as a response to <a href="http://groups.google.com/group/oursql-conference">complaints</a> about MySQL&rsquo;s annual conference not being a user&rsquo;s conference, but nobody actually doing anything about it. I decided to do something about it, in a more inclusive way. And judging by the attendees and talks at the two I&rsquo;ve gone to, people were happy to say yes to that. I think if there are continued events, that&rsquo;s the ultimate measure of success.</p>			<div class="clearBoth"></div>
		</div>
		<div class="entry-utility">
			<span class="cat-links"><a href="http://www.xaprb.com/blog/2009/11/17/recap-of-portland-opensql-camp-2009/"> on November 17, 2009 12:00 AM</a></span>
		</div>
	</div><!-- .post -->

			</div><!-- #content -->
		</div><!-- #container -->

		<div id="secondary" class="aside main-aside">
			<ul class="xoxo">
				<li class="widgetcontainer widget_text">
					<h3 class="widgettitle">About the Community MongoDB Blog Aggregator</h3>
					<p>This website is a community-maintained and inclusive feed of MongoDB-related blog posts. To add your blog's feed, please fork <a href="https://github.com/xaprb/planetmongo-src">the repository</a>, modify the config.ini file to add your feed at the end, and submit a pull request.</p>
				</li>
				<li class="widgetcontainer widget_extended-categories">
					<h3 class="widgettitle">Feeds</h3>
					<ul>
						<li>
							<a href="http://www.xaprb.com/tags/mongodb/index.xml" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)" /></a> <a href="http://www.xaprb.com/tags/mongodb/" title="">Baron Schwartz</a>
						</li>
						<li>
							<a href="http://www.acmebenchmarking.com//feeds/posts/default" title="subscribe"><img src="images/feed-icon-10x10.png" alt="(feed)" /></a> <a href="http://www.acmebenchmarking.com/" title="Acme Benchmarking">Tim Callaghan</a>
						</li>
					</ul>
				</li>
			</ul>
		</div><!-- #secondary .aside -->
		<div class="clearBoth"></div>
	</div><!-- #main -->
</div><!-- #wrapper .hfeed -->

<div id="footer">
	<div id="siteinfo">
		<p>
			Blog entries aggregated on this page are owned by, and represent the opinion of the author.<br />
			&copy; Baron Schwartz<br />
		</p>
	</div><!-- #siteinfo -->
</div><!-- #footer -->
</body>
</html>
