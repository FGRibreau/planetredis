<?xml version="1.0"?>
<rss version="2.0">

<channel>
	<title>Community MongoDB Aggregator</title>
	<link>http://www.planetmongo.org/</link>
	<language>en</language>
	<description>Community MongoDB Aggregator - http://www.planetmongo.org/</description>

<item>
	<title>MongoDB Spain: MongoDB reaches 10M+ downloads</title>
	<guid>http://www.mongodbspain.com/?p=2489</guid>
	<link>http://www.mongodbspain.com/en/2015/06/02/mongodb-reaches-10m-downloads/</link>
	<description>&lt;p&gt;As announced in early morning in the social networks, coinciding with the celebration of the MongoDB World 2015, it&amp;#8217;s official that MongoDB has reached 10,000,000 downloads, with the help of the whole community that continues supporting the biggest NoSQL database.&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; width=&quot;550&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;HUGE thank you to our users for helping use reach 10M+ downloads of MongoDB! We couldn't have done it w/out you. &lt;a href=&quot;http://t.co/QqoGZ6a8v7&quot;&gt;pic.twitter.com/QqoGZ6a8v7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&amp;mdash; MongoDB (@MongoDB) &lt;a href=&quot;https://twitter.com/MongoDB/status/605819536927899648&quot;&gt;June 2, 2015&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;It is important to thank you all for your support and loyalty.&lt;br /&gt;
We expect to continue growing with you.&lt;/p&gt;</description>
	<pubDate>Tue, 02 Jun 2015 21:59:05 +0000</pubDate>
</item>
<item>
	<title>A. Jesse Jiryu Davis: Begging, 2015</title>
	<guid>http://emptysqua.re/blog/begging-2015/</guid>
	<link>http://emptysqua.re/blog/begging-2015/</link>
	<description>&lt;p&gt;&lt;img src=&quot;http://emptysquare.net/blog/media/2013/04/7161960026_e92ea3c4bb.jpg&quot; alt=&quot;7161960026 e92ea3c4bb&quot; title=&quot;7161960026_e92ea3c4bb.jpg&quot; border=&quot;0&quot; /&gt;&lt;/p&gt;
&lt;p&gt;This July I'll spend four days homeless, with a Zen teacher named Genro and a small group of fellow Buddhists. We'll live, sleep, and meditate on the streets together and eat at soup kitchens. I think the retreat has a triple purpose: First, briefly abandoning the comfort and certainty of my regular life helps me practice non-attachment, the same as it helped the first Buddhist monks. Second, it gives me a taste of what it's like to be homeless, so I can better understand the homeless people I meet in NYC. And finally, it's an opportunity to raise money for homeless services.&lt;/p&gt;
&lt;p&gt;For the most part we blend in with the homeless population. On the occasions I've explained to street folk what we're doing, they have without exception appreciated the effort we make to walk a mile in their shoes.&lt;/p&gt;
&lt;p&gt;People often ask me whether, by doing a street retreat, we're competing with homeless people for scarce resources. I think not&amp;mdash;we stay at the back of the line in soup kitchens, and we sleep on sidewalks instead of in shelters. Although we beg for a few dollars on the street, we raise thousands of dollars for homeless services.&lt;/p&gt;
&lt;p&gt;That's where you come in: I have to raise $500 by July. The money will be distributed among the organizations that help us while we're on the street, places like the Catholic Worker, Coalition for the Homeless, and the Bowery Mission. And some of the money will support the social service activities of the &lt;a href=&quot;http://hudsonriverzencenter.org/&quot;&gt;Hudson River Zen Center&lt;/a&gt;. I'm not allowed to just use $500 of my own. I have to be humble and ask for the money from you.&lt;/p&gt;
&lt;p&gt;So I'm begging you: Will you please donate?&lt;/p&gt;
&lt;div&gt;
&lt;form action=&quot;https://www.paypal.com/cgi-bin/webscr&quot; method=&quot;post&quot; target=&quot;_top&quot;&gt;
&lt;input type=&quot;hidden&quot; name=&quot;cmd&quot; value=&quot;_s-xclick&quot; /&gt;
&lt;input type=&quot;hidden&quot; name=&quot;hosted_button_id&quot; value=&quot;73E4KKFX8C2A8&quot; /&gt;
&lt;input type=&quot;image&quot; src=&quot;https://www.paypalobjects.com/en_US/i/btn/btn_donate_LG.gif&quot; border=&quot;0&quot; name=&quot;submit&quot; alt=&quot;PayPal - The safer, easier way to pay online!&quot; /&gt;
&lt;img alt=&quot;&quot; border=&quot;0&quot; src=&quot;https://www.paypalobjects.com/en_US/i/scr/pixel.gif&quot; width=&quot;1&quot; height=&quot;1&quot; /&gt;
&lt;/form&gt;
&lt;/div&gt;

&lt;p&gt;(Tax-deductible. If you want a receipt, &lt;a href=&quot;mailto:jesse@emptysquare.net&quot;&gt;let me know&lt;/a&gt;.)&lt;/p&gt;&lt;img src=&quot;http://emptysqua.re/blog/analytics/http%3A//emptysqua.re/blog/begging-2015//pixel.gif&quot; width=&quot;1px&quot; height=&quot;1px&quot; /&gt;</description>
	<pubDate>Fri, 29 May 2015 19:24:47 +0000</pubDate>
	<author>ajdavis@cs.oberlin.edu (A. Jesse Jiryu Davis)</author>
</item>
<item>
	<title>Gustavo Niemeyer: mgo r2015.05.29</title>
	<guid>http://blog.labix.org/?p=2692</guid>
	<link>http://blog.labix.org/2015/05/29/mgo-r2015-05-29</link>
	<description>&lt;p&gt;Another release of mgo hits the shelves, just in time for the upcoming &lt;a href=&quot;http://mongodbworld.com/&quot;&gt;MongoDB World&lt;/a&gt; event.&lt;/p&gt;
&lt;p&gt;A number of of relevant improvements have landed since the last stable release:&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;more-2692&quot;&gt;&lt;/span&gt;&lt;br /&gt;
&lt;strong&gt;New package for having a MongoDB server in test suites&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The new &lt;a href=&quot;http://gopkg.in/mgo.v2/testserver#TestServer&quot;&gt;gopkg.in/mgo.v2/testserver&lt;/a&gt; package makes it comfortable to plug a real MongoDB server into test suites. Its simple interface consists of a handful of methods, which together allow obtaining a new mgo session to the server, wiping all existent data, or stopping it altogether once the suite is done. This design encourages an efficient use of resources, by only starting the server if necessary, and then quickly cleaning data across runs instead of restarting the server.&lt;/p&gt;
&lt;p&gt;See the &lt;a href=&quot;http://gopkg.in/mgo.v2/testserver#TestServer&quot;&gt;documentation&lt;/a&gt; for more details.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Full support for write commands&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This release includes full support for the write commands first introduced in MongoDB 2.6. This was done in a compatible way, both in the sense that the driver will continue to use the wire protocol to perform writes on older servers, and also in the sense that the public API has not changed.&lt;/p&gt;
&lt;p&gt;Tests for the new code path have been successfully run against MongoDB 2.6 and 3.0. Even then, as an additional measure to prevent breakage of existent applications, in this release the new code path will be enabled only when interacting with MongoDB 3.0+. The next stable release should then enable it for earlier releases as well, after some additional real world usage took place.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;New ParseURL function&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As perhaps one of the most requested features of all times, there&amp;#8217;s now a public &lt;a href=&quot;http://gopkg.in/mgo.v2#ParseURL&quot;&gt;ParseURL&lt;/a&gt; function which allows code to parse a URL in any of the formats accepted by &lt;a href=&quot;http://gopkg.in/mgo.v2#Dial&quot;&gt;Dial&lt;/a&gt; into a &lt;a href=&quot;http://gopkg.in/mgo.v2#DialInfo&quot;&gt;DialInfo&lt;/a&gt; value which may be provided back into &lt;a href=&quot;http://gopkg.in/mgo.v2#DialWithInfo&quot;&gt;DialWithInfo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;New BucketSize field in mgo.Index&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The new BucketSize field in &lt;a href=&quot;http://gopkg.in/mgo.v2#Index&quot;&gt;mgo.Index&lt;/a&gt; supports the use of indexes of type &lt;code&gt;geoHaystack&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Contributed by Deiwin Sarjas.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Handle Setter and Getter interfaces in slice types&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Slice types that implement the &lt;a href=&quot;http://gopkg.in/mgo.v2/bson#Getter&quot;&gt;Getter&lt;/a&gt; and/or &lt;a href=&quot;http://gopkg.in/mgo.v2/bson#Setter&quot;&gt;Setter&lt;/a&gt; interfaces will now be custom encoded/decoded as usual for other types.&lt;/p&gt;
&lt;p&gt;Problem reported by Thomas Bouldin.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;New Query.SetMaxTime method&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The new &lt;a href=&quot;http://gopkg.in/mgo.v2#Query.SetMaxTime&quot;&gt;Query.SetMaxTime&lt;/a&gt; method enables the use of the special &lt;a href=&quot;http://blog.mongodb.org/post/83621787773&quot;&gt;$maxTimeMS&lt;/a&gt; query parameter, which constrains the query to stop after running for the specified time. See the method documentation for details.&lt;/p&gt;
&lt;p&gt;Feature implemented by Min-Young Wu.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;New Query.Comment method&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The new &lt;a href=&quot;http://gopkg.in/mgo.v2#Query.Comment&quot;&gt;Query.Comment&lt;/a&gt; method may be used to annotate queries for further analysis within the profiling data.&lt;/p&gt;
&lt;p&gt;Feature requested by Mike O&amp;#8217;Brien.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;sasl sub-package moved into internal&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The sasl sub-package is part of the implementation of SASL support in mgo, and is not meant to be accessed directly. For that reason, that package was moved to internal/sasl, which according to recent Go conventions is meant to explicitly flag that this is part of mgo&amp;#8217;s implementation rather than its public API.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Improvements in txn&amp;#8217;s PurgeMissing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The PurgeMissing logic was improved to work better in older server versions which retained all aggregation pipeline results in memory.&lt;/p&gt;
&lt;p&gt;Improvements made by Menno Smits.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fix connection statistics bug&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Change prevents the number of slave connections from going negative on a particular case.&lt;/p&gt;
&lt;p&gt;Fix by Oleg Bulatov.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;EnsureIndex support for createIndexes command&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;http://gopkg.in/mgo.v2#Collection.EnsureIndex&quot;&gt;EnsureIndex&lt;/a&gt; method will now use the createIndexes command where available.&lt;/p&gt;
&lt;p&gt;Feature requested by Louisa Berger.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Support encoding byte arrays&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Support encoding byte arrays in an equivalent way to byte slices.&lt;/p&gt;
&lt;p&gt;Contributed by Tej Chajed.&lt;/p&gt;</description>
	<pubDate>Fri, 29 May 2015 12:55:25 +0000</pubDate>
</item>
<item>
	<title>MongoDB Management Service: How Many Agents Do I Need (and Why)?</title>
	<guid>http://blog.mms.mongodb.com/post/120124825840</guid>
	<link>http://blog.mms.mongodb.com/post/120124825840</link>
	<description>&lt;p&gt;If you’re an MMS user, you may be familiar with the concept of the MMS Agents. MMS uses agents to get information into and out of your deployment without needing direct network access. All of our Agents operate by making HTTPS requests out to MMS and then performing MongoDB commands within your data center. This blog post is here to help you know how many of each agent you need and why.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Automation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Every server instance in your deployment needs an Automation Agent to use Automation. Only one Automation Agent is required per server instance, each one can manage multiple MongoDB instances. Running multiple Automation Agents per server is not supported and will not work. The Automation Agent is responsible for configuring, launching, and maintaining MongoDB processes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://41.media.tumblr.com/dcdfb56a16c369cf08fe894ecbba60d1/tumblr_np2piq1mh21sdaytmo2_1280.png&quot;&gt;&lt;img src=&quot;https://41.media.tumblr.com/dcdfb56a16c369cf08fe894ecbba60d1/tumblr_np2piq1mh21sdaytmo2_1280.png&quot; alt=&quot;Automation Agents&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Monitoring&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Every MMS group has only one active Monitoring Agent. This Monitoring Agent must be able to reach every member of your MongoDB deployment in the MMS group. You can run more than one Monitoring Agent, but more than three is a bit excessive. The advantage of running multiple Monitoring Agents is only in terms of increasing availability in case one goes down – they do not allow for ways to get to hosts that are not normally accessible.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://41.media.tumblr.com/566dc7e75737535d68151a5fd26c8918/tumblr_np2piq1mh21sdaytmo3_1280.png&quot;&gt;&lt;img src=&quot;https://41.media.tumblr.com/566dc7e75737535d68151a5fd26c8918/tumblr_np2piq1mh21sdaytmo3_1280.png&quot; alt=&quot;Monitoring Agent&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Backup&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Backup Agents have similar characteristics to Monitoring Agents. There is only one active Backup Agent per MMS group, and more than one only provides high availability, not network access. Each Backup Agent must be able to connect to every process in your MMS deployment by the hostnames that MMS uses.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://41.media.tumblr.com/ee389c27b3361ac445e1429f07c900d4/tumblr_np2piq1mh21sdaytmo1_1280.png&quot;&gt;&lt;img src=&quot;https://41.media.tumblr.com/ee389c27b3361ac445e1429f07c900d4/tumblr_np2piq1mh21sdaytmo1_1280.png&quot; alt=&quot;Backup Agent&quot; /&gt;&lt;/a&gt;&lt;/p&gt;</description>
	<pubDate>Thu, 28 May 2015 19:36:48 +0000</pubDate>
</item>
<item>
	<title>A. Jesse Jiryu Davis: Tornado Locks And Queues (The End Of Toro)</title>
	<guid>http://emptysqua.re/blog/tornado-locks-and-queues/</guid>
	<link>http://emptysqua.re/blog/tornado-locks-and-queues/</link>
	<description>&lt;p&gt;&lt;img src=&quot;http://emptysquare.net/blog/media/2012/11/toro.png&quot; alt=&quot;Toro&quot; title=&quot;toro.png&quot; border=&quot;0&quot; /&gt;&lt;/p&gt;
&lt;p&gt;On Tuesday, Ben Darnell released Tornado 4.2, with two new modules: &lt;a href=&quot;http://www.tornadoweb.org/en/stable/locks.html&quot;&gt;tornado.locks&lt;/a&gt; and &lt;a href=&quot;http://www.tornadoweb.org/en/stable/queues.html&quot;&gt;tornado.queues&lt;/a&gt;. These new modules help you coordinate Tornado's asynchronous coroutines with patterns familiar from multi-threaded programming.&lt;/p&gt;
&lt;p&gt;I originally developed these features in my &lt;a href=&quot;https://toro.readthedocs.org/&quot;&gt;Toro&lt;/a&gt; package, which I began almost three years ago, and I'm honored that Ben has adopted my code into Tornado's core. It's a bit sad, though, because this is the end of the line for Toro, one of the best ideas of my career. Skip to the bottom for my thoughts on Toro's retirement.&lt;/p&gt;
&lt;p&gt;The classes Condition and Queue are representative of Tornado's new features. Here's how one coroutine signals another, using a Condition:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;condition &lt;span&gt;=&lt;/span&gt; locks&lt;span&gt;.&lt;/span&gt;Condition()

&lt;span&gt;@gen.coroutine&lt;/span&gt;
&lt;span&gt;def&lt;/span&gt; &lt;span&gt;waiter&lt;/span&gt;():
    &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&amp;quot;I'll wait right here&amp;quot;&lt;/span&gt;)
    &lt;span&gt;yield&lt;/span&gt; condition&lt;span&gt;.&lt;/span&gt;wait()  &lt;span&gt;# Yield a Future.&lt;/span&gt;
    &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&amp;quot;I'm done waiting&amp;quot;&lt;/span&gt;)

&lt;span&gt;@gen.coroutine&lt;/span&gt;
&lt;span&gt;def&lt;/span&gt; &lt;span&gt;notifier&lt;/span&gt;():
    &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&amp;quot;About to notify&amp;quot;&lt;/span&gt;)
    condition&lt;span&gt;.&lt;/span&gt;notify()
    &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&amp;quot;Done notifying&amp;quot;&lt;/span&gt;)

&lt;span&gt;@gen.coroutine&lt;/span&gt;
&lt;span&gt;def&lt;/span&gt; &lt;span&gt;runner&lt;/span&gt;():
    &lt;span&gt;# Yield two Futures; wait for waiter() and notifier() to finish.&lt;/span&gt;
    &lt;span&gt;yield&lt;/span&gt; [waiter(), notifier()]

io_loop&lt;span&gt;.&lt;/span&gt;run_sync(runner)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This script prints:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;I&lt;span&gt;'&lt;/span&gt;ll wait right here
About to notify
Done notifying
I&lt;span&gt;'&lt;/span&gt;m done waiting
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As you can see, the Condition interface is close to the Python standard library's Condition. But instead of coordinating threads, Tornado's Condition coordinates asynchronous coroutines.&lt;/p&gt;
&lt;p&gt;Tornado's Queue is similarly analogous to the standard Queue:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;q &lt;span&gt;=&lt;/span&gt; queues&lt;span&gt;.&lt;/span&gt;Queue(maxsize&lt;span&gt;=2&lt;/span&gt;)

&lt;span&gt;@gen.coroutine&lt;/span&gt;
&lt;span&gt;def&lt;/span&gt; &lt;span&gt;consumer&lt;/span&gt;():
    &lt;span&gt;while&lt;/span&gt; &lt;span&gt;True&lt;/span&gt;:
        item &lt;span&gt;=&lt;/span&gt; &lt;span&gt;yield&lt;/span&gt; q&lt;span&gt;.&lt;/span&gt;get()
        &lt;span&gt;try&lt;/span&gt;:
            &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;'Doing work on &lt;/span&gt;&lt;span&gt;%s&lt;/span&gt;&lt;span&gt;'&lt;/span&gt; &lt;span&gt;%&lt;/span&gt; item)
            &lt;span&gt;yield&lt;/span&gt; gen&lt;span&gt;.&lt;/span&gt;sleep(&lt;span&gt;0.01&lt;/span&gt;)
        &lt;span&gt;finally&lt;/span&gt;:
            q&lt;span&gt;.&lt;/span&gt;task_done()

&lt;span&gt;@gen.coroutine&lt;/span&gt;
&lt;span&gt;def&lt;/span&gt; &lt;span&gt;producer&lt;/span&gt;():
    &lt;span&gt;for&lt;/span&gt; item &lt;span&gt;in&lt;/span&gt; &lt;span&gt;range&lt;/span&gt;(&lt;span&gt;5&lt;/span&gt;):
        &lt;span&gt;yield&lt;/span&gt; q&lt;span&gt;.&lt;/span&gt;put(item)
        &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;'Put &lt;/span&gt;&lt;span&gt;%s&lt;/span&gt;&lt;span&gt;'&lt;/span&gt; &lt;span&gt;%&lt;/span&gt; item)

&lt;span&gt;@gen.coroutine&lt;/span&gt;
&lt;span&gt;def&lt;/span&gt; &lt;span&gt;main&lt;/span&gt;():
    consumer()           &lt;span&gt;# Start consumer.&lt;/span&gt;
    &lt;span&gt;yield&lt;/span&gt; producer()     &lt;span&gt;# Wait for producer to put all tasks.&lt;/span&gt;
    &lt;span&gt;yield&lt;/span&gt; q&lt;span&gt;.&lt;/span&gt;join()       &lt;span&gt;# Wait for consumer to finish all tasks.&lt;/span&gt;
    &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;'Done'&lt;/span&gt;)

io_loop&lt;span&gt;.&lt;/span&gt;run_sync(main)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This will print:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;Put 0
Put 1
Put 2
Doing work on 0
Doing work on 1
Put 3
Doing work on 2
Put 4
Doing work on 3
Doing work on 4
Done
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Tornado's new locks and queues implement the same familiar patterns we've used for decades to coordinate threads. There's no need to invent these techniques anew for coroutines.&lt;/p&gt;
&lt;p&gt;I was inspired to write these classes in 2012, when I was deep in the initial implementation of &lt;a href=&quot;https://motor.readthedocs.org/&quot;&gt;Motor&lt;/a&gt;, my MongoDB driver for Tornado. The time I spent learning about coroutines for Motor's sake provoked me to wonder, how far could I push them? How much of the threading API was applicable to coroutines? The outcome was Toro&amp;mdash;not necessarily evidence of my genius, but a very good idea that led me far. Toro's scope was straightforward, and I had to make very few decisions. The initial implementation took a week or two. I commissioned the cute bull character from &lt;a href=&quot;http://whimsyload.com/&quot;&gt;Musho Rodney Alan Greenblat&lt;/a&gt;. The cuteness of Musho's art matched the simplicity of Toro's purpose.&lt;/p&gt;
&lt;p&gt;When I heard about Guido van Rossum's Tulip project at his PyCon talk in 2013, I thought he could use Toro's locks and queues. It would be an excuse for me to work with Guido. I found that Tulip already had locks, implemented by Nikolay Kim if I remember right, but it didn't have queues yet so I jumped in and contributed mine. It was a chance to be code-reviewed by Guido and other Python core developers. In the long run, when Tulip became the &lt;code&gt;asyncio&lt;/code&gt; standard library module, &lt;a href=&quot;https://docs.python.org/3.4/library/asyncio-queue.html&quot;&gt;my queues&lt;/a&gt; became my first big contribution to the Python standard library.&lt;/p&gt;
&lt;p&gt;Toro has led me to collaborate with Guido van Rossum and Ben Darnell, two of the coders I admire most. And now Toro's life is over. Its code is split up and merged into much larger and better-known projects. The name &quot;Toro&quot; and the character are relics. When I find the time I'll post the deprecation notice and direct people to use the locks and queues in Tornado core. Toro was the most productive idea of my career. Now I'm waiting for the next one.&lt;/p&gt;&lt;img src=&quot;http://emptysqua.re/blog/analytics/http%3A//emptysqua.re/blog/tornado-locks-and-queues//pixel.gif&quot; width=&quot;1px&quot; height=&quot;1px&quot; /&gt;</description>
	<pubDate>Thu, 28 May 2015 15:46:28 +0000</pubDate>
	<author>ajdavis@cs.oberlin.edu (A. Jesse Jiryu Davis)</author>
</item>
<item>
	<title>Baron Schwartz: History Repeats: MySQL, MongoDB, Percona, and Open Source</title>
	<guid>http://www.xaprb.com/blog/2015/05/22/percona-mongodb-mysql-history-repeat/</guid>
	<link>http://www.xaprb.com/blog/2015/05/22/percona-mongodb-mysql-history-repeat/</link>
	<description>&lt;p&gt;History is repeating again. MongoDB is breaking out of the niche into the
mainstream, performance and instrumentation are terrible in specific cases,
MongoDB isn&amp;rsquo;t able to fix all the problems alone, and an ecosystem is growing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.xaprb.com/media/2015/05/leaf.jpg&quot; alt=&quot;Leaf&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;This should really be a series of blog posts, because there&amp;rsquo;s a book&amp;rsquo;s worth of
things happening, but I&amp;rsquo;ll summarize instead. Randomly ordered:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;MongoDB is in many respects closely following MySQL&amp;rsquo;s development, 10 years
offset. Single index per query, MyISAM-like storage engine, etc.
&lt;a href=&quot;http://www.xaprb.com/blog/2013/04/29/what-tokudb-might-mean-for-mongodb/&quot;&gt;Background&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Tokutek built an excellent transactional storage engine and replaced
MongoDB&amp;rsquo;s, calling it TokuMX. Results were dramatically better performance
(plus ACID). MongoDB&amp;rsquo;s response was to buy WiredTiger and make it the default
storage engine in MongoDB 3.0.&lt;/li&gt;
&lt;li&gt;Percona acquired Tokutek. A book should be written about this someday. The
impact to both the MySQL and MongoDB communities cannot be overstated. This
changes everything. It also changes everything for Percona, which now has a
truly differentiated product for both database offerings. This moves them
solidly into being a product company, not just support/services/consulting; it
is a good answer to the quandary of trying to keep up with the InnoDB
engineers.&lt;/li&gt;
&lt;li&gt;Facebook acquired Parse, which is probably one of the larger MongoDB
installations.&lt;/li&gt;
&lt;li&gt;Facebook&amp;rsquo;s Mark Callaghan, among others, stopped spending all his time on
InnoDB mutexes and so forth. For the last year or so he&amp;rsquo;s been extremely
active in the MongoDB community. The MongoDB community is lucky to have a
genius of Mark&amp;rsquo;s caliber finding and solving problems. There are others, but
if Mark Callaghan is working on your open source product in earnest, you&amp;rsquo;ve
arrived.&lt;/li&gt;
&lt;li&gt;Just as in MySQL, but even earlier, there are lots of -As-A-Service providers
for MongoDB, and it&amp;rsquo;s likely a significant portion of future growth happens
here.&lt;/li&gt;
&lt;li&gt;MongoDB&amp;rsquo;s conference is jaw-droppingly expensive for a vendor, to the point of
being exclusive. At the same time, MongoDB hasn&amp;rsquo;t quite recognized and
embraced some of the things going on outside their walls. If you remember &lt;a href=&quot;https://www.percona.com/blog/2009/02/05/announcing-percona-performance-conference-2009-on-april-22-23/&quot;&gt;the
events of 2009 in the MySQL
community&lt;/a&gt;,
Percona&amp;rsquo;s &lt;a href=&quot;https://www.percona.com/news-and-events/mongodb-events/mongodb-community-openhouse&quot;&gt;announcement of an alternative MongoDB
conference&lt;/a&gt;
might feel a little like deja vu. I&amp;rsquo;m not sure of the backstory behind this,
though.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At the same time that history is repeating in the MongoDB world, a tremendous
amount of stuff is happening quietly in other major communities too. Especially
MySQL, but also in PostgreSQL, ElasticSearch, Cassandra and other opensource
databases. I&amp;rsquo;m probably only qualified to write about the MySQL side of things;
I&amp;rsquo;m pretty sure most people don&amp;rsquo;t know a lot of the interesting things that are
going on behind the scenes that will have long-lasting effects. Maybe I&amp;rsquo;ll write
about that someday.&lt;/p&gt;

&lt;p&gt;In the meanwhile, I think we&amp;rsquo;re all in for an exciting ride as MongoDB &lt;a href=&quot;http://www.xaprb.com/blog/2013/01/10/bold-predictions-on-which-nosql-databases-will-survive/&quot;&gt;proves me right&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;PS: VividCortex is building a MongoDB monitoring solution that will address many
of the shortcomings of existing ones. (We have been a bit quiet about it, just
out of busyness rather than a desire for secrecy, but now you know.) It&amp;rsquo;s in
beta now.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.flickr.com/photos/96dpi/3645537177/&quot;&gt;Cropped image by 96dpi&lt;/a&gt;&lt;/p&gt;</description>
	<pubDate>Fri, 22 May 2015 18:51:18 +0000</pubDate>
</item>
<item>
	<title>Tim Callaghan: Diving deeper into MongoDB 2.8 collection level locking performance</title>
	<guid>tag:blogger.com,1999:blog-8043938871710850997.post-5804590304723988843</guid>
	<link>http://www.acmebenchmarking.com/2015/01/diving-deeper-into-mongodb-28.html</link>
	<description>&lt;span&gt;Last month I &lt;a href=&quot;http://www.acmebenchmarking.com/2014/12/benchmarking-mongodb-28-mmapv1.html&quot; target=&quot;_blank&quot;&gt;wrote a blog&lt;/a&gt; about the closing of MongoDB ticket &lt;a href=&quot;https://jira.mongodb.org/browse/SERVER-1240&quot; target=&quot;_blank&quot;&gt;SERVER-1240&lt;/a&gt;, which brings Collection Level Locking (CLL) to the MMAPV1 storage engine in MongoDB 2.8.&amp;nbsp;&lt;/span&gt;&lt;span&gt;In MongoDB 2.6 there is a writer lock at the database level, so each database only allows one writer at a time. In concurrent write workloads, this means that all writers essentially form a single line and do their writes one at a time. In MongoDB 2.8 this lock has been moved to the collection level. Better yet is document level locking, but even though this feature was shown at &lt;a href=&quot;http://www.mongodb.com/mongodb-world/presentations&quot; target=&quot;_blank&quot;&gt;MongoDB World 2014&lt;/a&gt;&amp;nbsp;it's not going to ship. But it did make for one amazing demo by &lt;a href=&quot;https://www.linkedin.com/pub/dan-pasette/2/2b2/448&quot; target=&quot;_blank&quot;&gt;Dan&lt;/a&gt; and &lt;a href=&quot;https://www.linkedin.com/in/eliothorowitz&quot; target=&quot;_blank&quot;&gt;Eliot&lt;/a&gt;. Seriously, watch the &lt;a href=&quot;http://www.mongodb.com/presentations/mongodb-world-2014-keynote-eliot-horowitz&quot; target=&quot;_blank&quot;&gt;presentation&lt;/a&gt; for yourself.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;The MMAPV1 engine is still important in MongoDB as it will be the default in 2.8, though the &lt;a href=&quot;http://www.zdnet.com/article/mongodb-cto-how-our-new-wiredtiger-storage-engine-will-earn-its-stripes/&quot; target=&quot;_blank&quot;&gt;plan is to make WiredTiger the default engine in MongoDB 3.0&lt;/a&gt;. Perhaps that plan explains why the performance gains aren't nearly as interesting as one might expect, as MongoDB might simply be focusing their resources on the future default engine.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;I ran a &lt;a href=&quot;http://www.acmebenchmarking.com/2014/12/benchmarking-mongodb-28-mmapv1.html&quot; target=&quot;_blank&quot;&gt;series of benchmarks&lt;/a&gt; last month to see for myself how much of a performance improvement CLL makes, and the performance gains were not what I expected to see.&amp;nbsp;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;This month I decided to drill deeper into a specific use-case, simply inserting data into a single collection. The experiment starts with loading just one collection, then simultaneously loading two, then three, and then four. All collections for the experiments are in the same database, as the point of CLL was to eliminate the workaround of putting each collection into a different database (and each database has it's own write lock). To eliminate variables I ran on a server with plenty of RAM, so the test is not IO limited. The test environment was as follows:&lt;/span&gt;&lt;br /&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;span&gt;&lt;i&gt;Dell R710 server : Ubuntu 14.04, 2 x Intel Xeon L5520 CPUs, 48GB RAM, 8 x 10K SAS in RAID10 (~2000 Random IOPs)&lt;/i&gt;&lt;/span&gt;&lt;/blockquote&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;span&gt;&lt;i&gt;MongoDB 2.6.6 and 2.8.0.RC4&lt;/i&gt;&lt;/span&gt;&lt;/blockquote&gt;&lt;span&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/span&gt;&lt;span&gt;&lt;b&gt;Benchmark 1 : Load data into 1 .. 4 collections&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://3.bp.blogspot.com/-xMRx0jpCfNc/VLPtpJt2BJI/AAAAAAAAB4s/BoV1BFiql-U/s1600/28-cll-benchmark-26-vs-28.png&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;354&quot; src=&quot;http://3.bp.blogspot.com/-xMRx0jpCfNc/VLPtpJt2BJI/AAAAAAAAB4s/BoV1BFiql-U/s1600/28-cll-benchmark-26-vs-28.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;The raw performance by thread count is as follows:&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;MongoDB 2.6.6 =&amp;nbsp;20399,&amp;nbsp;24387,&amp;nbsp;24787,&amp;nbsp;23808&lt;/span&gt;&lt;br /&gt;&lt;span&gt;MongoDB 2.8.0.RC4 =&amp;nbsp;22720,&amp;nbsp;30764,&amp;nbsp;34879,&amp;nbsp;36691&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;Making the performance increase from 1-2, 2-3, and 3-4 threads:&lt;/span&gt;&lt;br /&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;span&gt;MongoDB 2.6.6 = +19.5%,&amp;nbsp;+1.6%, &lt;span&gt;-3.9%&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;MongoDB 2.8.0.RC4 =&amp;nbsp;+35.4%,&amp;nbsp;+11.8%,&amp;nbsp;+5.2%&lt;/span&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;span&gt;This shows that some improvement was made, but it feels to me that there is some other bottleneck in the system. A perfectly scaling system could theoretically increase 100% from 1-2 threads, 50% from 2-3 threads, and 33% from 3-4 threads.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;I decided to rerun the experiment using MongoDB 2.8.0.RC4 with journaling disabled.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;b&gt;Benchmark 2 : Load data into 1 .. 4 collections, with Journaling ON and OFF&lt;/b&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://2.bp.blogspot.com/-Apl8swJB8NA/VLP1p2DX19I/AAAAAAAAB48/nHSbB3rbOKc/s1600/28-cll-benchmark-28-journal-on-vs-off.png&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;354&quot; src=&quot;http://2.bp.blogspot.com/-Apl8swJB8NA/VLP1p2DX19I/AAAAAAAAB48/nHSbB3rbOKc/s1600/28-cll-benchmark-28-journal-on-vs-off.png&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;The raw performance by thread count is as follows:&lt;/span&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;Journal Enabled =&amp;nbsp;22720,&amp;nbsp;30764,&amp;nbsp;34879,&amp;nbsp;36691&lt;/span&gt;&lt;br /&gt;&lt;span&gt;Journal Disabled = 37034, 71410, 103430, 129013&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;Making the performance increase from 1-2, 2-3, and 3-4 threads:&lt;/span&gt;&lt;br /&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;span&gt;Journal Enabled =&amp;nbsp;&lt;/span&gt;&lt;span&gt;+35.4%,&amp;nbsp;+11.8%,&amp;nbsp;+5.2%&lt;/span&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Journal Disabled =&amp;nbsp;&lt;b&gt;+92.8%&lt;/b&gt;,&amp;nbsp;&lt;b&gt;+44.8%&lt;/b&gt;,&amp;nbsp;&lt;b&gt;+24.7%&lt;/b&gt;&lt;/span&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;span&gt;&lt;b&gt;The performance scaling looks much better here, in fact it's almost perfectly linear.&lt;/b&gt; My assumption is that MongoDB did indeed add collection level locking to the MMAPV1 storage engine in 2.8 but that a high-performance group commit algorithm was not included (or considered important).&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;If anyone with more information wants to share, please do. I'm extremely curious as to why the performance with journaling enabled hasn't improved more with collection level locking. And running without a journal is a really bad idea, IMHO.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;</description>
	<pubDate>Thu, 21 May 2015 21:02:36 +0000</pubDate>
	<author>noreply@blogger.com (Tim Callaghan)</author>
</item>
<item>
	<title>Tim Callaghan: Percona Acquires Tokutek : My Thoughts #3 : Fractal Tree Indexes</title>
	<guid>tag:blogger.com,1999:blog-8043938871710850997.post-4792396011832957535</guid>
	<link>http://www.acmebenchmarking.com/2015/05/percona-acquires-tokutek-my-thoughts-3.html</link>
	<description>&lt;span&gt;Last week I wrote up my thoughts about the Percona acquisition of Tokutek from the perspective of &lt;a href=&quot;http://www.acmebenchmarking.com/2015/04/percona-acquires-tokutek-my-thoughts-1.html&quot; target=&quot;_blank&quot;&gt;TokuDB&lt;/a&gt; and &lt;a href=&quot;http://www.acmebenchmarking.com/2015/04/percona-acquires-tokutek-my-thoughts-2.html&quot; target=&quot;_blank&quot;&gt;TokuMX[se]&lt;/a&gt;. In this third blog of the trilogy I'll cover the acquisition and the future of the Fractal Tree Index. The Fractal Tree Index is the foundational technology upon which all Tokutek products are built.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://4.bp.blogspot.com/-1xCQlcZCiDk/VV5vv7vKcoI/AAAAAAAACGo/LYcTiWV1Thk/s1600/FracTree_SS.jpg&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://4.bp.blogspot.com/-1xCQlcZCiDk/VV5vv7vKcoI/AAAAAAAACGo/LYcTiWV1Thk/s1600/FracTree_SS.jpg&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot;&gt;&lt;/div&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;So what is a Fractal Tree Index? To quote the &lt;a href=&quot;http://en.wikipedia.org/wiki/Fractal_tree_index&quot; target=&quot;_blank&quot;&gt;Wikipedia page&lt;/a&gt;:&lt;/span&gt;&lt;br /&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;span&gt;&lt;i&gt;&quot;a Fractal Tree index is a tree data structure that keeps data sorted and allows searches and sequential access in the same time as a B-tree but with insertions and deletions that are asymptotically faster than a B-tree.&quot;&lt;/i&gt;&lt;/span&gt;&lt;/blockquote&gt;&lt;span&gt;Fractal Tree Indexes are really cool, they enable the following capabilities in TokuDB and TokuMX[se]:&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Great compression&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;High performance index maintenance&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;ACID, MVCC&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;&lt;i&gt;Lastly&lt;/i&gt;&lt;/span&gt;&lt;i&gt;&lt;span&gt;, I think it's important to  disclose that I worked at Tokutek for 3.5 years (08/2011 -  01/2015) as VP/Engineering and I do not have any equity in Tokutek or  Percona.&lt;/span&gt;&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;&lt;b&gt;&lt;span&gt;Thoughts on Percona + Fractal Tree Indexes&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;b&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/b&gt; &lt;/span&gt;&lt;br /&gt;&lt;span&gt;Files, Files, Files&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Currently, each Fractal Tree Index is stored in it's own file. The benefit of this approach is that dropping an index instantly returns all space used by the index to the filesystem and the execution of the drop-index operation is very fast. The downside is the number of files on a server can become overwhelming with a large number of tables/collections.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;I think it would be a great feature to allow users the choice of file-per-index, file-per-table, and tablespaces. &lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;There is a &lt;a href=&quot;https://tokutek.atlassian.net/browse/FT-605&quot; target=&quot;_blank&quot;&gt;Jira ticket&lt;/a&gt; for the effort. &lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;Competition&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Things are heating up in the write-optimized storage engine space.&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;MySQL: &lt;a href=&quot;http://deepis.com/insights/press-releases/deep-launches-deep-engine-leverages-machine-learning-usher-mysql-big-data&quot; target=&quot;_blank&quot;&gt;DeepDB launched at Percona Live 2015&lt;/a&gt; and &lt;a href=&quot;https://github.com/MySQLOnRocksDB/mysql-5.6/&quot; target=&quot;_blank&quot;&gt;RocksDB is working on a storage engine&lt;/a&gt;.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;MongoDB: &lt;a href=&quot;http://www.zdnet.com/article/mongodb-cto-how-our-new-wiredtiger-storage-engine-will-earn-its-stripes/&quot; target=&quot;_blank&quot;&gt;WiredTiger contains a yet-to-be-supported LSM implementation&lt;/a&gt; and &lt;a href=&quot;http://blog.parse.com/announcements/mongodb-rocksdb-parse/&quot; target=&quot;_blank&quot;&gt;RocksDB is already in production at Parse&lt;/a&gt;.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span&gt;This raises two concerns:&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Choice is great for the customer, but how much room is there for these technologies to differentiate from each other?&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Facebook is backing RocksDB and MongoDB is backing WiredTiger, both of these companies have vast resources. It's as much about the marketing as it is the technology.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;span&gt;Online Backup&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Tokutek's &quot;hot backup&quot; functionality is closed source and only provided with enterprise editions of TokuDB and TokuMX.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;The other backup solution is file system snapshots. &lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;I'm curious to see if Percona &quot;open sources&quot; hot backup or creates a different open source hot backup technology. &lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;Compression&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;As far as I know, the Fractal Tree Index currently supports quicklz, zlib, and lzma compression libraries.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;There are likely benefits to be had with Snappy, especially on fast storage.&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;https://twitter.com/BohuTANG/status/562434664753664000&quot; target=&quot;_blank&quot;&gt;BohuTANG&lt;/a&gt; from the community seems to have a it working.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span&gt;It might be interesting to experiment with index-prefix-compression, as &lt;a href=&quot;http://www.mongodb.com/blog/post/whats-new-mongodb-30-part-3-performance-efficiency-gains-new-storage-architecture&quot; target=&quot;_blank&quot;&gt;WiredTiger has done&lt;/a&gt;. WiredTiger claims both on-disk and in-memory space savings using this technique.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;Checkpointing&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Simply put, a checkpoint is a process by which a database gets to a known state on disk. Should the database server crash (or lose power) the recovery process requires starting from the checkpoint and playing forward all subsequent committed transactions.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;By default Fractal Tree Indexes checkpoint every 60 seconds.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;A checkpoint affects the server's performance in that it requires CPU, RAM, and IO.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;I assume some effort will go toward reducing the impact of a checkpoint on the running system.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;Code Complexity&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;I don't have specific numbers but I suspect that the Fractal Tree Index code base is more complicated than the other open source write-optimized storage engines.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;I'm happy to be proven wrong about this if anyone wants to present their findings based on lines of code or other accepted code metrics.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;On a side note WiredTiger has some &lt;a href=&quot;http://source.wiredtiger.com/&quot; target=&quot;_blank&quot;&gt;very nice developer documentation&lt;/a&gt;, I'm not sure about the RocksDB documentation.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;License&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;From the &lt;a href=&quot;http://www.tokutek.com/resources/technology/&quot; target=&quot;_blank&quot;&gt;Tokutek website&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;span&gt;&lt;i&gt;Tokutek’s patented Fractal Tree indexing technology is a result of ten  years of research and development by experts in cache-oblivious  algorithmics and is protected by multiple patents.&lt;/i&gt;&lt;/span&gt;&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;The Fractal Tree Index is licensed as GNU GPL v2 &lt;u&gt;plus&lt;/u&gt; a &quot;Patent Rights Grant&quot;.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Does the modified GPLv2 license affects potential users or developers?&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;&lt;span&gt;Anti-use-cases&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;span&gt;I'd like to highlight a three &quot;soft spots&quot; in Fractal Tree Indexing, as in areas where there is room for improvement or simply things to be aware of.&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;span&gt;Leftmost deletion patterns&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;span&gt;Fractal Tree Indexes contain large message buffers in the upper levels. These buffers provide much allow for IO &quot;avoidance&quot; of certain operations.&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;span&gt;Consider a workload where 100 million rows are inserted using an auto-incrementing primary key, then 50 million rows are deleted. &lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;span&gt;At this point the Fractal Tree Index will likely have &quot;buffered&quot; the deletes (deletes are just small messages), the inserted data is still in the leaf nodes.&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;span&gt;Queries against this deleted data will perform poorly, the extreme case is to restart your server and &quot;select min(pk) from foo;&quot;&amp;nbsp; &lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;span&gt;Partitioning is one way to deal with this pattern, rather than deleting the rows you'd merely drop them one partition at a time. &lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Random primary key inserts&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Fractal Tree Indexes have a huge advantage, from an IO perspective, when maintaining non-unique indexes. Insert, update, and delete operations can be buffered.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;However, unique indexes must be checked for uniqueness, and thus an IO is required unless the node holding the key is in memory.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Primary key indexes are always unique.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;So Fractal Tree Indexes perform much like a standard B-tree when randomly inserting into a primary key index. When the data set is larger than RAM, each insert will require an IO to check for uniqueness.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span&gt;&lt;span&gt;Latency&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;span&gt;Fractal Tree Indexes employ two techniques to achieve high compression&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;span&gt;Large block size - The default is 64KB, and can be set higher.&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;span&gt;Algorithms - LZMA &amp;gt; zlib &amp;gt; quicklz (and Snappy is likely coming soon) &lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span&gt;&lt;span&gt;Compression comes at a cost, latency.&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;span&gt;The larger the node size, the longer the decompress operation. &lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;span&gt;The higher the compression, the longer the decompress operation.&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;span&gt;Users are purchasing SSD/Flash for their IO performance, but they also want high compression because these devices are expensive.&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;span&gt;At the moment it's complicated to determine the best combination of node size and compression algorithm, creating user-facing metrics will be helpful.&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;span&gt;Human Resources&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;As with TokuDB and TokuMX[se], I'm curious to see how much Percona is&amp;nbsp; looking to grow the team.  They've already posted on their &lt;a href=&quot;http://www.percona.com/about-us/careers/open-positions&quot;&gt;jobs page&lt;/a&gt; for &quot;&lt;/span&gt;&lt;span&gt;C/C++ Developers for TokuDB, TokuMX, and Tokutek Products&quot;.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Prioritizing resources between Fractal Tree Indexes, TokuDB, TokuMX, and TokuMXse will be tricky.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;Please asks questions or comment below.&lt;/span&gt;</description>
	<pubDate>Thu, 21 May 2015 20:52:41 +0000</pubDate>
	<author>noreply@blogger.com (Tim Callaghan)</author>
</item>
<item>
	<title>A. Jesse Jiryu Davis: Server Discovery And Monitoring In PyMongo, Perl, And C</title>
	<guid>http://emptysqua.re/blog/server-discovery-and-monitoring-in-pymongo-perl-and-c/</guid>
	<link>http://emptysqua.re/blog/server-discovery-and-monitoring-in-pymongo-perl-and-c/</link>
	<description>&lt;p&gt;&lt;em&gt;(Cross-posted from the MongoDB Blog.)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;How does a MongoDB driver discover and monitor a single server, a set of mongos servers, or a replica set? How does it determine what types of servers they are? How does it keep this information up to date? How does it discover an entire replica set given an initial host list, and how does it respond to stepdowns, elections, reconfigurations, network error, or the loss of a server?&lt;/p&gt;
&lt;p&gt;In the past each MongoDB driver answered these questions a little differently, and mongos differed a little from the drivers. We couldn't answer questions like, &quot;Once I add a secondary to my replica set, how long does it take for the driver to start using it?&quot; Or, &quot;How does a driver detect when the primary steps down, and how does it react?&quot;&lt;/p&gt;
&lt;p&gt;To standardize our drivers, I wrote the Server Discovery And Monitoring Spec, with David Golden, Craig Wilson, Jeff Yemin, and Bernie Hackett. Beginning with this spring's next-generation driver releases, all our drivers conform to the spec and answer these questions the same. Or, where there's a legitimate reason for them to differ, there are as few differences as possible and each is clearly explained in the spec. Even in cases where several answers seem equally good, drivers agree on one way to do it.&lt;/p&gt;
&lt;p&gt;The spec describes how a driver monitors a topology:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Topology:&lt;/em&gt; The state of your deployment. What type of deployment it is, which servers are available, and what type of servers (mongos, primary, secondary, ...) they are.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The spec covers all MongoDB topologies, but replica sets are the most interesting. So I'll explain the spec's algorithm for replica sets by telling the story of your application as it passes through life stages: it starts up, discovers a replica set, and reaches a steady state. Then there is a crisis&amp;mdash;I spill coffee on your primary server's motherboard&amp;mdash;and a resolution&amp;mdash;the replica set elects a new primary and the driver discovers it.&lt;/p&gt;
&lt;p&gt;At each stage we'll observe a typical multi-threaded driver, PyMongo 3.0, a typical single-threaded driver, the Perl Driver 1.0, and a hybrid, the C Driver 1.2. (I implemented PyMongo's server discovery and monitoring. David Golden wrote the Perl version, and Samantha Ritter and Jason Carey wrote the one in C.)&lt;/p&gt;
&lt;p&gt;To conclude, I'll tell you our strategy for verifying spec compliance in ten programming languages, and I'll share links for further reading.&lt;/p&gt;
&lt;h1 id=&quot;startup&quot;&gt;Startup&lt;/h1&gt;
&lt;p&gt;When your application initializes, it creates a MongoClient. In Python:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;client &lt;span&gt;=&lt;/span&gt; MongoClient(
    &lt;span&gt;'mongodb://hostA,hostB/?replicaSet=my_rs'&lt;/span&gt;)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In Perl:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;&lt;span&gt;my&lt;/span&gt; &lt;span&gt;$client&lt;/span&gt; &lt;span&gt;=&lt;/span&gt; &lt;span&gt;MongoDB::&lt;/span&gt;MongoClient&lt;span&gt;-&amp;gt;&lt;/span&gt;&lt;span&gt;new&lt;/span&gt;({
    host &lt;span&gt;=&amp;gt;&lt;/span&gt; &lt;span&gt;&amp;quot;mongodb://hostA,hostB/?replicaSet=my_rs&amp;quot;&lt;/span&gt;
});
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In C, you can either create a client directly:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;mongoc_client_t &lt;span&gt;*&lt;/span&gt;client &lt;span&gt;=&lt;/span&gt; mongoc_client_new (
    &lt;span&gt;&amp;quot;mongodb://hostA,hostB/?replicaSet=my_rs&amp;quot;&lt;/span&gt;);
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Or create a client pool:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;mongoc_client_pool_t &lt;span&gt;*&lt;/span&gt;pool &lt;span&gt;=&lt;/span&gt; mongoc_client_pool_new (
    &lt;span&gt;&amp;quot;mongodb://hostA,hostB/?replicaSet=my_rs&amp;quot;&lt;/span&gt;);

mongoc_client_t &lt;span&gt;*&lt;/span&gt;client &lt;span&gt;=&lt;/span&gt; mongoc_client_pool_pop (pool);
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A crucial improvement of the next gen drivers is, the constructor no longer blocks while it makes the initial connection. Instead, the constructor does no network I/O. PyMongo launches a background thread per server (two threads in this example) to initiate discovery, and returns control to your application without blocking. Perl does nothing until you attempt an operation; then it connects on demand.&lt;/p&gt;
&lt;p&gt;In the C Driver, if you create a client directly it behaves like the Perl Driver: it connects on demand, on the main thread. But the C Driver's client pool launches one background thread to discover and monitor all servers.&lt;/p&gt;
&lt;p&gt;The spec's &quot;no I/O in constructors&quot; rule is a big win for web applications that use our next gen drivers: In a crisis, your app servers might be restarted while your MongoDB servers are unreachable. Your application should not throw an error at startup, when it constructs the client object. It starts up disconnected and tries to reach your servers until it succeeds.&lt;/p&gt;
&lt;h1 id=&quot;discovery&quot;&gt;Discovery&lt;/h1&gt;
&lt;p&gt;The initial host list you provide is called the &quot;seed list&quot;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Seed list:&lt;/em&gt; The initial list of server addresses provided to the MongoClient.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The seed list is the stepping-off point for the driver's journey of discovery. As long as one seed is actually an available replica set member, the driver will discover the whole set and stay connected to it indefinitely, as described below. Even if every member of the set is replaced with a new host, like the &lt;a href=&quot;http://en.wikipedia.org/wiki/Ship_of_Theseus&quot;&gt;Ship of Theseus&lt;/a&gt;, it is still the same replica set and the driver remains connected to it.&lt;/p&gt;
&lt;p&gt;I tend to think of a driver as a tiny economy of information about your topology. Monitoring supplies information, and your application's operations demand information. Their demands are defined in David Golden's &lt;a href=&quot;http://www.mongodb.com/blog/post/server-selection-next-generation-mongodb-drivers&quot;&gt;Server Selection Spec&lt;/a&gt;, while the method of supplying information is defined here, in the Server Discovery And Monitoring Spec. In the beginning, there is no information, and the monitors rush to supply some. I'll talk more about the demand side later, in the &quot;Crisis&quot; section.&lt;/p&gt;
&lt;h1 id=&quot;multi-threaded&quot;&gt;Multi-threaded&lt;/h1&gt;
&lt;p&gt;Let's start with PyMongo. In PyMongo, like other multi-threaded drivers, the MongoClient constructor starts one monitor thread each for &quot;hostA&quot; and &quot;hostB&quot;.&lt;/p&gt;
&lt;p&gt;Monitor: A thread or async task that occasionally checks the state of one server.&lt;/p&gt;
&lt;p&gt;Each monitor connects to its assigned server and executes the &lt;a href=&quot;http://docs.mongodb.org/manual/reference/command/isMaster&quot;&gt;&quot;ismaster&quot; command&lt;/a&gt;. Ignore the command's archaic name, which dates from the days of master-slave replication, long superseded by replica sets. The ismaster command is the client-server handshake. Let's say the driver receives hostB's response first:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;ismaster &lt;span&gt;=&lt;/span&gt; {
    &lt;span&gt;&amp;quot;setName&amp;quot;&lt;/span&gt;&lt;span&gt;:&lt;/span&gt; &lt;span&gt;&amp;quot;my_rs&amp;quot;&lt;/span&gt;,
    &lt;span&gt;&amp;quot;ismaster&amp;quot;&lt;/span&gt;&lt;span&gt;:&lt;/span&gt; &lt;span&gt;false&lt;/span&gt;,
    &lt;span&gt;&amp;quot;secondary&amp;quot;&lt;/span&gt;&lt;span&gt;:&lt;/span&gt; &lt;span&gt;true&lt;/span&gt;,
    &lt;span&gt;&amp;quot;hosts&amp;quot;&lt;/span&gt;&lt;span&gt;:&lt;/span&gt; [
        &lt;span&gt;&amp;quot;hostA:27017&amp;quot;&lt;/span&gt;,
        &lt;span&gt;&amp;quot;hostB:27017&amp;quot;&lt;/span&gt;,
        &lt;span&gt;&amp;quot;hostC:27017&amp;quot;&lt;/span&gt;]}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;hostB confirms it belongs to your replica set, informs you that it is a secondary, and lists the members in the replica set config. PyMongo sees a host it didn't know about, hostC, so it launches a new thread to connect to it.&lt;/p&gt;
&lt;p&gt;If your application threads are waiting to do any operations with the MongoClient, they block while awaiting discovery. But since PyMongo now knows of a secondary, if your application is waiting to do a secondary read, it can now proceed:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;db &lt;span&gt;=&lt;/span&gt; client&lt;span&gt;.&lt;/span&gt;get_database(
    &lt;span&gt;&amp;quot;dbname&amp;quot;&lt;/span&gt;,
    read_preference&lt;span&gt;=&lt;/span&gt;ReadPreference&lt;span&gt;.&lt;/span&gt;SECONDARY)

&lt;span&gt;# Unblocks when a secondary is found.&lt;/span&gt;
db&lt;span&gt;.&lt;/span&gt;collection&lt;span&gt;.&lt;/span&gt;find_one()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Meanwhile, discovery continues. PyMongo waits for ismaster responses from hostA and hostC. Let's say hostC responds next, and its response includes &quot;ismaster&quot;: true:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;ismaster &lt;span&gt;=&lt;/span&gt; {
    &lt;span&gt;&amp;quot;setName&amp;quot;&lt;/span&gt;&lt;span&gt;:&lt;/span&gt; &lt;span&gt;&amp;quot;my_rs&amp;quot;&lt;/span&gt;,
    &lt;span&gt;&amp;quot;ismaster&amp;quot;&lt;/span&gt;&lt;span&gt;:&lt;/span&gt; &lt;span&gt;true&lt;/span&gt;,
    &lt;span&gt;&amp;quot;secondary&amp;quot;&lt;/span&gt;&lt;span&gt;:&lt;/span&gt; &lt;span&gt;false&lt;/span&gt;,
    &lt;span&gt;&amp;quot;hosts&amp;quot;&lt;/span&gt;&lt;span&gt;:&lt;/span&gt; [
        &lt;span&gt;&amp;quot;hostA:27017&amp;quot;&lt;/span&gt;,
        &lt;span&gt;&amp;quot;hostB:27017&amp;quot;&lt;/span&gt;,
        &lt;span&gt;&amp;quot;hostC:27017&amp;quot;&lt;/span&gt;]}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now PyMongo knows the primary, so all reads and writes are unblocked. PyMongo is still waiting to hear back from hostA; once it does, it can use hostA for secondary reads as well.&lt;/p&gt;
&lt;h1 id=&quot;single-threaded&quot;&gt;Single-threaded&lt;/h1&gt;
&lt;p&gt;Multithreaded Perl code is problematic, so the Perl Driver doesn't launch a thread per host. How, then does it discover your set? When you construct a MongoClient it does no I/O. It waits for you to begin an operation before it connects. Once you do, it scans the hosts serially, initially in random order.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Scan:&lt;/em&gt; A single-threaded driver's process of checking the state of all servers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let's say the driver begins with hostB, a secondary. Here's a detail I didn't show you earlier: replica set members tell you who they think the primary is. HostB's reply includes &quot;primary&quot;: &quot;hostC:27017&quot;:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;ismaster &lt;span&gt;=&lt;/span&gt; {
    &lt;span&gt;&amp;quot;setName&amp;quot;&lt;/span&gt;&lt;span&gt;:&lt;/span&gt; &lt;span&gt;&amp;quot;my_rs&amp;quot;&lt;/span&gt;,
    &lt;span&gt;&amp;quot;ismaster&amp;quot;&lt;/span&gt;&lt;span&gt;:&lt;/span&gt; &lt;span&gt;false&lt;/span&gt;,
    &lt;span&gt;&amp;quot;secondary&amp;quot;&lt;/span&gt;&lt;span&gt;:&lt;/span&gt; &lt;span&gt;true&lt;/span&gt;,
    &lt;span&gt;&amp;quot;primary&amp;quot;&lt;/span&gt;&lt;span&gt;:&lt;/span&gt; &lt;span&gt;&amp;quot;hostC:27017&amp;quot;&lt;/span&gt;,
    &lt;span&gt;&amp;quot;hosts&amp;quot;&lt;/span&gt;&lt;span&gt;:&lt;/span&gt; [
        &lt;span&gt;&amp;quot;hostA:27017&amp;quot;&lt;/span&gt;,
        &lt;span&gt;&amp;quot;hostB:27017&amp;quot;&lt;/span&gt;,
        &lt;span&gt;&amp;quot;hostC:27017&amp;quot;&lt;/span&gt;]}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The Perl Driver uses this hint to put hostC next in the scan order, because connecting to the primary is its top priority. It checks hostC and confirms that it's primary. Finally, it checks hostA to ensure it can connect, and discovers that hostA is another secondary. Scanning is now complete and the driver proceeds with your application's operation.&lt;/p&gt;
&lt;h1 id=&quot;hybrid&quot;&gt;Hybrid&lt;/h1&gt;
&lt;p&gt;The C driver has two &lt;em&gt;modes&lt;/em&gt; for server discovery and monitoring: single-threaded and pooled. Single-threaded mode is optimized for embedding the C Driver within languages like PHP: PHP applications deploy many single-threaded processes connected to MongoDB. Each process uses the same connections to scan the topology as it uses for application operations, so the total connection count from many processes is kept to a minimum.&lt;/p&gt;
&lt;p&gt;Other applications should use pooled mode: as we shall see, in pooled mode a background thread monitors the topology, so the application need not block to scan it.&lt;/p&gt;
&lt;h2 id=&quot;c-drivers-single-threaded-mode&quot;&gt;C Driver's single-threaded mode&lt;/h2&gt;
&lt;p&gt;The C driver scans servers on the main thread, if you construct a single client:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;mongoc_client_t &lt;span&gt;*&lt;/span&gt;client &lt;span&gt;=&lt;/span&gt; mongoc_client_new (
      &lt;span&gt;&amp;quot;mongodb://hostA,hostB/?replicaSet=my_rs&amp;quot;&lt;/span&gt;);
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In single-threaded mode, the C Driver blocks to scan your topology periodically with the main thread, just like the Perl Driver. But unlike the Perl Driver's serial scan, the C Driver checks all servers in parallel. Using a non-blocking socket per member, it begins a check on each member concurrently, and uses the asynchronous &quot;poll&quot; function to receive events from the sockets, until all have responded or timed out. The driver updates its topology as ismaster calls complete. Finally it ends the scan and returns control to your application.&lt;/p&gt;
&lt;p&gt;Whereas the Perl Driver's topology scan lasts for the sum of all server checks (including timeouts), the C Driver's topology scan lasts only the maximum of any one check's duration, or the connection timeout setting, whichever is shorter. Put another way, in single-threaded mode the C Driver fans out to begin all checks concurrently, then fans in once all checks have completed or timed out. This &quot;fan out, fan in&quot; topology scanning method gives the C Driver an advantage scanning very large replica sets, or sets with several high-latency members.&lt;/p&gt;
&lt;h2 id=&quot;c-drivers-pooled-mode&quot;&gt;C Driver's pooled mode&lt;/h2&gt;
&lt;p&gt;To activate the C Driver's pooled mode, make a client pool:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;mongoc_client_pool_t &lt;span&gt;*&lt;/span&gt;pool &lt;span&gt;=&lt;/span&gt; mongoc_client_pool_new (
    &lt;span&gt;&amp;quot;mongodb://hostA,hostB/?replicaSet=my_rs&amp;quot;&lt;/span&gt;);

mongoc_client_t &lt;span&gt;*&lt;/span&gt;client &lt;span&gt;=&lt;/span&gt; mongoc_client_pool_pop (pool);
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The pool launches one background thread for monitoring. When the thread begins, it fans out and connects to all servers in the seed list, using non-blocking sockets and a simple event loop. As it receives ismaster responses from the servers, it updates its view of your topology, the same as a multi-threaded driver like PyMongo does. When it discovers a new server it begins connecting to it, and adds the new socket to the list of non-blocking sockets in its event loop.&lt;/p&gt;
&lt;p&gt;As with PyMongo, when the C Driver is in background-thread mode, your application's operations are unblocked as soon as monitoring discovers a usable server. For example, if your C code is blocked waiting to insert into the primary, it is unblocked as soon as the primary is discovered, rather than waiting for all secondaries to be checked too.&lt;/p&gt;
&lt;h1 id=&quot;steady-state&quot;&gt;Steady State&lt;/h1&gt;
&lt;p&gt;Once the driver has discovered your whole replica set, it periodically re-checks each server. The periodic check is necessary to keep track of your network latency to each server, and to detect when a new secondary joins the set. And in some cases periodic monitoring can head off errors, by proactively discovering when a server is offline.&lt;/p&gt;
&lt;p&gt;By default, the monitor threads in PyMongo check their servers every ten seconds, as does the C Driver's monitor in background-thread mode. The Perl driver, and the C Driver in single-threaded mode, block your application to re-scan the replica set once per minute.&lt;/p&gt;
&lt;p&gt;If you like my supply-and-demand model of a driver, the steady state is when your application's demand for topology information is satisfied. The driver occasionally refreshes its stock of information to make sure it's ready for future demands, but there is no urgency.&lt;/p&gt;
&lt;h1 id=&quot;crisis&quot;&gt;Crisis&lt;/h1&gt;
&lt;p&gt;So I wander into your data center, swirling my cappuccino, and I stumble and spill it on hostC's motherboard. Now your replica set has no primary. What happens next?&lt;/p&gt;
&lt;p&gt;When your application next writes to the primary, it gets a socket timeout. Now it knows the primary is gone. Its demand for information is no longer in balance with supply. The next attempt to write blocks until a primary is found.&lt;/p&gt;
&lt;p&gt;To meet demand, the driver works overtime. How exactly it responds to the crisis depends on which type of monitoring it uses.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Multi-threaded:&lt;/strong&gt; In drivers like PyMongo, the monitor threads wait only half a second between server checks, instead of ten seconds. They want to know as soon as possible if the primary has come back, or if one of the secondaries has been elected primary.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Single-threaded:&lt;/strong&gt; Drivers like the Perl Driver sleep half a second between scans of the topology. The application's write operation remains blocked until the driver finds the primary.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;C Driver Single-Threaded:&lt;/strong&gt; In single-threaded mode, the C Driver sleeps half a second between scans, just like the Perl Driver. During the scan the driver launches non-blocking &quot;ismaster&quot; commands on all servers concurrently, as I described above.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;C Driver Pooled Mode:&lt;/strong&gt; Each time the driver's monitor thread receives an ismaster response, schedules that server's next ismaster call on the event loop only a half-second in the future.&lt;/p&gt;
&lt;h1 id=&quot;resolution&quot;&gt;Resolution&lt;/h1&gt;
&lt;p&gt;Your secondaries, hostA and hostB, promptly detect my sabotage of hostC, and hold an election. In MongoDB 3.0, the election takes just a couple seconds. Let's say hostA becomes primary.&lt;/p&gt;
&lt;p&gt;A half second or less later, your driver rechecks hostA and sees that it is now the primary. It unblocks your application's writes and sends them to hostA. In PyMongo, the monitor threads relax, and return to their slow polling strategy: they sleep ten seconds between server checks. Same for the C Driver's monitor in background-thread mode. The Perl Driver, and the C Driver in single-threaded mode, do not rescan the topology for another minute. Demand and supply are once again in balance.&lt;/p&gt;
&lt;h1 id=&quot;compliance-testing&quot;&gt;Compliance Testing&lt;/h1&gt;
&lt;p&gt;I am particularly excited about the unit tests that accompany the Server Discovery And Monitoring Spec. We have 38 tests that are specified formally in YAML files, with inputs and expected outcomes for a range of scenarios. For each driver we write a test runner that feeds the inputs to the driver and verifies the outcome. This ends confusion about what the spec means, or whether all drivers conform to it. You can track our progress toward full compliance in MongoDB's issue tracker.&lt;/p&gt;
&lt;h1 id=&quot;further-study&quot;&gt;Further Study&lt;/h1&gt;
&lt;p&gt;The spec is long but tractable. It explains the monitoring algorithm in very fine detail. You can read a summary, and the spec itself, here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://emptysqua.re/server-discovery-and-monitoring-summary.html&quot;&gt;A summary of the spec.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://emptysqua.re/server-discovery-and-monitoring.html&quot;&gt;The Server Discovery And Monitoring Spec.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/mongodb/specifications/tree/master/source/server-discovery-and-monitoring&quot;&gt;The spec source, including the YAML test files.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Its job is to describe the demand side of the driver's information economy. For the supply side, read my colleague David Golden's &lt;a href=&quot;http://www.mongodb.com/blog/post/server-selection-next-generation-mongodb-drivers&quot;&gt;article on his Server Selection Spec&lt;/a&gt;.&lt;/p&gt;&lt;img src=&quot;http://emptysqua.re/blog/analytics/http%3A//emptysqua.re/blog/server-discovery-and-monitoring-in-pymongo-perl-and-c//pixel.gif&quot; width=&quot;1px&quot; height=&quot;1px&quot; /&gt;</description>
	<pubDate>Wed, 20 May 2015 03:09:35 +0000</pubDate>
	<author>ajdavis@cs.oberlin.edu (A. Jesse Jiryu Davis)</author>
</item>
<item>
	<title>A. Jesse Jiryu Davis: More Photos My Shuso Hossen</title>
	<guid>http://emptysqua.re/blog/more-photos-my-shuso-hossen/</guid>
	<link>http://emptysqua.re/blog/more-photos-my-shuso-hossen/</link>
	<description>&lt;p&gt;A larger batch of images by photographer &lt;a href=&quot;https://twitter.com/stillman_brown&quot;&gt;Stillman Brown&lt;/a&gt;, taken at my shuso hossen ceremony last month. My teacher Enkyo Roshi allowed me to give &lt;a href=&quot;http://emptysqua.re/blog/yangshan-plants-his-hoe/&quot;&gt;my first Zen talk&lt;/a&gt; and I became a senior student at the Village Zendo.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/shuso-hossen.jpg&quot; alt=&quot;Shuso hossen&quot; title=&quot;Shuso hossen&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/shuso-hossen-2.jpg&quot; alt=&quot;Shuso hossen 2&quot; title=&quot;Shuso hossen 2&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/shuso-hossen-3.jpg&quot; alt=&quot;Shuso hossen 3&quot; title=&quot;Shuso hossen 3&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/shuso-hossen-4.jpg&quot; alt=&quot;Shuso hossen 4&quot; title=&quot;Shuso hossen 4&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/shuso-hossen-5.jpg&quot; alt=&quot;Shuso hossen 5&quot; title=&quot;Shuso hossen 5&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/shuso-hossen-vertical.jpg&quot; alt=&quot;Shuso hossen vertical&quot; title=&quot;Shuso hossen vertical&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/shuso-hossen-6.jpg&quot; alt=&quot;Shuso hossen 6&quot; title=&quot;Shuso hossen 6&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/shuso-hossen-7.jpg&quot; alt=&quot;Shuso hossen 7&quot; title=&quot;Shuso hossen 7&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/shuso-hossen-8.jpg&quot; alt=&quot;Shuso hossen 8&quot; title=&quot;Shuso hossen 8&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/shuso-hossen-9.jpg&quot; alt=&quot;Shuso hossen 9&quot; title=&quot;Shuso hossen 9&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/shuso-hossen-10.jpg&quot; alt=&quot;Shuso hossen 10&quot; title=&quot;Shuso hossen 10&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/shuso-hossen-11.jpg&quot; alt=&quot;Shuso hossen 11&quot; title=&quot;Shuso hossen 11&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/shuso-hossen-12.jpg&quot; alt=&quot;Shuso hossen 12&quot; title=&quot;Shuso hossen 12&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/shuso-hossen-13.jpg&quot; alt=&quot;Shuso hossen 13&quot; title=&quot;Shuso hossen 13&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/shuso-hossen-14.jpg&quot; alt=&quot;Shuso hossen 14&quot; title=&quot;Shuso hossen 14&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/shuso-hossen-15.jpg&quot; alt=&quot;Shuso hossen 15&quot; title=&quot;Shuso hossen 15&quot; /&gt;&lt;/p&gt;&lt;img src=&quot;http://emptysqua.re/blog/analytics/http%3A//emptysqua.re/blog/more-photos-my-shuso-hossen//pixel.gif&quot; width=&quot;1px&quot; height=&quot;1px&quot; /&gt;</description>
	<pubDate>Tue, 19 May 2015 01:24:33 +0000</pubDate>
	<author>ajdavis@cs.oberlin.edu (A. Jesse Jiryu Davis)</author>
</item>
<item>
	<title>A. Jesse Jiryu Davis: Announcing libmongoc 1.1.6</title>
	<guid>http://emptysqua.re/blog/announcing-libmongoc-1-1-6/</guid>
	<link>http://emptysqua.re/blog/announcing-libmongoc-1-1-6/</link>
	<description>&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/Hans_Egede_sea_serpent_1734.jpg&quot; alt=&quot;Hans Egede sea serpent 1734&quot; title=&quot;Hans Egede sea serpent 1734&quot; /&gt;&lt;/p&gt;
&lt;p&gt;I released libmongoc 1.1.6 today with some bugfixes and a major performance enhancement.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://api.mongodb.org/c/current/mongoc_bulk_operation_execute.html&quot;&gt;&lt;code&gt;mongoc_bulk_operation_execute&lt;/code&gt;&lt;/a&gt; now coalesces consecutive update operations
  into a single message to a MongoDB 2.6+ server, yielding huge performance
  gains. Same for remove operations. (Inserts were always coalesced.)&lt;/li&gt;
&lt;li&gt;Large numbers of insert operations are now properly batched according to
  number of documents and total data size.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://api.mongodb.org/c/current/authentication.html#kerberos&quot;&gt;GSSAPI / Kerberos auth&lt;/a&gt; now works.&lt;/li&gt;
&lt;li&gt;The driver no longer tries three times in vain to reconnect to a primary,
  so &lt;code&gt;socketTimeoutMS&lt;/code&gt; and &lt;code&gt;connectTimeoutMS&lt;/code&gt; now behave &lt;em&gt;closer&lt;/em&gt; to what you
  expect for replica sets with down members. A full fix awaits 1.2.0.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I snuck in a feature:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://api.mongodb.org/c/current/mongoc_matcher_t.html&quot;&gt;&lt;code&gt;mongoc_matcher_t&lt;/code&gt;&lt;/a&gt; does basic subdocument and array matching&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I also released libbson 1.1.6 to maintain version parity; it's identical to libbson 1.1.5.&lt;/p&gt;
&lt;p&gt;Release tarballs are available for download:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/mongodb/libbson/releases/download/1.1.6/libbson-1.1.6.tar.gz&quot;&gt;libbson-1.1.6.tar.gz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/mongodb/mongo-c-driver/releases/download/1.1.6/mongo-c-driver-1.1.6.tar.gz&quot;&gt;mongo-c-driver-1.1.6.tar.gz&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You'll notice this is just a week after &lt;a href=&quot;http://emptysqua.re/blog/announcing-libbson-and-libmongoc-1-1-5/&quot;&gt;the 1.1.5 release&lt;/a&gt;, since there were a users waiting on these particular fixes that I couldn't get in to last week's release.&lt;/p&gt;
&lt;p&gt;It's my intention to do only the most critical work for the 1.1.x line of the driver libraries, and concentrate on shipping 1.2.0 as soon as possible: a reasonably tested beta in the middle of June and a stable version at the beginning of August. (Circumstances are likely to intervene, of course.) Shipping version 1.2.0 will offer you a C driver that conforms with the modern MongoDB specs: &lt;a href=&quot;http://www.mongodb.com/blog/post/server-discovery-and-monitoring-next-generation-mongodb-drivers&quot;&gt;Server Discovery And Monitoring&lt;/a&gt;, and &lt;a href=&quot;http://www.mongodb.com/blog/post/server-selection-next-generation-mongodb-drivers&quot;&gt;Server Selection&lt;/a&gt;. It will resolve a heap of replica set issues in the current driver.&lt;/p&gt;
&lt;p&gt;For further information:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://api.mongodb.org/libbson/current/&quot;&gt;libbson documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://api.mongodb.org/c/current/&quot;&gt;libmongoc documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://jira.mongodb.org/secure/ReleaseNote.jspa?projectId=10030&amp;version=15434&quot;&gt;Full release notes for libmongoc 1.1.6 in Jira&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thanks to those who contributed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A. Jesse Jiryu Davis&lt;/li&gt;
&lt;li&gt;Jason Carey&lt;/li&gt;
&lt;li&gt;Kai Mast&lt;/li&gt;
&lt;li&gt;Matt Cotter&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/File:Hans_Egede_1734_sea_serpent.jpg&quot;&gt;Image: The &quot;Great Sea Serpent&quot; according to Hans Egede&lt;/a&gt;&lt;/p&gt;&lt;img src=&quot;http://emptysqua.re/blog/analytics/http%3A//emptysqua.re/blog/announcing-libmongoc-1-1-6//pixel.gif&quot; width=&quot;1px&quot; height=&quot;1px&quot; /&gt;</description>
	<pubDate>Mon, 18 May 2015 22:50:31 +0000</pubDate>
	<author>ajdavis@cs.oberlin.edu (A. Jesse Jiryu Davis)</author>
</item>
<item>
	<title>MongoDB Management Service: MMS Release Notes 2015-05-13</title>
	<guid>http://blog.mms.mongodb.com/post/118865820005</guid>
	<link>http://blog.mms.mongodb.com/post/118865820005</link>
	<description>&lt;p&gt;Welcome to another MMS release! We are excited to announce the following new features:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Account Self-Closure&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;While we hope that MMS will be a positive experience for all users, we recognize that sometimes it&amp;rsquo;s necessary to close an MMS group. To simplify this process, we&amp;rsquo;ve made it easier for users to close their own groups. Just go to your &lt;em&gt;Administration&lt;/em&gt; tab, select &lt;em&gt;Billing/Subscriptions&lt;/em&gt;. At the bottom of the page, you will find the close your group link:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://40.media.tumblr.com/389ff38de9cee9202452db45652f427f/tumblr_noam4fHtCa1sdaytmo2_1280.png&quot;&gt;&lt;img src=&quot;https://40.media.tumblr.com/389ff38de9cee9202452db45652f427f/tumblr_noam4fHtCa1sdaytmo2_1280.png&quot; alt=&quot;Close Account&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You will have to re-authenticate and confirm. This will remove all users from the group, delete all hosts from the group, and disable all alerts in the group. It is not possible to close a group with running backups or with an unpaid invoice. Group names cannot be reclaimed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Webhooks for Alert Notifications&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;MMS has supported notifications by email, SMS, Hipchat, and PagerDuty for some time now. We are happy to add &lt;a href=&quot;http://en.wikipedia.org/wiki/Webhook&quot;&gt;Webhooks&lt;/a&gt; to the mix. Simply add your webhook endpoint to your &lt;em&gt;Administration&amp;hellip; Group Settings&lt;/em&gt; tab, along with an optional secret for validation:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://40.media.tumblr.com/1efc878a4f406b27d62a49b45c5fd85f/tumblr_noam4fHtCa1sdaytmo3_1280.png&quot;&gt;&lt;img src=&quot;https://40.media.tumblr.com/1efc878a4f406b27d62a49b45c5fd85f/tumblr_noam4fHtCa1sdaytmo3_1280.png&quot; alt=&quot;Webhooks configuration&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Once added, you can choose your webhook as a destination for any alerts:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://41.media.tumblr.com/0335dec4ad0f5717dc481ee1fbce5a20/tumblr_noam4fHtCa1sdaytmo1_1280.png&quot;&gt;&lt;img src=&quot;https://41.media.tumblr.com/0335dec4ad0f5717dc481ee1fbce5a20/tumblr_noam4fHtCa1sdaytmo1_1280.png&quot; alt=&quot;Webhook in use&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thanks for reading!&lt;/p&gt;</description>
	<pubDate>Wed, 13 May 2015 14:44:16 +0000</pubDate>
</item>
<item>
	<title>A. Jesse Jiryu Davis: Announcing libbson and libmongoc 1.1.5</title>
	<guid>http://emptysqua.re/blog/announcing-libbson-and-libmongoc-1-1-5/</guid>
	<link>http://emptysqua.re/blog/announcing-libbson-and-libmongoc-1-1-5/</link>
	<description>&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/SeaSmoke.jpg&quot; alt=&quot;Sea Smoke&quot; title=&quot;Sea Smoke&quot; /&gt;&lt;/p&gt;
&lt;p&gt;I've released versions 1.1.5 today of libbson and libmongoc.&lt;/p&gt;
&lt;p&gt;libbson is a C library for creating, parsing, and manipulating BSON documents. libmongoc is the C Driver for MongoDB, a library for building high-performance applications that communicate with MongoDB in the C language. It also serves as the base for drivers in some higher-level languages.&lt;/p&gt;
&lt;p&gt;Release tarballs are available for download:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/mongodb/libbson/releases/download/1.1.5/libbson-1.1.5.tar.gz&quot;&gt;libbson-1.1.5.tar.gz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/mongodb/mongo-c-driver/releases/download/1.1.5/mongo-c-driver-1.1.5.tar.gz&quot;&gt;mongo-c-driver-1.1.5.tar.gz&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is a patch release with small bug fixes. In libbson:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fix link error &quot;missing __sync_add_and_fetch_4&quot; in GCC on i386 - the functions &lt;code&gt;bson_atomic_int_add&lt;/code&gt; and &lt;code&gt;bson_atomic_int64_add&lt;/code&gt; are now compiled and exported if needed in i386 mode&lt;/li&gt;
&lt;li&gt;Fix version check for GCC 5 and future versions of Clang&lt;/li&gt;
&lt;li&gt;Fix warnings and errors building on various platforms&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In libmongoc:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;fsync&lt;/code&gt; and &lt;code&gt;j&lt;/code&gt; write concern flags now imply acknowledged writes&lt;/li&gt;
&lt;li&gt;Prevent using &lt;code&gt;fsync&lt;/code&gt; or &lt;code&gt;j&lt;/code&gt; with conflicting &lt;code&gt;w=0&lt;/code&gt; write concern&lt;/li&gt;
&lt;li&gt;Obey socket timeout consistently in TLS/SSL mode&lt;/li&gt;
&lt;li&gt;Return an error promptly after a network hangup in TLS mode&lt;/li&gt;
&lt;li&gt;Prevent crash using SSL in FIPS mode&lt;/li&gt;
&lt;li&gt;Always return NULL from &lt;code&gt;mongoc_database_get_collection_names&lt;/code&gt; on error&lt;/li&gt;
&lt;li&gt;Fix version check for GCC 5 and future versions of Clang&lt;/li&gt;
&lt;li&gt;Fix warnings and errors building on various platforms&lt;/li&gt;
&lt;li&gt;Add configure flag to enable/disable shared memory performance counters&lt;/li&gt;
&lt;li&gt;Minor docs improvements and fix links from libmongoc to libbson docs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For further information:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://api.mongodb.org/libbson/current/&quot;&gt;libbson documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://api.mongodb.org/c/current/&quot;&gt;libmongoc documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://jira.mongodb.org/secure/ReleaseNote.jspa?projectId=10030&amp;version=15316&quot;&gt;Full release notes for libbson 1.1.5 and libmongoc 1.1.5 in Jira&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With this release, I abandon the convention that odd-numbered patch versions indicate unstable releases. I am switching to simple semantic versioning: 1.1.5 is a stable release with bug fixes since 1.1.4. During subsequent development the libmongoc and libbson versions will be &quot;1.1.6-dev&quot;.&lt;/p&gt;
&lt;p&gt;This is my first release of libbson and libmongoc; I needed a lot of help and I received it. Thanks to those who contributed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Christian Hergert&lt;/li&gt;
&lt;li&gt;Hannes Magnusson&lt;/li&gt;
&lt;li&gt;Jason Carey&lt;/li&gt;
&lt;li&gt;Jeremy Mikola&lt;/li&gt;
&lt;li&gt;Jeroen Ooms&lt;/li&gt;
&lt;li&gt;Paul Melnikow&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;a href=&quot;http://commons.wikimedia.org/wiki/File:SeaSmoke.jpg&quot;&gt;Image: Kristopher Wilson/ US Navy&lt;/a&gt;&lt;/p&gt;&lt;img src=&quot;http://emptysqua.re/blog/analytics/http%3A//emptysqua.re/blog/announcing-libbson-and-libmongoc-1-1-5//pixel.gif&quot; width=&quot;1px&quot; height=&quot;1px&quot; /&gt;</description>
	<pubDate>Tue, 12 May 2015 23:07:59 +0000</pubDate>
	<author>ajdavis@cs.oberlin.edu (A. Jesse Jiryu Davis)</author>
</item>
<item>
	<title>A. Jesse Jiryu Davis: Announcing PyMongo 3.0.2</title>
	<guid>http://emptysqua.re/blog/announcing-pymongo-3-0-2/</guid>
	<link>http://emptysqua.re/blog/announcing-pymongo-3-0-2/</link>
	<description>&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/leaf.jpg&quot; alt=&quot;Hepatica leaf&quot; title=&quot;Hepatica leaf&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Bernie Hackett and I are pleased to announce &lt;a href=&quot;https://pypi.python.org/pypi/pymongo/3.0.2&quot;&gt;PyMongo 3.0.2&lt;/a&gt;. This release fixes bugs reported since PyMongo 3.0.1&amp;mdash;most importantly, a bug that could route operations to replica set members that are not in primary or secondary state when using read preference &lt;code&gt;PrimaryPreferred&lt;/code&gt; or &lt;code&gt;Nearest&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For the full list of bugs fixed in PyMongo 3.0.2, please &lt;a href=&quot;https://jira.mongodb.org/browse/PYTHON/fixforversion/15430&quot;&gt;see the release in Jira&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you use PyMongo 3.0.x, upgrade.&lt;/p&gt;
&lt;p&gt;If you are on PyMongo 2.8.0, upgrade to &lt;a href=&quot;http://emptysqua.re/blog/announcing-pymongo-2-8-1/&quot;&gt;yesterday's bugfix release PyMongo 2.8.1 instead&lt;/a&gt;. &lt;a href=&quot;http://api.mongodb.org/python/current/changelog.html&quot;&gt;Read the changelog for major API changes in PyMongo 3&lt;/a&gt;, and test your application carefully with PyMongo 3.0.x before deploying.&lt;/p&gt;&lt;img src=&quot;http://emptysqua.re/blog/analytics/http%3A//emptysqua.re/blog/announcing-pymongo-3-0-2//pixel.gif&quot; width=&quot;1px&quot; height=&quot;1px&quot; /&gt;</description>
	<pubDate>Tue, 12 May 2015 23:04:36 +0000</pubDate>
	<author>ajdavis@cs.oberlin.edu (A. Jesse Jiryu Davis)</author>
</item>
<item>
	<title>A. Jesse Jiryu Davis: Announcing PyMongo 2.8.1</title>
	<guid>http://emptysqua.re/blog/announcing-pymongo-2-8-1/</guid>
	<link>http://emptysqua.re/blog/announcing-pymongo-2-8-1/</link>
	<description>&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/vines.jpg&quot; alt=&quot;Vines&quot; title=&quot;Vines&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://pypi.python.org/pypi/pymongo/2.8.1&quot;&gt;PyMongo 2.8.1&lt;/a&gt; is a bugfix release that addresses issues discovered since PyMongo 2.8 was released, primarily related to authentication and metadata operations on replica sets. If you're on PyMongo 2.8 and not ready to update your code for the &lt;a href=&quot;http://emptysqua.re/blog/pymongo-3-beta/&quot;&gt;new APIs and behaviors of the PyMongo 3.0 line&lt;/a&gt;, please upgrade to PyMongo 2.8.1 promptly.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://jira.mongodb.org/browse/PYTHON-842&quot;&gt;PYTHON-842&lt;/a&gt; -         Unable to specify 'ssl_cert_reqs' option using URI style connection string
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://jira.mongodb.org/browse/PYTHON-864&quot;&gt;PYTHON-864&lt;/a&gt; -         Fully support RFC-3339 offset format for $date
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://jira.mongodb.org/browse/PYTHON-893&quot;&gt;PYTHON-893&lt;/a&gt; -         Wrong wrapping function called for CommandCursor
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://jira.mongodb.org/browse/PYTHON-903&quot;&gt;PYTHON-903&lt;/a&gt; -         Properly handle network errors in auth
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://jira.mongodb.org/browse/PYTHON-913&quot;&gt;PYTHON-913&lt;/a&gt; -         UserWarning with read preference and command using direct connection
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://jira.mongodb.org/browse/PYTHON-915&quot;&gt;PYTHON-915&lt;/a&gt; - secondaryAcceptablelatencyMS should accept 0
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://jira.mongodb.org/browse/PYTHON-918&quot;&gt;PYTHON-918&lt;/a&gt; -         Auth err from resyncing member prevents primary discovery
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://jira.mongodb.org/browse/PYTHON-920&quot;&gt;PYTHON-920&lt;/a&gt; -         collection_names, options, and index_information prohibited on direct connection to secondary with MongoDB 3.0+
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://jira.mongodb.org/browse/PYTHON-921&quot;&gt;PYTHON-921&lt;/a&gt; -         database_names prohibited on direct connection to secondary with MongoDB 3.0+
&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the full list of bugs fixed in PyMongo 2.8.1, please &lt;a href=&quot;https://jira.mongodb.org/browse/PYTHON/fixforversion/15324&quot;&gt;see the release in Jira&lt;/a&gt;. &lt;/p&gt;&lt;img src=&quot;http://emptysqua.re/blog/analytics/http%3A//emptysqua.re/blog/announcing-pymongo-2-8-1//pixel.gif&quot; width=&quot;1px&quot; height=&quot;1px&quot; /&gt;</description>
	<pubDate>Tue, 12 May 2015 01:18:35 +0000</pubDate>
	<author>ajdavis@cs.oberlin.edu (A. Jesse Jiryu Davis)</author>
</item>
<item>
	<title>A. Jesse Jiryu Davis: Announcing Motor 0.4.1</title>
	<guid>http://emptysqua.re/blog/announcing-motor-0-4-1/</guid>
	<link>http://emptysqua.re/blog/announcing-motor-0-4-1/</link>
	<description>&lt;p&gt;&lt;img src=&quot;http://emptysquare.net/blog/media/2012/09/motor-musho.png&quot; alt=&quot;Motor&quot; title=&quot;motor-musho.png&quot; border=&quot;0&quot; /&gt;&lt;/p&gt;
&lt;p&gt;I received an &lt;a href=&quot;https://jira.mongodb.org/browse/MOTOR-66&quot;&gt;extraordinarily helpful bug report&lt;/a&gt; yesterday from Brent Miller, who showed me that Motor's replica set client hangs if it tries two operations at once, &lt;em&gt;while&lt;/em&gt; it is setting up its initial connection. He sent a script that not only reproduces the hang, but diagnoses it, too, by regularly dumping all threads' stacks to a file.&lt;/p&gt;
&lt;p&gt;A report this generous made my work easy. I found that I'd caused this bug while fixing another one. In the previous bug, if Motor's replica set client was under load while reconnecting to your servers, it could start multiple greenlets to monitor your replica set, instead of just one. (Eventually, Motor will be &lt;em&gt;designed&lt;/em&gt; to start multiple greenlets and &lt;a href=&quot;http://emptysqua.re/blog/announcing-pymongo-3/#replica-set-discovery-and-monitoring&quot;&gt;monitor all servers in parallel, the same as PyMongo 3&lt;/a&gt;, but for now, starting multiple monitor greenlets is a bug.)&lt;/p&gt;
&lt;p&gt;I fixed that bug overzealously: now if you start multiple operations on a replica set client as it connects, it does not start the monitor greenlet at all, and deadlocks. Motor 0.4.1 gets it right. It starts one and only one monitor greenlet as it connects to your replica set. Get it from PyPI:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;pip install motor&lt;span&gt;==&lt;/span&gt;0.4.1
&lt;/pre&gt;&lt;/div&gt;&lt;img src=&quot;http://emptysqua.re/blog/analytics/http%3A//emptysqua.re/blog/announcing-motor-0-4-1//pixel.gif&quot; width=&quot;1px&quot; height=&quot;1px&quot; /&gt;</description>
	<pubDate>Sat, 09 May 2015 16:07:05 +0000</pubDate>
	<author>ajdavis@cs.oberlin.edu (A. Jesse Jiryu Davis)</author>
</item>
<item>
	<title>A. Jesse Jiryu Davis: Download Stats By Python Version: PyMongo, Motor, Tornado</title>
	<guid>http://emptysqua.re/blog/download-stats-by-python-version-pymongo-motor-tornado/</guid>
	<link>http://emptysqua.re/blog/download-stats-by-python-version-pymongo-motor-tornado/</link>
	<description>&lt;p&gt;After PyCon last month, Python packaging saint Donald Stufft &lt;a href=&quot;https://twitter.com/dstufft/status/589596259071221762&quot;&gt;generously tweeted&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If you&amp;rsquo;re interested to see Py2 vs Py3 breakdowns for a Python package, let me know while I still have the data set loaded (~300GB DB).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;He had the database loaded for his diverting article on a &lt;a href=&quot;https://caremad.io/2015/04/a-year-of-pypi-downloads/&quot;&gt;Year of PyPI Downloads&lt;/a&gt;, but I was curious about three packages I own or contribute to.&lt;/p&gt;
&lt;hr /&gt;
&lt;div class=&quot;toc&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://emptysqua.re/blog/feed/#pymongo&quot;&gt;PyMongo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://emptysqua.re/blog/feed/#motor&quot;&gt;Motor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://emptysqua.re/blog/feed/#tornado&quot;&gt;Tornado&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://emptysqua.re/blog/feed/#conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h1 id=&quot;pymongo&quot;&gt;PyMongo&lt;/h1&gt;
&lt;p&gt;Here's PyMongo's downloads this year, by Python version:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/pymongo-downloads.png&quot; alt=&quot;PyMongo downloads&quot; title=&quot;PyMongo downloads&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Python 2.7 dominates, and it is growing at the expense of 2.6. Python 2.4 is absent, and PyMongo downloads for Python 2.5 vanished last year, which validates our decision to &lt;a href=&quot;http://api.mongodb.org/python/current/changelog.html&quot;&gt;drop Python 2.4 and 2.5 from the latest release, PyMongo 3.0&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Donald made a second chart isolating the Python 3 downloads:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/pymongo-downloads-python-3.png&quot; alt=&quot;PyMongo downloads for Python 3&quot; title=&quot;PyMongo downloads for Python 3&quot; /&gt;&lt;/p&gt;
&lt;p&gt;As one expects, people who use Python 3 use the latest one, Python 3.4. When we released PyMongo 2.8.0 at the end of January, the download spike was &lt;em&gt;entirely&lt;/em&gt; Python 3.4 users.&lt;/p&gt;
&lt;p&gt;I expect Python 3.5 will soon dominate among Python 3 users, and Python 2.6 will continue to decline asymptotically, while the Python 2 versus 3 ratio overall will stay steady for a few more years.&lt;/p&gt;
&lt;h1 id=&quot;motor&quot;&gt;Motor&lt;/h1&gt;
&lt;p&gt;Something is wrong with Donald's chart for Motor, but it suggests that Motor users are like PyMongo users: 10% run the latest Python 3, 50% run Python 2.7, and the others run a smattering of other Pythons. (Motor has never supported Python before 2.6.)&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/motor-downloads.png&quot; alt=&quot;Motor downloads&quot; title=&quot;Motor downloads&quot; /&gt;&lt;/p&gt;
&lt;p&gt;In Donald's Python 3 chart, it seems only Python 3.4 is represented:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/motor-downloads-python-3.png&quot; alt=&quot;Motor downloads for Python 3&quot; title=&quot;Motor downloads for Python 3&quot; /&gt;&lt;/p&gt;
&lt;h1 id=&quot;tornado&quot;&gt;Tornado&lt;/h1&gt;
&lt;p&gt;Motor's potential user base includes all Tornado users, so I was curious about Tornado's overall distribution:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/tornado-downloads.png&quot; alt=&quot;Tornado downloads&quot; title=&quot;Tornado downloads&quot; /&gt;&lt;/p&gt;
&lt;p&gt;I have no explanation for the spike of Python 2.6 downloads last fall; Tornado's release schedule doesn't strongly correlate with it. Tornado's user base is distributed similarly to PyMongo's, though more inclined to stay on Python 2.6.&lt;/p&gt;
&lt;p&gt;Again, Tornado's Python 3 users hold steady at ten percent, but they switched to Python 3.4 quickly after it was released.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/tornado-downloads-python-3.png&quot; alt=&quot;Tornado downloads for Python 3&quot; title=&quot;Tornado downloads for Python 3&quot; /&gt;&lt;/p&gt;
&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;The takeaways are: Python 2.4 is dead, and 2.5 is effectively dead for PyMongo, Motor, and Tornado users. It's important to keep packages working in Python 2.6&amp;mdash;mainly for enterprises with long-term support contracts for Linux versions that shipped with 2.6&amp;mdash;but adding features or optimizations that only work in Python 2.7 is reasonable now.&lt;/p&gt;
&lt;p&gt;It's critical we keep testing Python 3.5 alphas and betas as they come out, because the 10% of people who run Python 3 will migrate to 3.5 rapidly. Python 3.2 is nearly dead, and 3.3 will also vanish soon.&lt;/p&gt;&lt;img src=&quot;http://emptysqua.re/blog/analytics/http%3A//emptysqua.re/blog/download-stats-by-python-version-pymongo-motor-tornado//pixel.gif&quot; width=&quot;1px&quot; height=&quot;1px&quot; /&gt;</description>
	<pubDate>Wed, 06 May 2015 22:22:59 +0000</pubDate>
	<author>ajdavis@cs.oberlin.edu (A. Jesse Jiryu Davis)</author>
</item>
<item>
	<title>A. Jesse Jiryu Davis: Review of &quot;Autotools: A Practitioner's Guide&quot;</title>
	<guid>http://emptysqua.re/blog/review-of-autotools-a-practitioners-guide/</guid>
	<link>http://emptysqua.re/blog/review-of-autotools-a-practitioners-guide/</link>
	<description>&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/autotools-calcote.png&quot; alt=&quot;Autotools calcote&quot; title=&quot;Autotools calcote&quot; /&gt;&lt;/p&gt;
&lt;p&gt;My colleague &lt;a href=&quot;https://twitter.com/starsseldomseen&quot;&gt;Amalia Hawkins&lt;/a&gt; and I are choosing interns for our project this summer: a new Lua driver for MongoDB. Looking over their resumes I have to chuckle. Kid, you're 20 years old, you're not proficient in C, C++, Java, Javascript, and PHP. Maybe you did some homework in a language and you know some syntax, but it does not make you &quot;proficient.&quot; I am proficient in one language: Python. It took me a decade.&lt;/p&gt;
&lt;p&gt;When I began learning Python as a professional, what I didn't know that I didn't know was how to maintain a cross-platform open-source package like PyMongo. I didn't know how to write source code compatible with Python 2 and 3. I didn't know all the grotty little details about individual Python versions. I didn't know, for example, &lt;a href=&quot;http://emptysqua.re/blog/another-thing-about-pythons-threadlocals/&quot;&gt;that &lt;em&gt;assigning&lt;/em&gt; to a threadlocal isn't thread-safe in Python 2.6&lt;/a&gt;. I did not know &lt;a href=&quot;http://emptysqua.re/blog/python-c-extensions-and-mod-wsgi/&quot;&gt;how to design Python C extensions to run in mod_wsgi sub interpreters&lt;/a&gt;. I did not know that Python 2.7's unittest framework introduced &lt;a href=&quot;https://docs.python.org/2/library/unittest.html#unittest.TestCase.assertRaisesRegexp&quot;&gt;&lt;code&gt;assertRaisesRegexp&lt;/code&gt;&lt;/a&gt; and that it was reintroduced to Python 3.1 as &lt;code&gt;assertRaisesRegex&lt;/code&gt; without the &quot;p&quot;, then backported to Python 2.6 via &lt;code&gt;unittest2&lt;/code&gt;, again without the &quot;p&quot;. I did not know how to make a package installable on Linux, Mac, and Windows, with and without a C compiler.&lt;/p&gt;
&lt;p&gt;I tell you this, not to brag about what I have learned&amp;mdash;although to be honest I &lt;em&gt;am&lt;/em&gt; proud of how far I have come. I just want to explain how high my standards for &quot;proficient&quot; have risen, as a result of my experiences working on PyMongo.&lt;/p&gt;
&lt;p&gt;Now that I am taking over &lt;a href=&quot;https://github.com/mongodb/libbson&quot;&gt;libbson&lt;/a&gt; and &lt;a href=&quot;https://github.com/mongodb/mongo-c-driver&quot;&gt;libmongoc&lt;/a&gt;, the C libraries for MongoDB client applications, I am humbled by how much I do not know. I am even more humbled by the unknown unknowns. I expect they are primarily in the distribution tools, not the language. After all, I &quot;learned C&quot; in college&amp;mdash;or so I thought when I was 20 years old. But all I learned was its syntax. What I did not learn was how to maintain a cross-platform open-source distribution in C. So when I was charged with becoming an expert C programmer, I did not pick up a book on syntax or algorithms. I looked for a book on the Autotools.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.nostarch.com/autotools.htm&quot;&gt;Autotools: A Practioner's Guide to GNU Autoconf, Automake, and Libtool&lt;/a&gt;, by John Calcote, seems virtually the only modern book on the subject. Compared to all the C++ books out there, or all the &quot;Learn C in 21 Days&quot;-type introductions to the language, there is very little written about the Autotools. But a huge proportion of open source software for Linux and Unix uses the Autotools for packaging and distribution. Becoming an expert open source C programmer requires enough familiarity with the Autotools to create distributions and debug problems in them.&lt;/p&gt;
&lt;p&gt;Calcote's book tackles this tiresome subject in a stylishly written and carefully organized manner.&lt;/p&gt;
&lt;p&gt;We get off to a bumpy start, however. Calcote must begin by listing the parts of the Rube Goldberg device: primarily Automake, Autoconf, and Libtool. But there are wheels within wheels, like &lt;code&gt;autom4te&lt;/code&gt;, &lt;code&gt;aclocal&lt;/code&gt;, and a whole gearbox of little-known cogs. We barely understand the purpose of the Autotools, so we read the list of parts without understanding their functions. It is discouraging at the outset. But you will be alright if you just read the chapter once, do your best, then soldier on ignorantly and hope to muddle through.&lt;/p&gt;
&lt;p&gt;It only gets more disheartening when we come to the data-flow diagrams:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/05/data-flow.png&quot; alt=&quot;Data flow&quot; title=&quot;Data flow&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Data can flow in so many directions among the Autotools that the diagrams, although we desperately want them, are almost meaningless: every file depends on all the others, it seems, and the scripts all call each other. The system is so complex you need to read much of the book before the diagrams in Chapter One can make sense. Indeed, I have read the whole book, then reread the first chapter, and I still do not understand it. But the round trip brings a &lt;em&gt;bit&lt;/em&gt; more clarity, and more hope.&lt;/p&gt;
&lt;p&gt;The next chapters are tractable, since they explain the contraption one gear at a time, beginning at the lowest-level tools and working backwards from there. Due to the Autotools' design-by-accretion, each chapter discusses a tool, like &lt;code&gt;make&lt;/code&gt;, and the script it executes, like &lt;code&gt;Makefile&lt;/code&gt;, and then the tool that generates that script from a higher-level script. As we climb the levels of script-generating scripts we arrive, gasping, at the pinnacle: Automake. Except, of course, that the levels are not so cleanly separated at all, but are incomprehensibly intertwined. Calcote makes a heroic effort to focus each chapter on a specific aspect of the toolkit, but by necessity refers upwards and downwards, in the same way the tools themselves do.&lt;/p&gt;
&lt;p&gt;In Chapters Eight and Nine, Calcote describes the monstrous undertaking of converting a large project from hand-written Makefiles to the Autotools. Here I skimmed quickly, but my admiration deepened for the author's patience. If I am ever willing to make such an expedition myself, I will want Calcote beside me. As A. E. Housman put it:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It will do good to heart and head&lt;br /&gt;
When your soul is in my soul's stead;&lt;br /&gt;
And I will friend you, if I may,&lt;br /&gt;
In the dark and cloudy day.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The last two chapters best fit the archetype of a &quot;missing manual&quot; for the Autools. In Chapter Ten, Calcote teaches us to use the M4 macro language in the context of Autoconf. I had recently been frustrated Googling for something as simple as this: How to execute one M4 expression if a symbol is defined, and another expression if not. The main trouble is that &lt;a href=&quot;https://www.gnu.org/software/m4/manual/m4.html&quot;&gt;the M4 Manual&lt;/a&gt; teaches you to use raw M4, and &lt;a href=&quot;https://www.gnu.org/software/autoconf/manual/autoconf-2.68/html_node/Redefined-M4-Macros.html&quot;&gt;the Autoconf Manual tells you that it has renamed all of M4's macros&lt;/a&gt;, so there is no one place on the Internet that actually tells you how to write Autoconf scripts. Calcote's explanation suffices to get you unstuck.&lt;/p&gt;
&lt;p&gt;The final chapter, &quot;A Catalog of Tips and Reusable Solutions,&quot; is without apology a bunch of facts that washed to the end of the book without coming to rest in any of the tutorial chapters. Yet the short sections here are more worthwhile than a &quot;catalog&quot; might promise. The opening bit on &quot;Keeping Private Details out of Public Interfaces&quot; teaches you enough about designing stable C APIs to keep you safe until you read David R. Hanson's &lt;a href=&quot;http://amzn.com/0201498413&quot;&gt;C Interfaces and Implementations&lt;/a&gt;. Later, the section on cross-compilation collects crucial details, not just for people who actually cross-compile libraries, but for maintainers like me who must ensure their libraries are truly portable.&lt;/p&gt;
&lt;p&gt;&quot;Autotools: A Practitioner's Guide&quot; is not fun to read, most likely. You read it when you must, and on that day it is indispensable. Calcote completes a tough job with honor and style, and his book testifies to his years of honest effort.&lt;/p&gt;&lt;img src=&quot;http://emptysqua.re/blog/analytics/http%3A//emptysqua.re/blog/review-of-autotools-a-practitioners-guide//pixel.gif&quot; width=&quot;1px&quot; height=&quot;1px&quot; /&gt;</description>
	<pubDate>Mon, 04 May 2015 04:07:48 +0000</pubDate>
	<author>ajdavis@cs.oberlin.edu (A. Jesse Jiryu Davis)</author>
</item>
<item>
	<title>Tim Callaghan: Percona Acquires Tokutek : My Thoughts #2 : TokuMX and TokuMXse</title>
	<guid>tag:blogger.com,1999:blog-8043938871710850997.post-7430198453412386362</guid>
	<link>http://www.acmebenchmarking.com/2015/04/percona-acquires-tokutek-my-thoughts-2.html</link>
	<description>&lt;span&gt;A few days ago I wrote up &lt;a href=&quot;http://www.acmebenchmarking.com/2015/04/percona-acquires-tokutek-my-thoughts-1.html?view=classic&quot; target=&quot;_blank&quot;&gt;my thoughts about the Percona acquisition of Tokutek with respect to TokuDB&lt;/a&gt;. In this blog I'm going to do the same for TokuMX and TokuMXse. And in a few days I'll wrap up this trilogy by sharing my thoughts about Fractal Tree Indexes.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;i&gt;&lt;span&gt;Again, when I'm writing up something that I was very involved with in the past I think it's important to disclose that I worked at Tokutek for 3.5 years (08/2011 -  01/2015) as VP/Engineering and I do not have any equity in Tokutek or  Percona.&lt;/span&gt;&lt;/i&gt;&lt;br /&gt;&lt;i&gt;&lt;span&gt;&lt;/span&gt;&lt;/i&gt;&lt;br /&gt;&lt;i&gt;&lt;b&gt;&lt;span&gt;&lt;/span&gt;&lt;/b&gt;&lt;/i&gt;&lt;span&gt;Since much of the MySQL crowd might be hearing about Tokutek's &quot;other products&quot; for the first time I'll provide a little history of both of the products before I dive in deeper.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;&lt;a href=&quot;http://www.tokutek.com/tokumx-for-mongodb/&quot; target=&quot;_blank&quot;&gt;TokuMX&lt;/a&gt; is a fork of MongoDB that was &lt;a href=&quot;http://www.tokutek.com/2013/06/announcing-tokumx-v1-0-tokumongo-you-can-have-it-all-2/&quot; target=&quot;_blank&quot;&gt;launched on June 19, 2013&lt;/a&gt;. It began as an experiment in late 2012 by adding Fractal Tree Indexes to handle the secondary indexing work for stock MongoDB while leaving management of the collection data to MongoDB's MMAPv1 storage code. Needless to say the &lt;a href=&quot;http://www.tokutek.com/2012/08/10x-insertion-performance-increase-for-mongodb-with-fractal-tree-indexes/&quot; target=&quot;_blank&quot;&gt;performance&lt;/a&gt; &lt;a href=&quot;http://www.tokutek.com/2012/08/268x-query-performance-increase-for-mongodb-with-fractal-tree-indexes-say-what/&quot; target=&quot;_blank&quot;&gt;gains&lt;/a&gt; of the prototype were stunning, it provided &lt;a href=&quot;http://www.tokutek.com/2013/02/mongodb-fractal-tree-indexes-high-compression/&quot; target=&quot;_blank&quot;&gt;amazing compression&lt;/a&gt;, and even &lt;a href=&quot;http://www.tokutek.com/2013/10/introducing-tokumx-transactions-for-mongodb-applications/&quot; target=&quot;_blank&quot;&gt;enabled transactions and snapshot queries&lt;/a&gt;. As a fork it, TokuMX is a drop-in replacement for MongoDB: same wire protocol, same client libraries, same commands and query syntax, etc. TokuMX has released many GA versions since the v1.0 launch in June 2013; including 1.1, 1.2, 1.3, 1.4, 1.5, and 2.0.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&lt;a href=&quot;http://www.tokutek.com/2015/02/tokutek-take-mongodb-v3-0/&quot; target=&quot;_blank&quot;&gt;TokuMXse&lt;/a&gt; is Tokutek's implementation of it's Fractal Tree Indexes in MongoDB v3.0, using the newly available Storage Engine API. One huge advantage of creating a storage engine version of TokuMX is to allow users to mix-and-match storage engines by adding a new server to a running MongoDB v3.0 replica set. A huge disadvantage is that most of the TokuMX goodness is unavailable, I'll get more into this later. Also worth noting is that&amp;nbsp;&lt;a href=&quot;http://www.tokutek.com/&quot; target=&quot;_blank&quot;&gt;Tokutek&lt;/a&gt; has not yet created a GA version of TokuMXse, the most recent information I can see is from this &lt;a href=&quot;https://groups.google.com/forum/?hl=en#!topic/tokumx-user/VOElRL1ok1Y&quot; target=&quot;_blank&quot;&gt;Google Groups post&lt;/a&gt;. I'm curious as to how/if the Percona acquisition will affect the timing of a TokuMXse GA version.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;&lt;b&gt;&lt;span&gt;Thoughts on Percona + TokuMX&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;b&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/b&gt; &lt;/span&gt;&lt;br /&gt;&lt;span&gt;Features - TokuMX brings the following exclusive* features to MongoDB users&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;In one of the v1.x releases TokuMX introduced partitioning, specifically to deal with performance issues relating to capped collections (the oplog was public enemy #1 when it came to killing the performance of the server). This enabled light-weight deletion of large amounts of data, just what the oplog needed. The TokuMX oplog is bounded by time (hours or days), and not size (as MongoDB is). Any TokuMX collection can be partitioned if the collection is not sharded. I believe that sharded TokuMX collections can now be partitioned but there are some significant restrictions on them. If you are curious, &lt;a href=&quot;https://twitter.com/zkasheff&quot; target=&quot;_blank&quot;&gt;Zardosht Kasheff&lt;/a&gt; wrote a lot of great content on the subject, &lt;a href=&quot;https://www.google.com/search?q=tokumx+partitioning&quot; target=&quot;_blank&quot;&gt;Google &quot;tokumx partitioning&quot;&lt;/a&gt; for blogs and documentation.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Some Fractal Tree Indexes operations can be done blindly, that is they don't require a read before a write. TokuMX enables &lt;a href=&quot;http://www.tokutek.com/2014/03/why-tokumx-replication-differs-from-mongodb-replication/&quot; target=&quot;_blank&quot;&gt;read-free replication to the secondaries&lt;/a&gt; and performs &lt;a href=&quot;http://www.tokutek.com/2014/09/fast-updates-coming-soon-in-tokumx-v2-0/&quot; target=&quot;_blank&quot;&gt;&quot;fast-updates&quot; ($inc, $set, ...)&lt;/a&gt; in a read-free manner.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;High-performance indexing, especially secondary indexes.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Document-level locking.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;High compression.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Multi-statement transactions in unsharded environments.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Snapshot queries. Queries don't need to worry about dirty-reads, or documents missing from a query because they were deleted before the reader got to them.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;TokuMX in the age of MongoDB v3.0+&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;I used an asterisk after the term &quot;exclusive&quot; in the above section as alternative storage engines will surely enable some of the named features.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;http://www.mongodb.com/press/wired-tiger&quot; target=&quot;_blank&quot;&gt;MongoDB acquired WiredTiger in December 2014&lt;/a&gt;, and is currently the only other &quot;in-the-box&quot; storage engine in MongoDB v3.0. The &lt;a href=&quot;http://www.wiredtiger.com/&quot; target=&quot;_blank&quot;&gt;WiredTiger&lt;/a&gt; storage engine provides document-level locking, compression (I haven't done much to measure it's compression vs. the Tokutek products, but I will), and will eventually provide a write-optimized options via it's &lt;a href=&quot;http://en.wikipedia.org/wiki/Log-structured_merge-tree&quot; target=&quot;_blank&quot;&gt;LSM&lt;/a&gt; implementation, but only it's B-trees are supported in MongoDB v3.0.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;http://www.rocksdb.org/&quot; target=&quot;_blank&quot;&gt;RocksDB&lt;/a&gt; powers a MongoDB v3.0 storage engine (it can be used natively and also coming as a MySQL storage engine), and it is backed by &lt;a href=&quot;http://www.facebook.com/&quot; target=&quot;_blank&quot;&gt;Facebook&lt;/a&gt;. A few weeks ago &lt;a href=&quot;https://twitter.com/mipsytipsy&quot; target=&quot;_blank&quot;&gt;Charity Majors&lt;/a&gt; at &lt;a href=&quot;https://parse.com/&quot; target=&quot;_blank&quot;&gt;Parse&lt;/a&gt; &lt;a href=&quot;http://blog.parse.com/announcements/mongodb-rocksdb-parse/&quot; target=&quot;_blank&quot;&gt;announced that Parse is running MongoDB + RocksDB and &quot;seeing great results&quot;&lt;/a&gt;. RocksDB is also an LSM implementation with compression and write-optimizations, and yes AcmeBenchmarking will be getting hands-on time with it soon.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;Opportunities in 2015 (and beyond)?&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;The storage engine API is brand new in MongoDB v3.0. It appears that MongoDB is looking to enhance the API with each coming release and has announced a &lt;a href=&quot;http://www.mongodb.com/blog/post/mongodb-storage-engine-summit-june-4th&quot; target=&quot;_blank&quot;&gt;MongoDB Storage Engine Summit&lt;/a&gt; as part of &lt;a href=&quot;http://mongodbworld.com/&quot; target=&quot;_blank&quot;&gt;MongoDB World 2015&lt;/a&gt;. Storage engines and storage engine APIs are hard. It will take time for the API to improve enough to provide many of the features that TokuMX provides (since June 2013).&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;WiredTiger is new to MongoDB, so much of 2015 will likely be spent learning/tweaking/fixing. At some point, maybe in 2015, MongoDB will support WiredTiger's LSM implementation which should improve secondary indexing performance significantly, especially for people with slower IO subsystems. It will likely be helpful on capped collections (think oplog).&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;RocksDB is likely not tuned for many MongoDB workloads, so there will be plenty of learning to do here as well. Facebook is likely to work on it's internal use-cases before ones from the rest of the world. If it's not significantly better than WiredTiger and isn't &quot;in-the-box&quot; then there will be little reason for people to try it, but that's just my opinion.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;MongoDB Version Support&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;TokuMX is based on MongoDB v2.4.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;MongoDB v2.6 was released in April 2014 and MongoDB v3.0 was released in February 2015.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;So TokuMX is now two major releases behind MongoDB and needs to catch up soon.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;&lt;span&gt;Files, Files, Files&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Just like TokuDB, TokuMX creates a lot of files. Two files per collection, plus another file for each secondary index.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;On the contrary, MongoDB MMAPv1 uses &quot;tablespaces&quot; so there are far fewer files.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;However, WiredTiger creates a similar number of files as TokuMX, so maybe this is a non-issue. I'd wager that there will be a &quot;collapsing of files&quot; effort at WiredTiger at some point.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;Hot Backup vs. MongoDB Backups&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;TokuMX's hot backup feature is  enterprise edition only (it is a paid feature) and is closed source.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Again, it feels weird that  Percona owns/offers a closed source technology, open sourcing the TokuMX hot backup would be nice to see.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;Other Enterprise Edition Features&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;TokuMX also offers audit capabilities and point in time recovery as paid closed-source features.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;And again, it feels weird that  Percona owns/offers a closed source technology, open sourcing all TokuMX enterprise features would be nice to see.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;&lt;span&gt;&lt;span&gt;&lt;b&gt;&lt;span&gt;Thoughts on Percona + TokuMXse&lt;/span&gt;&lt;/b&gt;&lt;/span&gt; &lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Meh&quot; target=&quot;_blank&quot;&gt;Meh&lt;/a&gt; Factor&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Sorry, I couldn't think of a better heading for this section.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;There are so many features in TokuMX that aren't in TokuMXse.&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Unless you need to inter-operate with MongoDB, use TokuMX and not TokuMXse (or if you need v2.6 or other v3.0 features)&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;span&gt;Release Schedule&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;As I mentioned above there have been several release candidates of TokuMXse, plus some chatter about an upcoming GA version, but nothing has materialized.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Also important is the definition of what exactly TokuMXse &quot;is&quot;.&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Is it an &quot;light&quot; version of TokuMX, intended to give users a taste of the technology?&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Will it be free or lower cost than TokuMX?&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;br /&gt;&lt;span&gt;Competition&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;TokuMXse has several advantages versus MMAPv1. &lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;WiredTiger and RocksDB are good technology.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;WiredTiger is backed by MongoDB, RocksDB is backed by Facebook. &lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;How much better is TokuMXse performance and compression in real-world usage?&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;&lt;span&gt;MongoDB Storage Engine API Additions&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;It is going to be interesting to see how the API evolves over time.&amp;nbsp;&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Additions will surely come, but how will additions that enable a feature for a single storage engine be handled.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;br /&gt;&lt;span&gt;&lt;span&gt;&lt;b&gt;&lt;span&gt;General Thoughts on Percona + TokuMX[se&lt;/span&gt;&lt;/b&gt;&lt;/span&gt;]&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;Human Resources&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Percona has been performing MySQL  related engineering for quite a while now, but TokuMX[se] are very different efforts.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;TokuMX is a fork of MongoDB, not a patch.&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Keeping up with new releases is a lot of work! &lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span&gt;As with TokuDB, I'm curious to see how much they  are looking to grow the team after adding TokuMX[se] to their product list.  They've already posted on their &lt;a href=&quot;http://www.percona.com/about-us/careers/open-positions&quot;&gt;jobs page&lt;/a&gt; for &quot;&lt;/span&gt;&lt;span&gt;C/C++ Developers for TokuDB, TokuMX, and Tokutek Products&quot;.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Prioritizing resources between TokuDB, TokuMX, and TokuMXse will be tricky.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Percona based support is attractive, IMHO.&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;But their support team has no experience with TokuMX[se].&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span&gt;Percona consulting also has no experience with TokuMX[se].&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;&lt;span&gt;Awareness&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;The Percona brand and marketing team will significantly raise awareness to the existence of TokuMX[se].&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Existing Percona customers will be exposed to TokuMX[se], especially if they are &quot;MongoDB curious&quot;.&amp;nbsp;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;The combination of these two should raise the number of TokuMX[se] customers and users.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;&lt;span&gt;Please asks questions or comment below.&lt;/span&gt;</description>
	<pubDate>Wed, 29 Apr 2015 14:10:59 +0000</pubDate>
	<author>noreply@blogger.com (Tim Callaghan)</author>
</item>
<item>
	<title>Tim Callaghan: Percona Acquires Tokutek : My Thoughts #1 : TokuDB</title>
	<guid>tag:blogger.com,1999:blog-8043938871710850997.post-4986317862176991822</guid>
	<link>http://www.acmebenchmarking.com/2015/04/percona-acquires-tokutek-my-thoughts-1.html</link>
	<description>&lt;span&gt;Two weeks ago Percona announced it's acquisition of Tokutek (April 14, 2015). The analyst coverage was a bit fluffy for my liking, but I decided to give it some time and see if anything &quot;meaty&quot; would come along, and ... it hasn't. The sheer number of &lt;a href=&quot;https://twitter.com/search?q=percona%20tokutek&amp;src=typd&quot;&gt;tweets on Twitter&lt;/a&gt; was impressive, which makes me hopeful that the acquisition raised awareness to the Tokutek technologies and that the Tokutek products have a found a good home&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;I've been thinking a lot about the future of the Tokutek technologies over these same two weeks and want to share them publicly. I'm going to cover TokuDB in this blog post, TokuMX in a few days, and finally Fractal Tree Indexes a few days later. &lt;i&gt;[Full disclosure: I worked at Tokutek for 3.5 years (08/2011 - 01/2015) as VP/Engineering and I do not have any equity in Tokutek or Percona]&lt;/i&gt;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;i&gt;&lt;b&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/b&gt;&lt;/i&gt;&lt;b&gt;&lt;span&gt;Thoughts on Percona + TokuDB&lt;/span&gt;&lt;/b&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Integration and Ease of Use&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Percona will certainly spend the time to make using the TokuDB storage engine in Percona Server as easy and foolproof as possible. Prior to the acquisition, users needed to download and install an additional package to use the TokuDB storage engine in Percona Server (this was not the case for those downloading directly from the &lt;a href=&quot;http://www.tokutek.com/&quot; target=&quot;_blank&quot;&gt;Tokutek web site&lt;/a&gt; or downloading from &lt;a href=&quot;https://downloads.mariadb.org/&quot; target=&quot;_blank&quot;&gt;MariaDB&lt;/a&gt;). I hope that TokuDB becomes part of the base Percona Server package and that the plugin is installed by default. At that point the process of trying TokuDB is as easy as adding &quot;engine=TokuDB&quot; in a CREATE TABLE statement.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Memory usage (cache). InnoDB supports a user defined cache size, as does TokuDB. Users often allocate more than 50% to their cache to their storage engine and can easily over allocate the server's memory if using both engines. I'm not sure what the ideal solution is for this problem. I think InnoDB supports a dynamic cache sizing in MySQL 5.7, perhaps adding this feature to TokuDB and automatically changing the [over]allocation would work.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;Foreign Keys&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Had MySQL implemented foreign key constraints above the storage engine this would have been a non-issue, but it didn't. They are implemented within InnoDB. Will foreign keys ever come to TokuDB? I'd argue that it's not worth the effort, and users needing foreign keys can always use InnoDB for those specific tables. But lack of foreign keys certainly complicates the user's experience.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;Files, Files, Files&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Most people use InnoDB's file-per-table option, meaning a single file is creating in the file system for each table (I'm not going to count .frm files). In contrast, TokuDB creates 2 files for a table, and another file for each secondary index. A great benefit of this approach is that dropping an index is instantaneous, and all the space for that index is returned immediately. The downside is the sheer number of files, especially if you have a large number of tables. And a lot more files if you partition your tables (a full set of the before mentioned file for each partition).&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;All TokuDB files are kept in the root of the data folder (or they can be put in a single TokuDB defined data directory). Moving the files to the individual database folders would be a nice feature.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;Hot Backup vs. XtraBackup&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Creating an online backup of TokuDB is significantly different than performing the same operation of InnoDB. Percona created XtraBackup to simplify the backup process for InnoDB and it is now a feature-rich backup technology (full backups, incremental backups, etc). XtraBackup &lt;u&gt;&lt;b&gt;does not&lt;/b&gt;&lt;/u&gt; work on TokuDB tables.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;TokuDB's hot backup feature is enterprise edition only (it is the paid feature), closed source, and only supports the creation of a full backup. It does work on InnoDB tables, as long as &lt;a href=&quot;http://dev.mysql.com/doc/innodb/1.1/en/innodb-performance-aio-linux.html&quot; target=&quot;_blank&quot;&gt;asynchronous IO&lt;/a&gt; is not enabled.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;What does the future hold? It would be great to see TokuDB's hot backup functionality merged into XtraBackup so a single backup technology existed that &quot;just worked&quot; for both storage engines.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;At the moment it feels weird that Percona owns/offers a closed source technology, open sourcing the TokuDB hot backup would be nice to see.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;Instrumentation and Utilities&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Percona Server is well known for the additional instrumentation it provides, it would be awesome if this operational &quot;tooling&quot; could also be applied to the TokuDB storage engine internals and exposed easy consumption.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;It will also be interesting to see if TokuDB gets more attention in Percona's &lt;a href=&quot;https://cloud.percona.com/&quot; target=&quot;_blank&quot;&gt;cloud tools&lt;/a&gt;. Percona could collect and analyze information from servers using InnoDB and make the recommendation that TokuDB be adopted by analyzing the user's workload.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;As Percona provides support to TokuDB customers and gathers feedback from the TokuDB community there will likely be features and new utilities added to the &lt;a href=&quot;http://www.percona.com/software/percona-toolkit&quot; target=&quot;_blank&quot;&gt;Percona Toolkit&lt;/a&gt;.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;Native Partitioning&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;InnoDB is adding native partitioning in MySQL 5.7. Partitioning is currently handled by what is essentially a &quot;storage engine&quot;, which is pretty cool. A big downside to this implementation is that queries needing data from multiple partitions query each partition in order, and can take a long time when the number of partitions is large. I assume that InnoDB's long term plans for native partitioning is to support concurrent queries on multiple partitions, we shall see. Percona will need to invest in TokuDB to bring native partitioning to it as well.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;MySQL and MariaDB Support&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Will Percona assist MariaDB with the engineering/QA/packaging of TokuDB?&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Will Percona offer a MySQL version of TokuDB as Tokutek has in the past?&amp;nbsp;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Time will tell.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;Human Resources&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Percona has been performing MySQL related engineering for quite a while now, but TokuDB is not exactly the same effort as XtraDB and XtraBackup. I'm curious to see how much they are looking to grow the team after adding TokuDB to their product list. They've already posted on their &lt;a href=&quot;http://www.percona.com/about-us/careers/open-positions&quot;&gt;jobs page&lt;/a&gt; for &quot;&lt;/span&gt;&lt;span&gt;C/C++ Developers for TokuDB, TokuMX, and Tokutek Products&quot;.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Adding Percona based support for TokuDB is a huge win for current and future TokuDB customers.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Percona consulting will quickly learn the best workloads for TokuDB which should grow the user base (both paid and community).&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;I'm excited about all of these possibilities for TokuDB.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;I will probably come up with more thoughts over time, but this feels like a good place to stop for now. I'll post my TokuMX and TokuMXse thoughts in a few days.&amp;nbsp;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;Please asks questions or comment below.&lt;/span&gt;</description>
	<pubDate>Wed, 29 Apr 2015 08:38:29 +0000</pubDate>
	<author>noreply@blogger.com (Tim Callaghan)</author>
</item>
<item>
	<title>A. Jesse Jiryu Davis: Photos From My Shuso Hossen</title>
	<guid>http://emptysqua.re/blog/photos-from-my-shuso-hossen/</guid>
	<link>http://emptysqua.re/blog/photos-from-my-shuso-hossen/</link>
	<description>&lt;p&gt;My fellow meditator and photographer &lt;a href=&quot;https://twitter.com/stillman_brown&quot;&gt;Stillman Brown&lt;/a&gt; was generous enough to take pictures during the shuso hossen ceremony the other week. My teacher Enkyo Roshi allowed me to give &lt;a href=&quot;http://emptysqua.re/blog/yangshan-plants-his-hoe/&quot;&gt;my first Zen talk&lt;/a&gt; and I became a senior student at the Village Zendo.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/04/jiryu_selects-3.jpg&quot; alt=&quot;Jiryu selects 3&quot; title=&quot;Jiryu selects 3&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/04/jiryu_selects-6.jpg&quot; alt=&quot;Jiryu selects 6&quot; title=&quot;Jiryu selects 6&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The verse that comes with the koan I spoke on, &quot;Yangshan Plants His Hoe.&quot; Calligraphy by &lt;a href=&quot;http://whimsyload.com/&quot;&gt;Musho&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/04/jiryu_selects-8.jpg&quot; alt=&quot;Jiryu selects 8&quot; title=&quot;Jiryu selects 8&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/04/jiryu_selects-9.jpg&quot; alt=&quot;Jiryu selects 9&quot; title=&quot;Jiryu selects 9&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/04/jiryu_selects-10.jpg&quot; alt=&quot;Jiryu selects 10&quot; title=&quot;Jiryu selects 10&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/04/jiryu_selects-15.jpg&quot; alt=&quot;Jiryu selects 15&quot; title=&quot;Jiryu selects 15&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/04/jiryu_selects-16.jpg&quot; alt=&quot;Jiryu selects 16&quot; title=&quot;Jiryu selects 16&quot; /&gt;&lt;/p&gt;
&lt;p&gt;There's a moment of high ritual where I return the koan to my teacher.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/04/jiryu_selects-20.jpg&quot; alt=&quot;Jiryu selects 20&quot; title=&quot;Jiryu selects 20&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Carrying the short staff back to my seat so I can engage in &quot;dharma combat&quot;, the questions and answers that follow my talk.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/04/jiryu_selects-23.jpg&quot; alt=&quot;Jiryu selects 23&quot; title=&quot;Jiryu selects 23&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Jennifer asks me a question.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/04/jiryu_selects-31.jpg&quot; alt=&quot;Jiryu selects 31&quot; title=&quot;Jiryu selects 31&quot; /&gt;&lt;/p&gt;
&lt;p&gt;With my teacher Enkyo Roshi.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/04/jiryu_selects-33.jpg&quot; alt=&quot;Jiryu selects 33&quot; title=&quot;Jiryu selects 33&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/04/jiryu_selects-37.jpg&quot; alt=&quot;Jiryu selects 37&quot; title=&quot;Jiryu selects 37&quot; /&gt;&lt;/p&gt;&lt;img src=&quot;http://emptysqua.re/blog/analytics/http%3A//emptysqua.re/blog/photos-from-my-shuso-hossen//pixel.gif&quot; width=&quot;1px&quot; height=&quot;1px&quot; /&gt;</description>
	<pubDate>Sun, 26 Apr 2015 01:07:32 +0000</pubDate>
	<author>ajdavis@cs.oberlin.edu (A. Jesse Jiryu Davis)</author>
</item>
<item>
	<title>MongoDB Management Service: What Storage Engine Should my Backups Use?</title>
	<guid>http://blog.mms.mongodb.com/post/117267984515</guid>
	<link>http://blog.mms.mongodb.com/post/117267984515</link>
	<description>&lt;p&gt;&lt;strong&gt;What is WiredTiger? What is MMAPv1?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;WiredTiger is MongoDB’s new storage engine. It is available as an option for all 64-bit MongoDB 3.0 and higher builds. Among other features, it supports document-level locking and compression on disk. Check out &lt;a href=&quot;http://docs.mongodb.org/manual/core/storage/#storage-wiredtiger&quot;&gt;MongoDB’s docs on WiredTiger&lt;/a&gt; for more information.&lt;/p&gt;

&lt;p&gt;MMAPv1 is MongoDB’s traditional memory-mapped files storage engine. In MongoDB 3.0 we added collection-level locking while remaining compatible with the on-disk storage of MongoDB 2.6 and older.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why you should match&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Matching your currently running storage engine to your backed-up engine is critical to easy restores. If your restore files are in a different format than what you are used to running, you will have to make certain that you set your command line options correctly, and differently from what you are running elsewhere.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Where to change your setting&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;First, you have to be running the new MMS. MMS Classic customers cannot change their backup format, and only have MMAPv1 backups available to them. You are running the new MMS if the upper-left of your MMS window looks like this:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://41.media.tumblr.com/ebe571577931ebd5e5a0df4ea56e311a/tumblr_nnbp61gU3M1sdaytmo2_250.png&quot;&gt;&lt;img src=&quot;https://41.media.tumblr.com/ebe571577931ebd5e5a0df4ea56e311a/tumblr_nnbp61gU3M1sdaytmo2_250.png&quot; alt=&quot;new MMS&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Once you are in the new MMS, you can change your backup format by going to the “Backup” tab, click the “&amp;hellip;” for your replica set or cluster and choose “Edit Storage Engine”&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://41.media.tumblr.com/a86ccd3f3f0a1f4acae2e1ecf6bff399/tumblr_nnbp61gU3M1sdaytmo3_1280.png&quot;&gt;&lt;img src=&quot;https://41.media.tumblr.com/a86ccd3f3f0a1f4acae2e1ecf6bff399/tumblr_nnbp61gU3M1sdaytmo3_1280.png&quot; alt=&quot;... Edit Storage Engine&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Once there, you can choose MMAPv1 (“MongoDB Memory Mapped Files”) or WiredTiger:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://41.media.tumblr.com/180c411d78343c84e024ad521f724533/tumblr_nnbp61gU3M1sdaytmo4_1280.png&quot;&gt;&lt;img src=&quot;https://41.media.tumblr.com/180c411d78343c84e024ad521f724533/tumblr_nnbp61gU3M1sdaytmo4_1280.png&quot; alt=&quot;mmapv1 vs WT&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Once you make this change, an initial sync will be triggered. Choose the server you want to sync from and confirm. An initial sync is required so we can build your new backup. This will not change your existing snapshot formats, so if you request an older restore, it will still be in MMAPv1. You can tell if a snapshot is in WiredTiger format by looking at the “Mongod Version” column on your snapshots listing page (just click on a replica set name on your Backup tab). If the version has “(wiredTiger)” after it, the snapshot is in WiredTiger format. You can see I converted this replica set to WiredTiger:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://36.media.tumblr.com/91835dde5076f0f763892fd6ccd31720/tumblr_nnbp61gU3M1sdaytmo1_250.png&quot;&gt;&lt;img src=&quot;https://36.media.tumblr.com/91835dde5076f0f763892fd6ccd31720/tumblr_nnbp61gU3M1sdaytmo1_250.png&quot; alt=&quot;historical snapshots will not change&quot; /&gt;&lt;/a&gt;&lt;/p&gt;</description>
	<pubDate>Fri, 24 Apr 2015 18:27:54 +0000</pubDate>
</item>
<item>
	<title>A. Jesse Jiryu Davis: Samantha Ritter And Me At Open Source Bridge 2015</title>
	<guid>http://emptysqua.re/blog/speaking-at-open-source-bridge-2015/</guid>
	<link>http://emptysqua.re/blog/speaking-at-open-source-bridge-2015/</link>
	<description>&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2013/10/open-source-bridge-1.jpg&quot; alt=&quot;Open source bridge 1&quot; title=&quot;Open source bridge 1&quot; /&gt;&lt;/p&gt;
&lt;p&gt;I'm so excited to tell you, my colleague Samantha Ritter and I were accepted to speak in Portland, Oregon this June.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://opensourcebridge.org/proposals/1580&quot;&gt;Cat-herd's Crook: Enforcing Standards in 10 Programming Languages&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Samantha and I helped specify and test how MongoDB drivers behave in ten programming languages, and we persuaded dozens of open source programmers to implement these specifications the same. We created a test framework that verified compliance with YAML descriptions of how drivers should act, and we communicated updates to the specs by pushing changes to the YAML files.&lt;/p&gt;
&lt;p&gt;We want to show you how we herded all the cats in the same direction.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://opensourcebridge.org/proposals/1582&quot;&gt;How Do Python Coroutines Work?&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I'll explain asyncio's new coroutine implementation in depth, including the mystical &quot;yield from&quot; statement. You'll know better than any of your peers how this amazing new programming model works. Plus, in a magical and entertaining feat of daring, I plan to live-code an asynchronous coroutine implementation before your very eyes!&lt;/p&gt;&lt;img src=&quot;http://emptysqua.re/blog/analytics/http%3A//emptysqua.re/blog/speaking-at-open-source-bridge-2015//pixel.gif&quot; width=&quot;1px&quot; height=&quot;1px&quot; /&gt;</description>
	<pubDate>Fri, 24 Apr 2015 17:01:39 +0000</pubDate>
	<author>ajdavis@cs.oberlin.edu (A. Jesse Jiryu Davis)</author>
</item>
<item>
	<title>A. Jesse Jiryu Davis: Announcing PyMongo 3.0.1</title>
	<guid>http://emptysqua.re/blog/announcing-pymongo-3-0-1/</guid>
	<link>http://emptysqua.re/blog/announcing-pymongo-3-0-1/</link>
	<description>&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/04/pymongo-301-leaf.jpg&quot; alt=&quot;Leaf&quot; title=&quot;Leaf&quot; /&gt;&lt;/p&gt;
&lt;p&gt;It's my pleasure to announce the release of &lt;a href=&quot;https://pypi.python.org/pypi/pymongo/&quot;&gt;PyMongo 3.0.1&lt;/a&gt;, a bugfix release that addresses issues discovered since PyMongo 3.0 was released a couple weeks ago. The main bugs were related to queries and cursors in complex sharding setups, but there was an unintentional change to the return value of &lt;code&gt;save&lt;/code&gt;, GridFS file-deletion didn't work properly, passing a hint with a count didn't always work, and there were some obscure bugs and undocumented features.&lt;/p&gt;
&lt;p&gt;For the full list of bugs fixed in PyMongo 3.0.1, please &lt;a href=&quot;https://jira.mongodb.org/browse/PYTHON/fixforversion/15322&quot;&gt;see the release in Jira&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you are using PyMongo 3.0, please upgrade immediately.&lt;/p&gt;
&lt;p&gt;If you are on PyMongo 2.8, &lt;a href=&quot;http://api.mongodb.org/python/current/changelog.html&quot;&gt;read the changelog for major API changes in PyMongo 3&lt;/a&gt;, and test your application carefully with PyMongo 3 before deploying.&lt;/p&gt;&lt;img src=&quot;http://emptysqua.re/blog/analytics/http%3A//emptysqua.re/blog/announcing-pymongo-3-0-1//pixel.gif&quot; width=&quot;1px&quot; height=&quot;1px&quot; /&gt;</description>
	<pubDate>Tue, 21 Apr 2015 21:10:54 +0000</pubDate>
	<author>ajdavis@cs.oberlin.edu (A. Jesse Jiryu Davis)</author>
</item>
<item>
	<title>A. Jesse Jiryu Davis: Vote For Me At Open Source Bridge!</title>
	<guid>http://emptysqua.re/blog/vote-for-me-at-open-source-bridge/</guid>
	<link>http://emptysqua.re/blog/vote-for-me-at-open-source-bridge/</link>
	<description>&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/04/portland-old-town.jpg&quot; alt=&quot;Portland old town&quot; title=&quot;Portland old town&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; &lt;a href=&quot;http://emptysqua.re/blog/speaking-at-open-source-bridge-2015/&quot;&gt;Two of these talks were accepted&lt;/a&gt;, thanks for your support.&lt;/p&gt;
&lt;p&gt;This is a shameless plug. I'm not ashamed of how much I want to speak at Open Source Bridge this year&amp;mdash;it's my favorite conference with my favorite people and I desperately want to speak there again. So vote for me! Add stars and comments to the three talks I proposed.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://opensourcebridge.org/proposals/1580&quot;&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/04/add-star.png&quot; alt=&quot;Add star&quot; title=&quot;Add star&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://opensourcebridge.org/proposals/1580&quot;&gt;Cat-herd's Crook: Enforcing Standards in 10 Programming Languages&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I'm proposing this talk with my colleague and first-time speaker Samantha Ritter. We helped MongoDB specify and test our specifications for driver APIs and behaviors, and we persuaded dozens of open source programmers to implement these specifications the same. We want to show you how we herded all the cats in the same direction.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://opensourcebridge.org/proposals/1581&quot;&gt;Dodge Disasters and March to Triumph as a Mentor&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you're ambitious to advance in your career, or you care about your junior colleagues' advancement, then it is time for you to learn how to be a great mentor. Especially if you&amp;rsquo;re committed to diversity: mentorship is critical to the careers of women and minorities in tech. I have failed at mentoring, then succeeded. Learn from me and march to mentorship triumph.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://opensourcebridge.org/proposals/1582&quot;&gt;How Do Python Coroutines Work?&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I'll explain asyncio's new coroutine implementation in depth, including the mystical &quot;yield from&quot; statement. You'll know better than any of your peers how this amazing new programming model works. Plus, in a magical and entertaining feat of daring, I plan to live-code an asynchronous coroutine implementation before your very eyes!&lt;/p&gt;&lt;img src=&quot;http://emptysqua.re/blog/analytics/http%3A//emptysqua.re/blog/vote-for-me-at-open-source-bridge//pixel.gif&quot; width=&quot;1px&quot; height=&quot;1px&quot; /&gt;</description>
	<pubDate>Thu, 16 Apr 2015 22:28:23 +0000</pubDate>
	<author>ajdavis@cs.oberlin.edu (A. Jesse Jiryu Davis)</author>
</item>
<item>
	<title>A. Jesse Jiryu Davis: Caution: Critical Bug In PyMongo 3, &quot;could not find cursor in cache&quot;</title>
	<guid>http://emptysqua.re/blog/caution-critical-bug-in-pymongo-3-could-not-find-cursor-in-cache/</guid>
	<link>http://emptysqua.re/blog/caution-critical-bug-in-pymongo-3-could-not-find-cursor-in-cache/</link>
	<description>&lt;p&gt;If you use multiple mongos servers in a sharded cluster, be cautious upgrading to PyMongo 3. We've just discovered &lt;a href=&quot;https://jira.mongodb.org/browse/PYTHON-898&quot;&gt;a critical bug&lt;/a&gt; related to our new mongos load-balancing feature.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; &lt;a href=&quot;http://emptysqua.re/blog/announcing-pymongo-3-0-1/&quot;&gt;PyMongo 3.0.1 was released April 21, 2015&lt;/a&gt; with fixes for this and other bugs.&lt;/p&gt;
&lt;p&gt;If you create a MongoClient instance with PyMongo 3 and pass the addresses of several mongos servers, like so:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;client &lt;span&gt;=&lt;/span&gt; MongoClient(&lt;span&gt;'mongodb://mongos1,mongos2'&lt;/span&gt;)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;...then the client load-balances among the lowest-latency of them. &lt;a href=&quot;http://api.mongodb.org/python/current/examples/high_availability.html#mongos-load-balancing&quot;&gt;Read the load-balancing documentation for details&lt;/a&gt;. This works correctly except when retrieving more than 101 documents, or more than 4MB of data, from a cursor:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;collection &lt;span&gt;=&lt;/span&gt; client&lt;span&gt;.&lt;/span&gt;db&lt;span&gt;.&lt;/span&gt;collection
&lt;span&gt;for&lt;/span&gt; document &lt;span&gt;in&lt;/span&gt; collection&lt;span&gt;.&lt;/span&gt;find():
    &lt;span&gt;# ... do something with each document ...&lt;/span&gt;
    &lt;span&gt;pass&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;PyMongo wrongly tries to get subsequent batches of documents from random mongos servers, instead of streaming results from the same server it chose for the initial query. The symptom is an OperationFailure with a server error message, &quot;could not find cursor in cache&quot;:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;&lt;span&gt;Traceback (most recent call last):&lt;/span&gt;
  File &lt;span&gt;&amp;quot;/usr/local/lib/python2.7/dist-packages/pymongo/cursor.py&amp;quot;&lt;/span&gt;, line &lt;span&gt;968&lt;/span&gt;, in __next__
        &lt;span&gt;if&lt;/span&gt; &lt;span&gt;len&lt;/span&gt;(&lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;__data) &lt;span&gt;or&lt;/span&gt; &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;_refresh():
  File &lt;span&gt;&amp;quot;/usr/local/lib/python2.7/dist-packages/pymongo/cursor.py&amp;quot;&lt;/span&gt;, line &lt;span&gt;922&lt;/span&gt;, in _refresh
        &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;__id))
  File &lt;span&gt;&amp;quot;/usr/local/lib/python2.7/dist-packages/pymongo/cursor.py&amp;quot;&lt;/span&gt;, line &lt;span&gt;838&lt;/span&gt;, in __send_message
        codec_options&lt;span&gt;=&lt;/span&gt;&lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;__codec_options)
  File &lt;span&gt;&amp;quot;/usr/local/lib/python2.7/dist-packages/pymongo/helpers.py&amp;quot;&lt;/span&gt;, line &lt;span&gt;110&lt;/span&gt;, in _unpack_response
        cursor_id)
&lt;span&gt;pymongo.errors.CursorNotFound&lt;/span&gt;: cursor id '1025112076089406867' not valid at server
&lt;/pre&gt;&lt;/div&gt;&lt;img src=&quot;http://emptysqua.re/blog/analytics/http%3A//emptysqua.re/blog/caution-critical-bug-in-pymongo-3-could-not-find-cursor-in-cache//pixel.gif&quot; width=&quot;1px&quot; height=&quot;1px&quot; /&gt;</description>
	<pubDate>Wed, 15 Apr 2015 21:39:28 +0000</pubDate>
	<author>ajdavis@cs.oberlin.edu (A. Jesse Jiryu Davis)</author>
</item>
<item>
	<title>Tim Callaghan: How to Purchase [Benchmarking] Hardware on a Budget</title>
	<guid>tag:blogger.com,1999:blog-8043938871710850997.post-3874887240496864162</guid>
	<link>http://www.acmebenchmarking.com/2015/04/how-to-purchase-benchmarking-hardware.html</link>
	<description>&lt;span&gt;One of my goals at Acmebenchmarking is make sure I'm running on hardware that is representative of real-world infrastructure, while at the same time doing it as inexpensively as possible.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;To date I've been running on two custom built &quot;desktops&quot; (for lack of a better term). Both have an Intel Core i7 4790K processor (quad core plus hyperthreading, 4Ghz), 32GB RAM (dual channel), and a quality SSD. They are named acmebench01 and acmebench02.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;Alas, it is time to expand. &lt;i&gt;&lt;b&gt;MUST...PURCHASE...MORE...HARDWARE!&lt;/b&gt;&lt;/i&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;In order to maintain the inexpensive theme I tend to buy used hardware, my goal on this purchase was to achieve many more cores and greater memory bandwidth than my existing machines can provide. Keep in mind that used hardware is great for benchmarking (and likely development and QA environments) but you might want to avoid it for production. For years now I've been purchasing used hardware from &lt;a href=&quot;http://neweraserver.com/&quot;&gt;NES International&lt;/a&gt;, so I brought up their configurator and ordered the following.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;b&gt;&lt;span&gt;Dell Workstation T7500&lt;/span&gt;&lt;/b&gt;&lt;br /&gt;&lt;span&gt;- (2) Intel Xeon 5560 CPU&lt;/span&gt;&lt;br /&gt;&lt;span&gt;- 48GB RAM&lt;/span&gt;&lt;br /&gt;&lt;span&gt;- 250GB SATA&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;The &quot;server&quot; purchase included free shipping (at 70 pounds that is no small benefit) and landed on my doorstep for &lt;u&gt;&lt;b&gt;$819&lt;/b&gt;&lt;/u&gt;. I replaced the cheap SATA drive with a quality SSD and now have a pretty nice rig for testing workloads with high thread counts or the need for a significant bump in memory bandwidth. Again, I can't say enough about the folks at NES International (they even have an &lt;a href=&quot;http://www.ebay.com/usr/neweraserver&quot;&gt;eBay Store&lt;/a&gt;). Be careful, you might spend a lot of time looking over their stuff as they currently have over 20,000 used Dell servers for sale.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Say hello to acmebench03, (12) 2.8Ghz cores plus hyper-threading and 3 memory channels per socket. We shall see which databases and storage engines can actually harness such power!&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://2.bp.blogspot.com/-KQCerHPvyaE/VSwDMzyTahI/AAAAAAAACEA/rbdOqDMVHz0/s1600/20150410_01_acmebench03-cpuinfo.jpg&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://2.bp.blogspot.com/-KQCerHPvyaE/VSwDMzyTahI/AAAAAAAACEA/rbdOqDMVHz0/s1600/20150410_01_acmebench03-cpuinfo.jpg&quot; height=&quot;320&quot; width=&quot;314&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Here are a few more pictures for anyone into the hardware &quot;unboxing&quot; scene.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;div&gt;&lt;span&gt;Unopened.&lt;/span&gt;&lt;/div&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt; &lt;/span&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://1.bp.blogspot.com/-nPIR4prQDBs/VSwDSOqPYoI/AAAAAAAACEI/TE4lZnsCPYA/s1600/20150410_02_acmebench03-unopened.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://1.bp.blogspot.com/-nPIR4prQDBs/VSwDSOqPYoI/AAAAAAAACEI/TE4lZnsCPYA/s1600/20150410_02_acmebench03-unopened.png&quot; height=&quot;320&quot; width=&quot;236&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;div&gt;&lt;span&gt;Amazing packing job.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://2.bp.blogspot.com/-Whx46Hx001o/VSwDjXXB9NI/AAAAAAAACEQ/Ao1V96N7wEM/s1600/20150410_03_acmebench03-packing-job.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://2.bp.blogspot.com/-Whx46Hx001o/VSwDjXXB9NI/AAAAAAAACEQ/Ao1V96N7wEM/s1600/20150410_03_acmebench03-packing-job.png&quot; height=&quot;320&quot; width=&quot;236&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;Outside.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://1.bp.blogspot.com/-V4UQYVpsK8g/VSwDm23odCI/AAAAAAAACEY/Lo-UgVYePqY/s1600/20150410_04_acmebench03-outside.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://1.bp.blogspot.com/-V4UQYVpsK8g/VSwDm23odCI/AAAAAAAACEY/Lo-UgVYePqY/s1600/20150410_04_acmebench03-outside.png&quot; height=&quot;320&quot; width=&quot;236&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;Inside.&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://4.bp.blogspot.com/-8qZo2nv0kKY/VSwDpqs6WkI/AAAAAAAACEg/xE5imlfu9uE/s1600/20150410_05_acmebench03-inside.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://4.bp.blogspot.com/-8qZo2nv0kKY/VSwDpqs6WkI/AAAAAAAACEg/xE5imlfu9uE/s1600/20150410_05_acmebench03-inside.png&quot; height=&quot;320&quot; width=&quot;236&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt; &lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;</description>
	<pubDate>Mon, 13 Apr 2015 15:01:36 +0000</pubDate>
	<author>noreply@blogger.com (Tim Callaghan)</author>
</item>
<item>
	<title>A. Jesse Jiryu Davis: PyCon Video: &quot;Eventually Correct: Async Testing&quot;</title>
	<guid>http://emptysqua.re/blog/pycon-video-eventually-correct-async-testing/</guid>
	<link>http://emptysqua.re/blog/pycon-video-eventually-correct-async-testing/</link>
	<description>&lt;p&gt;Async frameworks like Tornado scramble our usual unittest strategies: how can you validate the outcome when you do not know when to expect it? Here's my PyCon 2015 talk about Tornado's testing module. You can also &lt;a href=&quot;http://emptysqua.re/blog/eventually-correct-async-testing-tornado/&quot;&gt;read my article on the topic&lt;/a&gt; or see &lt;a href=&quot;http://emptysqua.re/blog/screencast-of-eventually-correct-async-testing-with-tornado/&quot;&gt;a screencast which I closed-captioned&lt;/a&gt;.&lt;/p&gt;
&lt;img src=&quot;http://emptysqua.re/blog/analytics/http%3A//emptysqua.re/blog/pycon-video-eventually-correct-async-testing//pixel.gif&quot; width=&quot;1px&quot; height=&quot;1px&quot; /&gt;</description>
	<pubDate>Mon, 13 Apr 2015 03:23:22 +0000</pubDate>
	<author>ajdavis@cs.oberlin.edu (A. Jesse Jiryu Davis)</author>
</item>
<item>
	<title>A. Jesse Jiryu Davis: Screencast of &quot;Eventually Correct: Async Testing With Tornado&quot;</title>
	<guid>http://emptysqua.re/blog/screencast-of-eventually-correct-async-testing-with-tornado/</guid>
	<link>http://emptysqua.re/blog/screencast-of-eventually-correct-async-testing-with-tornado/</link>
	<description>&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/04/toad-vs-birdo.jpg&quot; alt=&quot;Toad vs Birdo&quot; title=&quot;Toad vs Birdo&quot; /&gt;&lt;/p&gt;
&lt;p&gt;In preparation for &lt;a href=&quot;https://us.pycon.org/2015/schedule/presentation/404/&quot;&gt;my PyCon talk today in Montr&amp;eacute;al&lt;/a&gt; I recorded and closed-captioned a screencast of it. This is a talk about testing asynchronous Python code with &lt;a href=&quot;http://www.tornadoweb.org/&quot;&gt;Tornado&lt;/a&gt;. I also wrote &lt;a href=&quot;http://emptysqua.re/blog/eventually-correct-async-testing-tornado/&quot;&gt;an article that covers this talk&lt;/a&gt;, plus additional information about testing with coroutines.&lt;/p&gt;
&lt;div&gt;

&lt;/div&gt;

&lt;p&gt;You can visit &lt;a href=&quot;http://emptysqua.re/blog/eventually-correct-links/&quot;&gt;my &quot;Eventually Correct&quot; landing page&lt;/a&gt; for further links about Tornado, testing, and asynchronous coroutines.&lt;/p&gt;&lt;img src=&quot;http://emptysqua.re/blog/analytics/http%3A//emptysqua.re/blog/screencast-of-eventually-correct-async-testing-with-tornado//pixel.gif&quot; width=&quot;1px&quot; height=&quot;1px&quot; /&gt;</description>
	<pubDate>Sun, 12 Apr 2015 02:35:57 +0000</pubDate>
	<author>ajdavis@cs.oberlin.edu (A. Jesse Jiryu Davis)</author>
</item>
<item>
	<title>A. Jesse Jiryu Davis: Eventually Correct: Async Testing With Tornado</title>
	<guid>http://emptysqua.re/blog/eventually-correct-async-testing-tornado/</guid>
	<link>http://emptysqua.re/blog/eventually-correct-async-testing-tornado/</link>
	<description>&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/04/toad-vs-birdo.jpg&quot; alt=&quot;Toad vs Birdo&quot; title=&quot;Toad vs Birdo&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Async frameworks like Tornado scramble our usual unittest strategies: how can you validate the outcome when you do not know when to expect it? Tornado ships with a &lt;code&gt;tornado.testing&lt;/code&gt; module that provides two solutions: the &lt;code&gt;wait&lt;/code&gt; / &lt;code&gt;stop&lt;/code&gt; pattern, and &lt;code&gt;gen_test&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&quot;toc&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://emptysqua.re/blog/feed/#wait-stop&quot;&gt;Wait / Stop&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://emptysqua.re/blog/feed/#gen_test&quot;&gt;gen_test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://emptysqua.re/blog/feed/#further-study&quot;&gt;Further Study&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h1 id=&quot;wait-stop&quot;&gt;Wait / Stop&lt;/h1&gt;
&lt;p&gt;To begin, let us say we are writing an async application with feature like Gmail's &lt;a href=&quot;https://support.google.com/mail/answer/1284885?hl=en&quot;&gt;undo send&lt;/a&gt;: when I click &quot;send&quot;, Gmail delays a few seconds before actually sending the email. It is a funny phenomenon, that during the seconds after clicking &quot;sending&quot; I experience a special clarity about my email. It was too angry, or I forgot an attachment, most often both. If I click the &quot;undo&quot; button in time, the email reverts to a draft and I can tone it down, add the attachment, and send it again.&lt;/p&gt;
&lt;p&gt;To write an application with this feature, we will need an asynchronous &quot;delay&quot; function, and we must test it. If we were testing a normal blocking delay function we could use &lt;code&gt;unittest.TestCase&lt;/code&gt; from the standard library:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;&lt;span&gt;import&lt;/span&gt; &lt;span&gt;time&lt;/span&gt;
&lt;span&gt;import&lt;/span&gt; &lt;span&gt;unittest&lt;/span&gt;

&lt;span&gt;from&lt;/span&gt; &lt;span&gt;my_application&lt;/span&gt; &lt;span&gt;import&lt;/span&gt; delay


&lt;span&gt;class&lt;/span&gt; &lt;span&gt;MyTestCase&lt;/span&gt;(unittest&lt;span&gt;.&lt;/span&gt;TestCase):
    &lt;span&gt;def&lt;/span&gt; &lt;span&gt;test_delay&lt;/span&gt;(&lt;span&gt;self&lt;/span&gt;):
        start &lt;span&gt;=&lt;/span&gt; time&lt;span&gt;.&lt;/span&gt;time()
        delay(&lt;span&gt;1&lt;/span&gt;)
        duration &lt;span&gt;=&lt;/span&gt; time&lt;span&gt;.&lt;/span&gt;time() &lt;span&gt;-&lt;/span&gt; start
        &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;assertAlmostEqual(duration, &lt;span&gt;1&lt;/span&gt;, places&lt;span&gt;=2&lt;/span&gt;)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;When we run this, it prints:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;Ran 1 test in 1.000s
OK
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And if we replace &lt;code&gt;delay(1)&lt;/code&gt; with &lt;code&gt;delay(2)&lt;/code&gt; it fails as expected:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;&lt;span&gt;======================================================&lt;/span&gt;=
FAIL: test_delay (delay0.MyTestCase)
&lt;span&gt;-------------------------------------------------------&lt;/span&gt;
Traceback (most recent call last):
File &amp;quot;delay0.py&amp;quot;, line 12, in test_delay
    self.assertAlmostEqual(duration, 1, places=2)
AssertionError: 2.000854969024658 != 1 within 2 places

&lt;span&gt;-------------------------------------------------------&lt;/span&gt;
Ran 1 test in 2.002s
FAILED (failures=1)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Great! What about testing a &lt;code&gt;delay_async(seconds, callback)&lt;/code&gt; function?&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;    &lt;span&gt;def&lt;/span&gt; &lt;span&gt;test_delay&lt;/span&gt;(&lt;span&gt;self&lt;/span&gt;):
        start &lt;span&gt;=&lt;/span&gt; time&lt;span&gt;.&lt;/span&gt;time()
&lt;span&gt;        delay_async(&lt;span&gt;1&lt;/span&gt;, callback&lt;span&gt;=&lt;/span&gt;)  &lt;span&gt;# What goes here?&lt;/span&gt;
&lt;/span&gt;        duration &lt;span&gt;=&lt;/span&gt; time&lt;span&gt;.&lt;/span&gt;time() &lt;span&gt;-&lt;/span&gt; start
        &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;assertAlmostEqual(duration, &lt;span&gt;1&lt;/span&gt;, places&lt;span&gt;=2&lt;/span&gt;)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;An asynchronous &quot;delay&quot; function can't block the caller, so it must take a callback and execute it once the delay is over. (In fact we are just reimplementing Tornado's &lt;a href=&quot;http://tornado.readthedocs.org/en/latest/ioloop.html#tornado.ioloop.IOLoop.call_later&quot;&gt;&lt;code&gt;call_later&lt;/code&gt;&lt;/a&gt;, but please pretend for pedagogy's sake this is a new function that we must test.) To test our &lt;code&gt;delay_async&lt;/code&gt;, we will try a series of testing techniques until we have effectively built Tornado's test framework from scratch&amp;mdash;you will see why we need special test tools for async code and how Tornado's tools work.&lt;/p&gt;
&lt;p&gt;So, we define a function &lt;code&gt;done&lt;/code&gt; to measure the delay, and pass it as the callback to &lt;code&gt;delay_async&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;    &lt;span&gt;def&lt;/span&gt; &lt;span&gt;test_delay&lt;/span&gt;(&lt;span&gt;self&lt;/span&gt;):
        start &lt;span&gt;=&lt;/span&gt; time&lt;span&gt;.&lt;/span&gt;time()

        &lt;span&gt;def&lt;/span&gt; &lt;span&gt;done&lt;/span&gt;():
            duration &lt;span&gt;=&lt;/span&gt; time&lt;span&gt;.&lt;/span&gt;time() &lt;span&gt;-&lt;/span&gt; start
            &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;assertAlmostEqual(duration, &lt;span&gt;1&lt;/span&gt;, places&lt;span&gt;=2&lt;/span&gt;)

        delay_async(&lt;span&gt;1&lt;/span&gt;, done)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If we run this:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;Ran 1 test in 0.001s
OK
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Success! ...right? But why does it only take a millisecond? And what happens if we delay by two seconds instead?&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;    &lt;span&gt;def&lt;/span&gt; &lt;span&gt;test_delay&lt;/span&gt;(&lt;span&gt;self&lt;/span&gt;):
        start &lt;span&gt;=&lt;/span&gt; time&lt;span&gt;.&lt;/span&gt;time()

        &lt;span&gt;def&lt;/span&gt; &lt;span&gt;done&lt;/span&gt;():
            duration &lt;span&gt;=&lt;/span&gt; time&lt;span&gt;.&lt;/span&gt;time() &lt;span&gt;-&lt;/span&gt; start
            &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;assertAlmostEqual(duration, &lt;span&gt;1&lt;/span&gt;, places&lt;span&gt;=2&lt;/span&gt;)

&lt;span&gt;        delay_async(&lt;span&gt;2&lt;/span&gt;, done)
&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Run it again:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;Ran 1 test in 0.001s
OK
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Something is very wrong here. The test appears to pass instantly, regardless of the argument to &lt;code&gt;delay_async&lt;/code&gt;, because we neither start the event loop nor wait for it to complete. We have to actually pause the test until the callback has executed:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;    &lt;span&gt;def&lt;/span&gt; &lt;span&gt;test_delay&lt;/span&gt;(&lt;span&gt;self&lt;/span&gt;):
        start &lt;span&gt;=&lt;/span&gt; time&lt;span&gt;.&lt;/span&gt;time()
&lt;span&gt;        io_loop &lt;span&gt;=&lt;/span&gt; IOLoop&lt;span&gt;.&lt;/span&gt;instance()
&lt;/span&gt;
        &lt;span&gt;def&lt;/span&gt; &lt;span&gt;done&lt;/span&gt;():
            duration &lt;span&gt;=&lt;/span&gt; time&lt;span&gt;.&lt;/span&gt;time() &lt;span&gt;-&lt;/span&gt; start
            &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;assertAlmostEqual(duration, &lt;span&gt;1&lt;/span&gt;, places&lt;span&gt;=2&lt;/span&gt;)
&lt;span&gt;            io_loop&lt;span&gt;.&lt;/span&gt;stop()
&lt;/span&gt;
        delay_async(&lt;span&gt;1&lt;/span&gt;, done)
&lt;span&gt;        io_loop&lt;span&gt;.&lt;/span&gt;start()
&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now if we run the test with a delay of one second:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;Ran 1 test in 1.002s
OK
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That looks better. And if we delay for two seconds?&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;ERROR:tornado.application:Exception in callback
&lt;span&gt;Traceback (most recent call last):&lt;/span&gt;
  File &lt;span&gt;&amp;quot;site-packages/tornado/ioloop.py&amp;quot;&lt;/span&gt;, line &lt;span&gt;568&lt;/span&gt;, in _run_callback
    ret &lt;span&gt;=&lt;/span&gt; callback()
  File &lt;span&gt;&amp;quot;site-packages/tornado/stack_context.py&amp;quot;&lt;/span&gt;, line &lt;span&gt;275&lt;/span&gt;, in null_wrapper
    &lt;span&gt;return&lt;/span&gt; fn(&lt;span&gt;*&lt;/span&gt;args, &lt;span&gt;**&lt;/span&gt;kwargs)
  File &lt;span&gt;&amp;quot;delay3.py&amp;quot;&lt;/span&gt;, line &lt;span&gt;16&lt;/span&gt;, in done
    &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;assertAlmostEqual(duration, &lt;span&gt;1&lt;/span&gt;, places&lt;span&gt;=2&lt;/span&gt;)
  File &lt;span&gt;&amp;quot;unittest/case.py&amp;quot;&lt;/span&gt;, line &lt;span&gt;845&lt;/span&gt;, in assertAlmostEqual
    &lt;span&gt;raise&lt;/span&gt; &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;failureException(msg)
&lt;span&gt;AssertionError&lt;/span&gt;: 2.001540184020996 != 1 within 2 places
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The test appears to fail, as expected, but there are a few problems. First, notice that it is not the unittest that prints the traceback: it is Tornado's application logger. We do not get the unittest's characteristic output. Second, the process is now hung and remains so until I type Control-C. Why?&lt;/p&gt;
&lt;p&gt;The bug is here:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;        &lt;span&gt;def&lt;/span&gt; &lt;span&gt;done&lt;/span&gt;():
            duration &lt;span&gt;=&lt;/span&gt; time&lt;span&gt;.&lt;/span&gt;time() &lt;span&gt;-&lt;/span&gt; start
&lt;span&gt;            &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;assertAlmostEqual(duration, &lt;span&gt;1&lt;/span&gt;, places&lt;span&gt;=2&lt;/span&gt;)
&lt;/span&gt;            io_loop&lt;span&gt;.&lt;/span&gt;stop()
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Since the failed assertion raises an exception, we never reach the call to &lt;code&gt;io_loop.stop()&lt;/code&gt;, so the loop continues running and the process does not exit. We need to register an exception handler. Exception handling with callbacks is convoluted; we have to use a &lt;a href=&quot;http://www.tornadoweb.org/en/branch2.3/stack_context.html&quot;&gt;stack context&lt;/a&gt; to install a handler with Tornado:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;&lt;span&gt;from&lt;/span&gt; &lt;span&gt;tornado.stack_context&lt;/span&gt; &lt;span&gt;import&lt;/span&gt; ExceptionStackContext

&lt;span&gt;class&lt;/span&gt; &lt;span&gt;MyTestCase&lt;/span&gt;(unittest&lt;span&gt;.&lt;/span&gt;TestCase):
    &lt;span&gt;def&lt;/span&gt; &lt;span&gt;test_delay&lt;/span&gt;(&lt;span&gt;self&lt;/span&gt;):
        start &lt;span&gt;=&lt;/span&gt; time&lt;span&gt;.&lt;/span&gt;time()
        io_loop &lt;span&gt;=&lt;/span&gt; IOLoop&lt;span&gt;.&lt;/span&gt;instance()

        &lt;span&gt;def&lt;/span&gt; &lt;span&gt;done&lt;/span&gt;():
            duration &lt;span&gt;=&lt;/span&gt; time&lt;span&gt;.&lt;/span&gt;time() &lt;span&gt;-&lt;/span&gt; start
            &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;assertAlmostEqual(duration, &lt;span&gt;1&lt;/span&gt;, places&lt;span&gt;=2&lt;/span&gt;)
            io_loop&lt;span&gt;.&lt;/span&gt;stop()

        &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;failure &lt;span&gt;=&lt;/span&gt; &lt;span&gt;None&lt;/span&gt;

        &lt;span&gt;def&lt;/span&gt; &lt;span&gt;handle_exception&lt;/span&gt;(typ, value, tb):
            io_loop&lt;span&gt;.&lt;/span&gt;stop()
            &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;failure &lt;span&gt;=&lt;/span&gt; value
            &lt;span&gt;return&lt;/span&gt; &lt;span&gt;True&lt;/span&gt;  &lt;span&gt;# Stop propagation.&lt;/span&gt;

        &lt;span&gt;with&lt;/span&gt; ExceptionStackContext(handle_exception):
            delay_async(&lt;span&gt;2&lt;/span&gt;, callback&lt;span&gt;=&lt;/span&gt;done)

        io_loop&lt;span&gt;.&lt;/span&gt;start()
        &lt;span&gt;if&lt;/span&gt; &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;failure:
            &lt;span&gt;raise&lt;/span&gt; &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;failure
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The loop can now be stopped two ways: if the test passes, then &lt;code&gt;done&lt;/code&gt; stops the loop as before. If it fails, &lt;code&gt;handle_exception&lt;/code&gt; stores the error and stops the loop. At the end, if an error was stored we re-raise it to make the test fail:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;=======================================================
FAIL: test_delay (delay4.MyTestCase)
-------------------------------------------------------
&lt;span&gt;Traceback (most recent call last):&lt;/span&gt;
  File &lt;span&gt;&amp;quot;delay4.py&amp;quot;&lt;/span&gt;, line &lt;span&gt;31&lt;/span&gt;, in test_delay
    &lt;span&gt;raise&lt;/span&gt; &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;failure
  File &lt;span&gt;&amp;quot;tornado/ioloop.py&amp;quot;&lt;/span&gt;, line &lt;span&gt;568&lt;/span&gt;, in _run_callback
    ret &lt;span&gt;=&lt;/span&gt; callback()
  File &lt;span&gt;&amp;quot;tornado/stack_context.py&amp;quot;&lt;/span&gt;, line &lt;span&gt;343&lt;/span&gt;, in wrapped
    raise_exc_info(exc)
  File &lt;span&gt;&amp;quot;&amp;lt;string&amp;gt;&amp;quot;&lt;/span&gt;, line &lt;span&gt;3&lt;/span&gt;, in raise_exc_info
  File &lt;span&gt;&amp;quot;tornado/stack_context.py&amp;quot;&lt;/span&gt;, line &lt;span&gt;314&lt;/span&gt;, in wrapped
    ret &lt;span&gt;=&lt;/span&gt; fn(&lt;span&gt;*&lt;/span&gt;args, &lt;span&gt;**&lt;/span&gt;kwargs)
  File &lt;span&gt;&amp;quot;delay4.py&amp;quot;&lt;/span&gt;, line &lt;span&gt;17&lt;/span&gt;, in done
    &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;assertAlmostEqual(duration, &lt;span&gt;1&lt;/span&gt;, places&lt;span&gt;=2&lt;/span&gt;)
&lt;span&gt;AssertionError&lt;/span&gt;: 2.0015950202941895 != 1 within 2 places
-------------------------------------------------------
Ran 1 test in 2.004s
FAILED (failures=1)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now the test ends promptly, whether it succeeds or fails, with unittest's typical output.&lt;/p&gt;
&lt;p&gt;This is a lot of tricky code to write just to test a trivial delay function, and it seems hard to get right each time. What does Tornado provide for us? Its &lt;a href=&quot;http://www.tornadoweb.org/en/branch2.3/testing.html&quot;&gt;AsyncTestCase&lt;/a&gt; gives us &lt;code&gt;start&lt;/code&gt; and &lt;code&gt;stop&lt;/code&gt; methods to control the event loop. If we then move the duration-testing outside the callback we radically simplify our test:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;&lt;span&gt;from&lt;/span&gt; &lt;span&gt;tornado&lt;/span&gt; &lt;span&gt;import&lt;/span&gt; testing

&lt;span&gt;class&lt;/span&gt; &lt;span&gt;MyTestCase&lt;/span&gt;(testing&lt;span&gt;.&lt;/span&gt;AsyncTestCase):
    &lt;span&gt;def&lt;/span&gt; &lt;span&gt;test_delay&lt;/span&gt;(&lt;span&gt;self&lt;/span&gt;):
        start &lt;span&gt;=&lt;/span&gt; time&lt;span&gt;.&lt;/span&gt;time()
&lt;span&gt;        delay_async(&lt;span&gt;1&lt;/span&gt;, callback&lt;span&gt;=&lt;/span&gt;&lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;stop)
&lt;/span&gt;&lt;span&gt;        &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;wait()
&lt;/span&gt;        duration &lt;span&gt;=&lt;/span&gt; time&lt;span&gt;.&lt;/span&gt;time() &lt;span&gt;-&lt;/span&gt; start
        &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;assertAlmostEqual(duration, &lt;span&gt;1&lt;/span&gt;, places&lt;span&gt;=2&lt;/span&gt;)
&lt;/pre&gt;&lt;/div&gt;


&lt;h1 id=&quot;gen_test&quot;&gt;&lt;code&gt;gen_test&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;But modern async code is not primarily written with callbacks: these days we use &lt;a href=&quot;http://tornado.readthedocs.org/en/latest/guide/coroutines.html&quot;&gt;coroutines&lt;/a&gt;. Let us begin a new example test, one that uses &lt;a href=&quot;https://motor.readthedocs.org/&quot;&gt;Motor, my asynchronous MongoDB driver for Tornado&lt;/a&gt;. Although Motor supports the old callback style, it encourages you to use coroutines and &quot;yield&quot; statements, so we can write some Motor code to demonstrate Tornado coroutines and unittesting.&lt;/p&gt;
&lt;p&gt;To begin, say we want to execute &lt;a href=&quot;http://motor.readthedocs.org/en/latest/api/motor_collection.html#motor.MotorCollection.find_one&quot;&gt;&lt;code&gt;find_one&lt;/code&gt;&lt;/a&gt; and test its return value:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;&lt;span&gt;from&lt;/span&gt; &lt;span&gt;motor&lt;/span&gt; &lt;span&gt;import&lt;/span&gt; MotorClient
&lt;span&gt;from&lt;/span&gt; &lt;span&gt;tornado&lt;/span&gt; &lt;span&gt;import&lt;/span&gt; testing

&lt;span&gt;class&lt;/span&gt; &lt;span&gt;MyTestCase&lt;/span&gt;(testing&lt;span&gt;.&lt;/span&gt;AsyncTestCase):
    &lt;span&gt;def&lt;/span&gt; &lt;span&gt;setUp&lt;/span&gt;(&lt;span&gt;self&lt;/span&gt;):
        &lt;span&gt;super&lt;/span&gt;()&lt;span&gt;.&lt;/span&gt;setUp()
        &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;client &lt;span&gt;=&lt;/span&gt; MotorClient()

    &lt;span&gt;def&lt;/span&gt; &lt;span&gt;test_find_one&lt;/span&gt;(&lt;span&gt;self&lt;/span&gt;):
        collection &lt;span&gt;=&lt;/span&gt; &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;client&lt;span&gt;.&lt;/span&gt;test&lt;span&gt;.&lt;/span&gt;collection
&lt;span&gt;        document &lt;span&gt;=&lt;/span&gt; &lt;span&gt;yield&lt;/span&gt; collection&lt;span&gt;.&lt;/span&gt;find_one({&lt;span&gt;'_id'&lt;/span&gt;: &lt;span&gt;1&lt;/span&gt;})
&lt;/span&gt;&lt;span&gt;        &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;assertEqual({&lt;span&gt;'_id'&lt;/span&gt;: &lt;span&gt;1&lt;/span&gt;, &lt;span&gt;'key'&lt;/span&gt;: &lt;span&gt;'value'&lt;/span&gt;}, document)
&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Notice the &quot;yield&quot; statement: whenever you call a Motor method that does I/O, you must use &quot;yield&quot; to pause the current function and wait for the returned Future object to be resolved to a value. Including a yield statement makes this function a generator. But now there is a problem:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;TypeError&lt;span&gt;:&lt;/span&gt; Generator test methods should be decorated &lt;span&gt;with&lt;/span&gt; tornado&lt;span&gt;.&lt;/span&gt;&lt;span&gt;testing&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;span&gt;gen_test&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Tornado smartly warns us that our test method is merely a generator&amp;mdash;we must decorate it with &lt;a href=&quot;http://tornado.readthedocs.org/en/latest/testing.html#tornado.testing.gen_test&quot;&gt;gen_test&lt;/a&gt;. Otherwise the test method simply stops at the first yield, and never reaches the assert. It needs a coroutine &lt;em&gt;driver&lt;/em&gt; to run it to completion:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;span&gt;from&lt;/span&gt; &lt;span&gt;tornado.testing&lt;/span&gt; &lt;span&gt;import&lt;/span&gt; gen_test
&lt;/span&gt;
&lt;span&gt;class&lt;/span&gt; &lt;span&gt;MyTestCase&lt;/span&gt;(testing&lt;span&gt;.&lt;/span&gt;AsyncTestCase):
    &lt;span&gt;# ... same setup ...&lt;/span&gt;
&lt;span&gt;    &lt;span&gt;@gen_test&lt;/span&gt;
&lt;/span&gt;    &lt;span&gt;def&lt;/span&gt; &lt;span&gt;test_find_one&lt;/span&gt;(&lt;span&gt;self&lt;/span&gt;):
        collection &lt;span&gt;=&lt;/span&gt; &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;client&lt;span&gt;.&lt;/span&gt;test&lt;span&gt;.&lt;/span&gt;collection
        document &lt;span&gt;=&lt;/span&gt; &lt;span&gt;yield&lt;/span&gt; collection&lt;span&gt;.&lt;/span&gt;find_one({&lt;span&gt;'_id'&lt;/span&gt;: &lt;span&gt;1&lt;/span&gt;})
        &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;assertEqual({&lt;span&gt;'_id'&lt;/span&gt;: &lt;span&gt;1&lt;/span&gt;, &lt;span&gt;'key'&lt;/span&gt;: &lt;span&gt;'value'&lt;/span&gt;}, document)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;But now when I run the test, it fails:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;AssertionError: {&lt;span&gt;'key'&lt;/span&gt;: &lt;span&gt;'value'&lt;/span&gt;, &lt;span&gt;'_id'&lt;/span&gt;: 1} != None
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We need to insert some data in &lt;code&gt;setUp&lt;/code&gt; so that &lt;code&gt;find_one&lt;/code&gt; can find it! Since Motor is asynchronous, we cannot call its &lt;code&gt;insert&lt;/code&gt; method directly from &lt;code&gt;setUp&lt;/code&gt;, we must run the insertion in a coroutine as well:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;span&gt;from&lt;/span&gt; &lt;span&gt;tornado&lt;/span&gt; &lt;span&gt;import&lt;/span&gt; gen, testing
&lt;/span&gt;
&lt;span&gt;class&lt;/span&gt; &lt;span&gt;MyTestCase&lt;/span&gt;(testing&lt;span&gt;.&lt;/span&gt;AsyncTestCase):
    &lt;span&gt;def&lt;/span&gt; &lt;span&gt;setUp&lt;/span&gt;(&lt;span&gt;self&lt;/span&gt;):
        &lt;span&gt;super&lt;/span&gt;()&lt;span&gt;.&lt;/span&gt;setUp()
        &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;client &lt;span&gt;=&lt;/span&gt; MotorClient()
&lt;span&gt;        &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;setup_coro()
&lt;/span&gt;
&lt;span&gt;    &lt;span&gt;@gen.coroutine&lt;/span&gt;
&lt;/span&gt;    &lt;span&gt;def&lt;/span&gt; &lt;span&gt;setup_coro&lt;/span&gt;(&lt;span&gt;self&lt;/span&gt;):
        collection &lt;span&gt;=&lt;/span&gt; &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;client&lt;span&gt;.&lt;/span&gt;test&lt;span&gt;.&lt;/span&gt;collection

        &lt;span&gt;# Clean up from prior runs:&lt;/span&gt;
        &lt;span&gt;yield&lt;/span&gt; collection&lt;span&gt;.&lt;/span&gt;remove()

        &lt;span&gt;yield&lt;/span&gt; collection&lt;span&gt;.&lt;/span&gt;insert({&lt;span&gt;'_id'&lt;/span&gt;: &lt;span&gt;0&lt;/span&gt;})
        &lt;span&gt;yield&lt;/span&gt; collection&lt;span&gt;.&lt;/span&gt;insert({&lt;span&gt;'_id'&lt;/span&gt;: &lt;span&gt;1&lt;/span&gt;, &lt;span&gt;'key'&lt;/span&gt;: &lt;span&gt;'value'&lt;/span&gt;})
        &lt;span&gt;yield&lt;/span&gt; collection&lt;span&gt;.&lt;/span&gt;insert({&lt;span&gt;'_id'&lt;/span&gt;: &lt;span&gt;2&lt;/span&gt;})
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now when I run the test:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;AssertionError: {&lt;span&gt;'key'&lt;/span&gt;: &lt;span&gt;'value'&lt;/span&gt;, &lt;span&gt;'_id'&lt;/span&gt;: 1} != None
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It still fails! When I check in the mongo shell whether my data was inserted, only two of the three expected documents are there:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;&lt;span&gt;&amp;gt;&lt;/span&gt; db.collection.&lt;span&gt;find&lt;/span&gt;()
{ &amp;quot;_id&amp;quot; : 0 }
{ &amp;quot;_id&amp;quot; : 1, &amp;quot;key&amp;quot; : &amp;quot;value&amp;quot; }
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Why is it incomplete? Furthermore, since the document I actually query &lt;em&gt;is&lt;/em&gt; there, why did the test fail?&lt;/p&gt;
&lt;p&gt;When I called &lt;code&gt;self.setup_coro()&lt;/code&gt; in &lt;code&gt;setUp&lt;/code&gt;, I launched it as a &lt;em&gt;concurrent&lt;/em&gt; coroutine. It began running, but I did not wait for it to complete before beginning the test, so the test may reach its &lt;code&gt;find_one&lt;/code&gt; statement before the second document is inserted. Furthermore, &lt;code&gt;test_find_one&lt;/code&gt; can fail quickly enough that &lt;code&gt;setup_coro&lt;/code&gt; does not insert its third document before the whole test suite finishes, stopping the event loop and preventing the final document from ever being inserted.&lt;/p&gt;
&lt;p&gt;Clearly I must wait for the setup coroutine to complete before beginning the test. Tornado's &lt;code&gt;run_sync&lt;/code&gt; method is designed for uses like this:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;&lt;span&gt;class&lt;/span&gt; &lt;span&gt;MyTestCase&lt;/span&gt;(testing&lt;span&gt;.&lt;/span&gt;AsyncTestCase):
    &lt;span&gt;def&lt;/span&gt; &lt;span&gt;setUp&lt;/span&gt;(&lt;span&gt;self&lt;/span&gt;):
        &lt;span&gt;super&lt;/span&gt;()&lt;span&gt;.&lt;/span&gt;setUp()
        &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;client &lt;span&gt;=&lt;/span&gt; MotorClient()
&lt;span&gt;        &lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;io_loop&lt;span&gt;.&lt;/span&gt;run_sync(&lt;span&gt;self&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;setup_coro)
&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With my setup coroutine correctly executed, now &lt;code&gt;test_find_one&lt;/code&gt; passes.&lt;/p&gt;
&lt;h1 id=&quot;further-study&quot;&gt;Further Study&lt;/h1&gt;
&lt;p&gt;Now we have seen two techniques that make async testing with Tornado as convenient and reliable as standard unittests. To learn more, see my page of &lt;a href=&quot;http://emptysqua.re/blog/eventually-correct-links/&quot;&gt;links related to this article&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Plus, stay tuned for the next book in the &lt;a href=&quot;http://aosabook.org/&quot;&gt;Architecture of Open Source Applications&lt;/a&gt; series. It will be called &quot;500 Lines or Less&quot;, and my chapter is devoted to the implementation of coroutines in asyncio and Python 3.&lt;/p&gt;&lt;img src=&quot;http://emptysqua.re/blog/analytics/http%3A//emptysqua.re/blog/eventually-correct-async-testing-tornado//pixel.gif&quot; width=&quot;1px&quot; height=&quot;1px&quot; /&gt;</description>
	<pubDate>Sat, 11 Apr 2015 02:03:32 +0000</pubDate>
	<author>ajdavis@cs.oberlin.edu (A. Jesse Jiryu Davis)</author>
</item>
<item>
	<title>MongoDB Management Service: Upgrade your MongoDB Deployment from 2.6.9 to 3.0.1 with Automation</title>
	<guid>http://blog.mms.mongodb.com/post/115883288050</guid>
	<link>http://blog.mms.mongodb.com/post/115883288050</link>
	<description>&lt;p&gt;Using the MongoDB Management Service, you can now upgrade your MongoDB deployment safely and quickly from 2.6.9 to 3.0.1 via the user interface. To upgrade your own deployment, follow the tutorial below. As always, if you should run into any trouble, you can reach out to MongoDB via the &lt;a href=&quot;https://mms.mongodb.com/support&quot;&gt;MMS Support page&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 0. Considerations Before Upgrading&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;MongoDB 3.0.1 introduces significant changes to the authorization model. If you are using authSchemaVersion 1 (the schema used in MongoDB 2.4 and earlier), you must upgrade to authSchemaVersion 3 before proceeding to upgrade to 3.0.1. If using Automation, you can do the schema upgrade by clicking the wrench for your deployment. If you haven’t imported for Automation yet, &lt;a href=&quot;http://blog.mms.mongodb.com/post/111860709590/introduction-to-import-existing-deployment-for&quot;&gt;here are the manual instructions&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You cannot upgrade directly from 2.4 to 3.0, you must upgrade to 2.6 first.&lt;/li&gt;
&lt;li&gt;Driver compatibility:  As long as you leave the authSchemaVersion on 3, and are still on MMAPv1, your old driver should continue to work.  However, if you upgrade the authSchemaVersion to 5, or upgrade to wiredTiger, a &lt;a href=&quot;http://docs.mongodb.org/manual/release-notes/3.0-compatibility/#compatibility-driver-versions&quot;&gt;MongoDB 3.0 compatible driver&lt;/a&gt; will be necessary.
Many third-party tools will not work after you upgrade to authSchemaVersion 5. Check for MongoDB 3.0 compatible versions of your utilities.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;This is not a guide to upgrading to WiredTiger, stay tuned for that tutorial.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1. Review Your Current Deployment&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;On the deployment page, you should see that your current replica set has recent pings indicated by the green light to the right of each member and have no startup warnings. I’ve set up a sharded cluster here, but the process is the same.
&lt;a href=&quot;https://40.media.tumblr.com/b715902b9d01e26f4d7815f217e0324c/tumblr_nmib01FdkH1sdaytmo4_1280.png&quot;&gt;&lt;img src=&quot;https://40.media.tumblr.com/b715902b9d01e26f4d7815f217e0324c/tumblr_nmib01FdkH1sdaytmo4_1280.png&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;On the server tile page, each server where a node is located should have an automation agent already up and running. 
&lt;a href=&quot;https://36.media.tumblr.com/ed30c66cc275a5447ec5bf1d8b88a190/tumblr_nmib01FdkH1sdaytmo1_1280.png&quot;&gt;&lt;img src=&quot;https://36.media.tumblr.com/ed30c66cc275a5447ec5bf1d8b88a190/tumblr_nmib01FdkH1sdaytmo1_1280.png&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If each server does not have an automation agent on it, you will need to download the automation agent from the Administration -&amp;gt; Agents page, install it, and make sure that you have imported your deployment for automation.  If one of the hosts is unreachable, you will need to resolve this issue as well before being able to upgrade the replica set.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 2. Select the deployment and modify.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Click the wrench for your deployment to bring up the modification panel.
&lt;a href=&quot;https://36.media.tumblr.com/4c194b83aa03fb42e969a3a9175b9307/tumblr_nmib01FdkH1sdaytmo2_1280.png&quot;&gt;&lt;img src=&quot;https://36.media.tumblr.com/4c194b83aa03fb42e969a3a9175b9307/tumblr_nmib01FdkH1sdaytmo2_1280.png&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://41.media.tumblr.com/1154c645d355dc13212dcc558c29aa51/tumblr_nmib01FdkH1sdaytmo5_1280.png&quot;&gt;&lt;img src=&quot;https://41.media.tumblr.com/1154c645d355dc13212dcc558c29aa51/tumblr_nmib01FdkH1sdaytmo5_1280.png&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Click the version drop-down and select 3.0.1 and then click “Apply”. Review and deploy your changes. Now the Automation Agents will download the proper MongoDB version and do the upgrade in the proper order &lt;em&gt;for&lt;/em&gt; you.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 3: Enjoy your updated deployment&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://41.media.tumblr.com/f78b07187c5863efd7f4e4c8dc3c7878/tumblr_nmib01FdkH1sdaytmo3_1280.png&quot;&gt;&lt;img src=&quot;https://41.media.tumblr.com/f78b07187c5863efd7f4e4c8dc3c7878/tumblr_nmib01FdkH1sdaytmo3_1280.png&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you are ready with 3.0-Compatible drivers and utilities, you can now change the AuthSchema to 5 via the modifications sidebar from Step 2, above.&lt;/p&gt;</description>
	<pubDate>Wed, 08 Apr 2015 21:42:48 +0000</pubDate>
</item>
<item>
	<title>A. Jesse Jiryu Davis: Announcing PyMongo 3</title>
	<guid>http://emptysqua.re/blog/announcing-pymongo-3/</guid>
	<link>http://emptysqua.re/blog/announcing-pymongo-3/</link>
	<description>&lt;p&gt;&lt;img src=&quot;http://emptysqua.re/blog/media/2015/04/leaf.jpg&quot; alt=&quot;Leaf&quot; title=&quot;Leaf&quot; /&gt;&lt;/p&gt;
&lt;p&gt;PyMongo 3.0 is a partial rewrite of the Python driver for MongoDB. More than six years after the first release of the driver, this is the biggest release in PyMongo's history. Bernie Hackett, Luke Lovett, Anna Herlihy, and I are proud of its many improvements and eager for you to try it out. I will shoehorn the major improvements into four shoes: conformance, responsiveness, robustness, and modernity.&lt;/p&gt;
&lt;p&gt;(This article will be cross-posted on &lt;a href=&quot;http://www.mongodb.com/blog/&quot;&gt;the MongoDB Blog&lt;/a&gt;.)&lt;/p&gt;
&lt;hr /&gt;
&lt;div class=&quot;toc&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://emptysqua.re/blog/feed/#conformance&quot;&gt;Conformance&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://emptysqua.re/blog/feed/#crud-api&quot;&gt;CRUD API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://emptysqua.re/blog/feed/#one-client-class&quot;&gt;One Client Class&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://emptysqua.re/blog/feed/#non-conforming-features&quot;&gt;Non-Conforming Features&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://emptysqua.re/blog/feed/#responsiveness&quot;&gt;Responsiveness&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://emptysqua.re/blog/feed/#replica-set-discovery-and-monitoring&quot;&gt;Replica Set Discovery And Monitoring&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://emptysqua.re/blog/feed/#mongos-load-balancing&quot;&gt;Mongos Load-Balancing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://emptysqua.re/blog/feed/#throughput&quot;&gt;Throughput&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://emptysqua.re/blog/feed/#robustness&quot;&gt;Robustness&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://emptysqua.re/blog/feed/#disconnected-startup&quot;&gt;Disconnected Startup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://emptysqua.re/blog/feed/#one-monitor-thread-per-server&quot;&gt;One Monitor Thread Per Server&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://emptysqua.re/blog/feed/#thread-safety&quot;&gt;Thread Safety&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://emptysqua.re/blog/feed/#modernity&quot;&gt;Modernity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://emptysqua.re/blog/feed/#motor&quot;&gt;Motor&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h1 id=&quot;conformance&quot;&gt;Conformance&lt;/h1&gt;
&lt;p&gt;The motivation for PyMongo's overhaul is to supersede or remove its many idiosyncratic APIs. We want you to have a clean interface that is easy to learn and closely matches the interfaces of our other drivers.&lt;/p&gt;
&lt;h2 id=&quot;crud-api&quot;&gt;CRUD API&lt;/h2&gt;
&lt;p&gt;Mainly, &quot;conformance&quot; means we have implemented the same interface for create, read, update, and delete operations as the other drivers have, as standardized in Craig Wilson's &lt;a href=&quot;https://github.com/mongodb/specifications/blob/master/source/crud/crud.rst&quot;&gt;CRUD API Spec&lt;/a&gt;. The familiar old methods work the same in PyMongo 3, but they are deprecated:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;save&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;insert&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;update&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;remove&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;find_and_modify&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These methods were vaguely named. For example, &lt;code&gt;update&lt;/code&gt; updates or replaces some or all matching documents depending on its arguments. The arguments to &lt;code&gt;save&lt;/code&gt; and &lt;code&gt;remove&lt;/code&gt; are likewise finicky, and the many options for &lt;code&gt;find_and_modify&lt;/code&gt; are intimidating. Other MongoDB drivers do not have exactly the same arguments in the same order for all these methods. If you or other developers on your team are using a driver from a different language, it makes life a lot easier to have consistent interfaces.&lt;/p&gt;
&lt;p&gt;The new CRUD API names its methods like &lt;code&gt;update_one&lt;/code&gt;, &lt;code&gt;insert_many&lt;/code&gt;, &lt;code&gt;find_one_and_delete&lt;/code&gt;: they say what they mean and mean what they say. Even better, all MongoDB drivers have exactly the same methods with the same arguments. &lt;a href=&quot;https://github.com/mongodb/specifications/blob/master/source/crud/crud.rst&quot;&gt;See the spec&lt;/a&gt; for details.&lt;/p&gt;
&lt;h2 id=&quot;one-client-class&quot;&gt;One Client Class&lt;/h2&gt;
&lt;p&gt;In the past we had three client classes: Connection for any one server, and ReplicaSetConnection to connect to a replica set. We also had a MasterSlaveConnection that could distribute reads to slaves in a master-slave set. In November 2012 we created new classes, MongoClient and MongoReplicaSetClient, with better default settings, so now PyMongo had five clients! Even more confusingly, MongoClient could connect to a set of mongos servers and do hot failover.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://emptysqua.re/blog/good-idea-at-the-time-pymongo-mongoreplicasetclient/&quot;&gt;As I wrote earlier&lt;/a&gt;, the fine distinctions between the client classes baffled users. And the set of clients we provided did not conform with other drivers. But since PyMongo is among the most-used of all Python libraries we waited long, and thought hard, before making major changes.&lt;/p&gt;
&lt;p&gt;The day has come. MongoClient is now the one and only client class for a single server, a set of mongoses, or a replica set. It includes the functionality that had been split into MongoReplicaSetClient: it can connect to a replica set, discover all its members, and monitor the set for stepdowns, elections, and reconfigs. MongoClient now also supports the full &lt;a href=&quot;http://api.mongodb.org/python/current/examples/high_availability.html#secondary-reads&quot;&gt;ReadPreference API&lt;/a&gt;. MongoReplicaSetClient lives on for a time, for compatibility's sake, but new code should use MongoClient exclusively. The obsolete Connection, ReplicaSetConnection, and MasterSlaveConnection are removed.&lt;/p&gt;
&lt;p&gt;The options you pass to MongoClient in the URI now completely control the client's behavior:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;&lt;span&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span&gt;# Connect to one standalone, mongos, or replica set member.&lt;/span&gt;
&lt;span&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; client &lt;span&gt;=&lt;/span&gt; MongoClient(&lt;span&gt;'mongodb://server'&lt;/span&gt;)
&lt;span&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;span&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span&gt;# Connect to a replica set.&lt;/span&gt;
&lt;span&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; client &lt;span&gt;=&lt;/span&gt; MongoClient(
&lt;span&gt;...&lt;/span&gt;     &lt;span&gt;'mongodb://member1,member2/?replicaSet=my_rs'&lt;/span&gt;)
&lt;span&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;span&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span&gt;# Load-balance among mongoses.&lt;/span&gt;
&lt;span&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; client &lt;span&gt;=&lt;/span&gt; MongoClient(&lt;span&gt;'mongodb://mongos1,mongos2'&lt;/span&gt;)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is exciting because PyMongo applications are now so easy to deploy: your code simply loads a MongoDB URI from an environment variable or config file and passes it to a MongoClient. Code and configuration are cleanly separated. You can move smoothly from your laptop to a test server to the cloud, simply by changing the URI.&lt;/p&gt;
&lt;h2 id=&quot;non-conforming-features&quot;&gt;Non-Conforming Features&lt;/h2&gt;
&lt;p&gt;PyMongo 2 had some quirky features it did not share with other drivers. For one, we had a &lt;code&gt;copy_database&lt;/code&gt; method that only one other driver had, and which almost no one used. &lt;a href=&quot;http://emptysqua.re/blog/good-idea-at-the-time-pymongo-copy-database/&quot;&gt;It was hard to maintain&lt;/a&gt; and we believe you want us to focus on the features you use, so we removed it.&lt;/p&gt;
&lt;p&gt;A more pernicious misfeature was the &lt;code&gt;start_request&lt;/code&gt; method. It bound a thread to a socket, which hurt performance without actually guaranteeing monotonic write consistency. It was overwhelmingly misused, too: new PyMongo users naturally called &lt;code&gt;start_request&lt;/code&gt; before starting a request, but in truth the feature had nothing to do with its name. For the history and details, including some entertaining (in retrospect) tales of Python threadlocal bugs, &lt;a href=&quot;http://emptysqua.re/blog/good-idea-at-the-time-pymongo-start-request/&quot;&gt;see my article on the removal of start_request&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Finally, the Python team rewrote our distributed-systems internals to conform to the new standards we have specified for all our drivers. But if you are a Python programmer you may care only a little that the new code conforms to a spec; it is more interesting to you that the new code is responsive and robust.&lt;/p&gt;
&lt;h1 id=&quot;responsiveness&quot;&gt;Responsiveness&lt;/h1&gt;
&lt;p&gt;PyMongo 3's MongoClient can connect to a single server, a replica set, or a set of mongoses. It finds servers and reacts to changing conditions according to &lt;a href=&quot;http://www.mongodb.com/blog/post/server-discovery-and-monitoring-next-generation-mongodb-drivers&quot;&gt;the Server Discovery And Monitoring spec&lt;/a&gt;, and it chooses which server to use for each operation according to &lt;a href=&quot;http://www.mongodb.com/blog/post/server-selection-next-generation-mongodb-drivers&quot;&gt;the Server Selection Spec&lt;/a&gt;. David Golden and I explained these specs in general in the linked articles, but I can describe PyMongo&amp;rsquo;s implementation here.&lt;/p&gt;
&lt;h2 id=&quot;replica-set-discovery-and-monitoring&quot;&gt;Replica Set Discovery And Monitoring&lt;/h2&gt;
&lt;p&gt;In PyMongo 2, MongoReplicaSetClient used a single background thread to monitor all replica set members in series. So a slow or unresponsive member could block the thread for some time before the thread moved on to discover information about the other members, like their network latencies or which member is primary. If your application was waiting for that information&amp;mdash;say, to write to the new primary after an election&amp;mdash;these delays caused unneeded seconds of downtime.&lt;/p&gt;
&lt;p&gt;When PyMongo 3's new MongoClient connects to a replica set it starts one thread per mongod server. The threads fan out to connect to all members of the set in parallel, and they start additional threads as they discover more members. As soon as any thread discovers the primary, your application is unblocked, even while the monitor threads collect more information about the set. This new design improves PyMongo's response time tremendously. If some members are slow or down, or you have many members in your set, PyMongo's discovery is still just as fast.&lt;/p&gt;
&lt;p&gt;I explained the new design in &lt;a href=&quot;http://www.mongodb.com/blog/post/server-discovery-and-monitoring-next-generation-mongodb-drivers&quot;&gt;Server Discovery And Monitoring In Next Generation MongoDB Drivers&lt;/a&gt;, and I'll actually demonstrate it in my &lt;a href=&quot;http://mongodbworld.com/&quot;&gt;MongoDB World&lt;/a&gt; talk, Drivers And High Availability: Deep Dive.&lt;/p&gt;
&lt;h2 id=&quot;mongos-load-balancing&quot;&gt;Mongos Load-Balancing&lt;/h2&gt;
&lt;p&gt;Our multi-mongos behavior is improved, too. A MongoClient can connect to a set of mongos servers:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;&lt;span&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span&gt;# Two mongoses.&lt;/span&gt;
&lt;span&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; client &lt;span&gt;=&lt;/span&gt; MongoClient(&lt;span&gt;'mongodb://mongos1,mongos2'&lt;/span&gt;)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The behavior in PyMongo 2 was &quot;high availability&quot;: the client connected to the lowest-latency mongos in the list, and used it until a network error prompted it to re-evaluate their latencies and reconnect to one of them. If the driver chose unwisely at first, it stayed pinned to a higher-latency mongos for some time. In PyMongo 3, the background threads monitor the client's network latency to all the mongoses continuously, and the client distributes operations evenly among those with the lowest latency. See &lt;a href=&quot;http://api.mongodb.org/python/current/examples/high_availability.html#mongos-load-balancing&quot;&gt;mongos Load Balancing&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h2 id=&quot;throughput&quot;&gt;Throughput&lt;/h2&gt;
&lt;p&gt;Besides PyMongo's improved responsiveness to changing conditions in your deployment, its throughput is better too. We have written &lt;a href=&quot;https://jira.mongodb.org/browse/PYTHON-346&quot;&gt;a faster and more memory efficient pure python BSON module&lt;/a&gt;, which is particularly important for PyPy, and made substantial optimizations in our C extensions.&lt;/p&gt;
&lt;h1 id=&quot;robustness&quot;&gt;Robustness&lt;/h1&gt;
&lt;h2 id=&quot;disconnected-startup&quot;&gt;Disconnected Startup&lt;/h2&gt;
&lt;p&gt;The first change you may notice is, MongoClient's constructor no longer blocks while connecting. It does not raise ConnectionFailure if it cannot connect:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;&lt;span&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; client &lt;span&gt;=&lt;/span&gt; MongoClient(&lt;span&gt;'mongodb://no-host.com'&lt;/span&gt;)
&lt;span&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; client
MongoClient(&lt;span&gt;'no-host.com'&lt;/span&gt;, &lt;span&gt;27017&lt;/span&gt;)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The constructor returns immediately and launches the connection process on background threads. Of course, foreground operations might time out:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;&lt;span&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; client&lt;span&gt;.&lt;/span&gt;db&lt;span&gt;.&lt;/span&gt;collection&lt;span&gt;.&lt;/span&gt;find_one()
AutoReconnect: No servers found yet
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Meanwhile, the client's background threads keep trying to reach the server. This is a big win for web applications that use PyMongo&amp;mdash;in a crisis, your app servers might be restarted while your MongoDB servers are unreachable. Your applications should not throw an exception at startup, when they construct the client object. In PyMongo 3 the client can now start up disconnected; it tries to reach your servers until it succeeds.&lt;/p&gt;
&lt;p&gt;On the other hand if you wrote code like this to check if mongod is up:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;&lt;span&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span&gt;try&lt;/span&gt;:
&lt;span&gt;...&lt;/span&gt;     MongoClient()
&lt;span&gt;...&lt;/span&gt;     &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&amp;quot;it's working&amp;quot;&lt;/span&gt;)
&lt;span&gt;...&lt;/span&gt; &lt;span&gt;except&lt;/span&gt; pymongo&lt;span&gt;.&lt;/span&gt;errors&lt;span&gt;.&lt;/span&gt;ConnectionFailure:
&lt;span&gt;...&lt;/span&gt;     &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&amp;quot;please start mongod&amp;quot;&lt;/span&gt;)
&lt;span&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This will not work any more, since the constructor never throws ConnectionFailure now. Instead, choose how long to wait before giving up by setting &lt;code&gt;serverSelectionTimeoutMS&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;&lt;span&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; client &lt;span&gt;=&lt;/span&gt; MongoClient(serverSelectionTimeoutMS&lt;span&gt;=500&lt;/span&gt;)
&lt;span&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span&gt;try&lt;/span&gt;:
&lt;span&gt;...&lt;/span&gt;     client&lt;span&gt;.&lt;/span&gt;admin&lt;span&gt;.&lt;/span&gt;command(&lt;span&gt;'ping'&lt;/span&gt;)
&lt;span&gt;...&lt;/span&gt;     &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&amp;quot;it's working&amp;quot;&lt;/span&gt;)
&lt;span&gt;...&lt;/span&gt; &lt;span&gt;except&lt;/span&gt; pymongo&lt;span&gt;.&lt;/span&gt;errors&lt;span&gt;.&lt;/span&gt;ConnectionFailure:
&lt;span&gt;...&lt;/span&gt;     &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&amp;quot;please start mongod&amp;quot;&lt;/span&gt;)
&lt;/pre&gt;&lt;/div&gt;


&lt;h2 id=&quot;one-monitor-thread-per-server&quot;&gt;One Monitor Thread Per Server&lt;/h2&gt;
&lt;p&gt;Even during regular operations, connections may hang up or time out, and servers go down for periods; monitoring each on a separate thread keeps PyMongo abreast of changes before they cause errors. You will see fewer network exceptions than with PyMongo 2, and the new driver will recover much faster from the unexpected.&lt;/p&gt;
&lt;h2 id=&quot;thread-safety&quot;&gt;Thread Safety&lt;/h2&gt;
&lt;p&gt;Another source of fragility in PyMongo 2 was APIs that were not designed for multithreading. Too many of PyMongo's options could be changed at runtime. For example, if you created a database handle:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;&lt;span&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; db &lt;span&gt;=&lt;/span&gt; client&lt;span&gt;.&lt;/span&gt;test
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;...and changed the handle's read preference on a thread, the change appeared in all threads:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;&lt;span&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span&gt;def&lt;/span&gt; &lt;span&gt;thread_fn&lt;/span&gt;():
&lt;span&gt;...&lt;/span&gt;     db&lt;span&gt;.&lt;/span&gt;read_preference &lt;span&gt;=&lt;/span&gt; ReadPreference&lt;span&gt;.&lt;/span&gt;SECONDARY
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Making these options mutable encouraged such mistakes, so we made them immutable. Now you configure handles to databases and collections using thread-safe APIs:&lt;/p&gt;
&lt;div class=&quot;codehilite&quot;&gt;&lt;pre&gt;&lt;span&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span&gt;def&lt;/span&gt; &lt;span&gt;thread_fn&lt;/span&gt;():
&lt;span&gt;...&lt;/span&gt;     my_db &lt;span&gt;=&lt;/span&gt; client&lt;span&gt;.&lt;/span&gt;get_database(
&lt;span&gt;...&lt;/span&gt;         &lt;span&gt;'test'&lt;/span&gt;,
&lt;span&gt;...&lt;/span&gt;         read_preference&lt;span&gt;=&lt;/span&gt;ReadPreference&lt;span&gt;.&lt;/span&gt;SECONDARY)
&lt;/pre&gt;&lt;/div&gt;


&lt;h1 id=&quot;modernity&quot;&gt;Modernity&lt;/h1&gt;
&lt;p&gt;Last, and most satisfying to the team, we have completed our transition to modern Python.&lt;/p&gt;
&lt;p&gt;While PyMongo 2 already supported the latest version of Python 3, it did so tortuously by executing &lt;code&gt;auto2to3&lt;/code&gt; on its source at install time. This made it too hard for the open source community to contribute to our code, and it led to some &lt;a href=&quot;http://emptysqua.re/blog/a-normal-accident-in-python-and-mod-wsgi/&quot;&gt;absurdly obscure bugs&lt;/a&gt;. We have updated to a single code base that is compatible with Python 2 and 3. We had to drop support for the ancient Pythons 2.4 and 2.5; we were encouraged by recent download statistics to believe that these zombie Python versions are finally at rest.&lt;/p&gt;
&lt;h1 id=&quot;motor&quot;&gt;Motor&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;http://motor.readthedocs.org/en/stable/&quot;&gt;Motor&lt;/a&gt;, my async driver for &lt;a href=&quot;http://www.tornadoweb.org/en/stable/&quot;&gt;Tornado&lt;/a&gt; and MongoDB, has &lt;em&gt;not&lt;/em&gt; yet been updated to wrap PyMongo 3. The current release, Motor 0.4, wraps PyMongo 2.8. Motor's still compatible with the latest MongoDB server version, but it lacks the new PyMongo 3 features&amp;mdash;for example, it doesn't have the new CRUD API, and it still monitors replica set members serially instead of in parallel. The next release, Motor 0.5, &lt;em&gt;still&lt;/em&gt; won't wrap PyMongo 3, because Motor 0.5 will focus on asyncio support instead. It won't be until version 0.6 that I update Motor with the latest PyMongo changes.&lt;/p&gt;&lt;img src=&quot;http://emptysqua.re/blog/analytics/http%3A//emptysqua.re/blog/announcing-pymongo-3//pixel.gif&quot; width=&quot;1px&quot; height=&quot;1px&quot; /&gt;</description>
	<pubDate>Wed, 08 Apr 2015 01:45:04 +0000</pubDate>
	<author>ajdavis@cs.oberlin.edu (A. Jesse Jiryu Davis)</author>
</item>
<item>
	<title>Baron Schwartz: State Of The Storage Engine - DZone</title>
	<guid>http://www.xaprb.com/blog/2015/04/02/state-of-the-storage-engine/</guid>
	<link>http://www.xaprb.com/blog/2015/04/02/state-of-the-storage-engine/</link>
	<description>&lt;p&gt;I contributed an article on &lt;a href=&quot;http://www.dzone.com/articles/state-storage-engine&quot;&gt;modern database storage
engines&lt;/a&gt; to the recent
&lt;a href=&quot;http://dzone.com/research/guide-to-databases&quot;&gt;DZone Guide To Database and Persistence
Management&lt;/a&gt;. I&amp;rsquo;m cross-posting the
article below with DZone&amp;rsquo;s permission.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.xaprb.com/media/2015/04/boardwalk.jpg&quot; alt=&quot;Boardwalk&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Readers of this guide already know the database world is undergoing rapid change. From relational-only, to NoSQL and Big Data, the technologies we use for data storage and retrieval today are much different from even five years ago.&lt;/p&gt;

&lt;p&gt;Today’s datasets are so large, and the workloads so demanding, that one-size-fits-all databases rarely make much sense. When a small inefficiency is multiplied by a huge dataset, the opportunity to use a specialized database to save money, improve performance, and optimize for developer productivity and happiness can be very large. And today’s solid-state storage is vastly different from spinning disks, too. These factors are forcing fundamental changes for database internals: the underlying algorithms, file formats, and data structures. As a result, modern applications are often backed by as many as a dozen distinct types of databases (polyglot persistence). These trends signal significant, long-term change in how databases are built, chosen, and managed.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Most companies can afford only one or two proper in-depth evaluations for a new database.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;textbook-architectures-lose-relevance&quot;&gt;Textbook Architectures Lose Relevance&lt;/h3&gt;

&lt;p&gt;Many of today’s mature relational databases, such as MySQL, Oracle, SQL Server, and PostgreSQL, base much of their architecture and design on decades-old research into transactional storage and relational models that stem from two classic textbooks in the field—known simply as &lt;a href=&quot;http://www.amazon.com/dp/1558601902&quot;&gt;Gray &amp;amp; Reuters&lt;/a&gt; and &lt;a href=&quot;http://www.amazon.com/dp/1558605088&quot;&gt;Weikum &amp;amp; Vossen&lt;/a&gt;. This “textbook architecture” can be described briefly as having:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Row-based storage with fixed schemas&lt;/li&gt;
&lt;li&gt;B-Tree primary and secondary indexes&lt;/li&gt;
&lt;li&gt;ACID transaction support&lt;/li&gt;
&lt;li&gt;Row-based locking&lt;/li&gt;
&lt;li&gt;MVCC (multi-version concurrency control) implemented by keeping old row versions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But this textbook architecture has been increasingly questioned, not only by newcomers but by leading database architects such as &lt;a href=&quot;http://slideshot.epfl.ch/play/suri_stonebraker&quot;&gt;Michael Stonebraker&lt;/a&gt;. Some new databases depart significantly from the textbook architecture with concepts such as wide-row and columnar storage, no support for concurrency at all, and eventual consistency. It’s worth noting that although NoSQL databases represent obvious changes in the data model and language—how developers access the database—not all NoSQL databases innovate architecturally. Coping with today’s data storage challenges often requires breaking from tradition architecturally, especially in the storage engine.&lt;/p&gt;

&lt;h3 id=&quot;log-structured-merge-trees&quot;&gt;Log-Structured Merge Trees&lt;/h3&gt;

&lt;p&gt;One of the more interesting trends in storage engines is the emergence of log-structured merge trees (LSM trees) as a replacement for the venerable B-Tree index. LSM trees are now about two decades old, and LevelDB is perhaps the most popular implementation. Databases such as Apache HBase, Hyperdex, Apache Cassandra, RocksDB, WiredTiger, and Riak use various types of LSM trees.&lt;/p&gt;

&lt;p&gt;LSM trees work by recording data, and changes to the data, in immutable segments or runs. The segments are usually organized into levels or generations. There are several strategies, but the first level commonly contains the most recent and active data, and lower levels usually have progressively larger and/or older data, depending on the leveling strategy. As data is inserted or changed, the top level fills up and its data is copied into a segment in the second level. Background processes merge segments in each level together, pruning out obsolete data and building lower-level segments in batches. Some LSM tree implementations add other features such as automatic compression, too. There are several benefits to this approach as compared to the classic B-Tree approach:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Immutable storage segments are easily cached and backed up&lt;/li&gt;
&lt;li&gt;Writes can be performed without reading first, greatly speeding them up&lt;/li&gt;
&lt;li&gt;Some difficult problems such as fragmentation are avoided or replaced by simpler problems&lt;/li&gt;
&lt;li&gt;Some workloads can experience fewer random-access I/O operations, which are slow&lt;/li&gt;
&lt;li&gt;There may be less wear on solid-state storage, which can’t update data in-place&lt;/li&gt;
&lt;li&gt;It can be possible to eliminate the B-Tree “write cliff,” which happens when the working set no longer fits in memory and writes slow down drastically&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Although many of the problems with B-Tree indexes can be avoided, mitigated, or transformed, LSM tree indexes aren’t a panacea. There are always trade-offs and implementation details. The main set of trade-offs for LSM trees are usually explained in terms of amplification along several dimensions. The amplification is the average ratio of the database’s physical behavior to the logical behavior of the user’s request, over the long-term. It’s usually a ratio of bytes to bytes, but can also be expressed in terms of operations, e.g. number of physical I/O operations performed per logical user request.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Write amplification&lt;/strong&gt; is the multiple of bytes written by the database to bytes changed by the user. Since some LSM trees rewrite unchanging data over time, write amplification can be high in LSM trees.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Read amplification&lt;/strong&gt; is how many bytes the database has to physically read to return values to the user, compared to the bytes returned. Since LSM trees may have to look in several places to find data, or to determine what the data’s most recent value is, read amplification can be high.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Space amplification&lt;/strong&gt; is how many bytes of data are stored on disk, relative to how many logical bytes the database contains. Since LSM trees don’t update in place, values that are updated often can cause space amplification.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition to amplification, LSM trees can have other performance problems, such as read and write bursts and stalls. It’s important to note that amplification and other issues are heavily dependent on workload, configuration of the engine, and the specific implementation. Unlike B-Tree indexes, which have essentially a single canonical implementation, LSM trees are a group of related algorithms and implementations that vary widely.&lt;/p&gt;

&lt;p&gt;There are other interesting technologies to consider besides LSM trees. One is &lt;a href=&quot;https://symas.com/getting-down-and-dirty-with-lmdb-qa-with-symas-corporations-howard-chu-about-symass-lightning-memory-mapped-database/&quot;&gt;Howard Chu&lt;/a&gt;’s LMDB (Lightning Memory-Mapped Database), which is a copy-on-write B-Tree. It is widely used and has inspired clones such as &lt;a href=&quot;https://github.com/boltdb/bolt&quot;&gt;BoltDB&lt;/a&gt;, which is the storage engine behind the up-and-coming &lt;a href=&quot;http://influxdb.com/&quot;&gt;InfluxDB&lt;/a&gt; time-series database. Another LSM alternative is &lt;a href=&quot;http://www.tokutek.com/&quot;&gt;Tokutek’s&lt;/a&gt; fractal trees, which form the basis of high-performance write and space-optimized alternatives to MySQL and MongoDB.&lt;/p&gt;

&lt;h3 id=&quot;evaluating-databases-with-log-structured-merge-trees&quot;&gt;Evaluating Databases With Log-Structured Merge Trees&lt;/h3&gt;

&lt;p&gt;No matter what underlying storage you use, there’s always a trade-off. The iron triangle of storage engines is this:&lt;/p&gt;

&lt;p&gt;You can have &lt;strong&gt;sequential reads without amplification, sequential writes without amplification, or an immutable write-once design&lt;/strong&gt;—&lt;i&gt;pick any two&lt;/i&gt;.&lt;/p&gt;

&lt;p&gt;Today’s emerging Big Data use cases, in which massive datasets are kept in raw form for a long time instead of being summarized and discarded, represent some of the classes of workloads that can potentially be addressed well with LSM tree storage (time-series data is a good example). However, knowledge of the specific LSM implementation must be combined with a deep understanding of the workload, hardware, and application.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;hellip;although NoSQL databases represent obvious changes in the data model and language, not all NoSQL databases innovate architecturally.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Sometimes companies don’t find a database that’s optimized for their exact use case, so they build their own, often borrowing concepts from various databases and newer storage engines to achieve the efficiency and performance they need. An alternative is to adapt an efficient and trusted technology that’s almost good enough. At VividCortex, we ignore the relational features of MySQL and use it as a thin wrapper around InnoDB to store our large-scale, high-velocity time-series data.&lt;/p&gt;

&lt;p&gt;Whatever road you take, a good deal of creativity and experience is required from architects who are looking to overhaul their application’s capabilities. You can’t just assume you’ll plug in a database that will immediately fit your use case. You’ll need to take a much deeper look at the storage engine and the paradigms it is based on.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Baron Schwartz&lt;/strong&gt; is the founder and CEO of &lt;a href=&quot;https://vividcortex.com&quot;&gt;VividCortex&lt;/a&gt;, the best way to see what your production database servers are doing. He is the author of High Performance MySQL and many open-source tools for MySQL administration. He’s also an Oracle ACE and frequent participant in the PostgreSQL community.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To read the full report free of charge, download the
&lt;a href=&quot;http://dzone.com/research/guide-to-databases&quot;&gt;DZone Guide To Database and Persistence
Management&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cropped boardwalk image by &lt;a href=&quot;https://unsplash.com/nmsilva&quot;&gt;Nuno Silva&lt;/a&gt;.&lt;/p&gt;</description>
	<pubDate>Thu, 02 Apr 2015 08:51:18 +0000</pubDate>
</item>
<item>
	<title>MongoDB Spain: How to upgrade your MongoDB deployment to 3.0 version</title>
	<guid>http://www.mongodbspain.com/?p=2397</guid>
	<link>http://www.mongodbspain.com/en/2015/03/18/how-to-upgrade-your-mongodb-deployment-to-3-0-version/</link>
	<description>&lt;p&gt;It is the moment to upgrade our MongoDB installation, do we know how to do it? In this post we will explain how to achieve it for all sort of deployments, a stand-alone node, a Replica set or a Sharded Cluster. We will talk about how to upgrade to 3.0 version.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;&lt;b&gt;Release and Upgrade Notes&lt;/b&gt;&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;At the very beginning we must always read the Release Notes and the Upgrade Notes, specially between major releases. These are the official documentation urls:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a title=&quot;mongodb release notes 3.0 upgrade&quot; href=&quot;http://docs.mongodb.org/manual/release-notes/3.0-upgrade/&quot; target=&quot;_blank&quot;&gt;mongodb release notes 3.0 upgrade&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a title=&quot;mongodb release notes 3.0 upgrade replica set&quot; href=&quot;http://docs.mongodb.org/manual/release-notes/3.0-upgrade/#upgrade-a-replica-set-to-3-0&quot; target=&quot;_blank&quot;&gt;mongodb release notes 3.0 upgrade replica set&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a title=&quot;mongodb release notes 3.0 upgrade cluster&quot; href=&quot;http://docs.mongodb.org/manual/release-notes/3.0-upgrade/#upgrade-a-sharded-cluster-to-3-0&quot; target=&quot;_blank&quot;&gt;mongodb release notes 3.0 upgrade cluster&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;strong&gt;&lt;b&gt;Binary downloads&lt;/b&gt;&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Regardless what kind of installation we have, we will always need the binary files of the new version. We can download them from this url:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.mongodb.org/downloads&quot; target=&quot;_blank&quot;&gt;Downloads &amp;#8211; MongoDB&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We will obtain a compressed file that will be necessary to descompress to access those binaries.&lt;/p&gt;
&lt;p&gt;As always, we will choose the most appropiate version, we can pick out between:&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;SO&lt;/td&gt;
&lt;td&gt;64 bit&lt;/td&gt;
&lt;td&gt;32 bit&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Linux&lt;/td&gt;
&lt;td&gt;√&lt;/td&gt;
&lt;td&gt;√&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mac OS X&lt;/td&gt;
&lt;td&gt;√&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Windows&lt;/td&gt;
&lt;td&gt;√&lt;/td&gt;
&lt;td&gt;√&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Solaris&lt;/td&gt;
&lt;td&gt;√&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;strong&gt;&lt;b&gt;Restrictions&lt;/b&gt;&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;It will be necessary to have installed the 2.6 version in order to upgrade to 3.0.&lt;/p&gt;
&lt;p&gt;Once 3.0 installed we will not be able to downgrade to a less version than 2.6.5.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;&lt;b&gt;Package upgrades&lt;/b&gt;&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;If we have installed MongoDB from apt, yum or zypper repositories, we will upgrade the version using the package manager. We can read the instructions at this url: &lt;a title=&quot;installation instructions&quot; href=&quot;http://docs.mongodb.org/manual/administration/install-on-linux/&quot; target=&quot;_blank&quot;&gt;installation instructions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;b&gt;Upgrade of a standalone node installation&lt;/b&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These are the steps we must follow:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Shut down the MongoDB instance.&lt;/li&gt;
&lt;li&gt;Replace the existing binary with the 3.0 mongod binary.&lt;/li&gt;
&lt;li&gt;Restart &lt;a title=&quot;mongod&quot; href=&quot;http://docs.mongodb.org/v2.8/reference/program/mongod/#bin.mongod&quot; target=&quot;_blank&quot;&gt;mongod&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With this purpose we can use one of the following commands:&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;sudo service mongod stop&lt;/pre&gt;&lt;p&gt;&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;mongod --dbpath  --shutdown&lt;/pre&gt;&lt;p&gt;&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;CTRL+C&lt;/pre&gt;&lt;p&gt;&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;kill&lt;/pre&gt;&lt;p&gt;We will restart mongod instance by:&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;sudo service mongod restart&lt;/pre&gt;&lt;p&gt;&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;mongod --dbpath  with the remaining necessary options&lt;/pre&gt;&lt;p&gt;&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;&lt;b&gt;Upgrading a Replica Set&lt;/b&gt;&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Goals&lt;/p&gt;
&lt;p&gt;When carrying out maintenance work (as it can be a version upgrade)  in the nodes of a Replica Set, we will have two goals:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Do not run any risk.&lt;/li&gt;
&lt;li&gt;Do not interrupt the service at any time.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Requirements&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The Replica Set must have a minimum of three nodes, with two full secondaries for not having any risk and for having two data copies in every moment. This would not be possible with, for example, one primary, one secondary and an arbiter, because we would only have one copy of data (primary’s) while upgrading.&lt;/li&gt;
&lt;li&gt;We do not want to lose any operation while we are upgrading, for this reason the oplog time must be big enough. The oplog is a &lt;a href=&quot;http://docs.mongodb.org/manual/core/capped-collections/&quot; target=&quot;_blank&quot;&gt;capped collection&lt;/a&gt; in which MongoDB stores all the activity occurred with our data. We will use this register for catching up the node that has just been upgraded when it is working again in the Replica Set. We will not lose any operation when the time needed to upgrade is less than the time we are able to store in the oplog. We can know the size of our oplog window using this command:&lt;br /&gt;
&lt;pre class=&quot;crayon-plain-tag&quot;&gt;db.printReplicationInfo()&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Steps to follow with secondary nodes&lt;/p&gt;
&lt;p&gt;Logically, in order to keep two copies of our data in every moment, the Replica Set’s secondaries must be upgrade one by one.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Shutdown the mongod instance. The Replica Set still works with the primary and one secondary (both of them will continue to make ping to the downloaded node for checking its state).&lt;/li&gt;
&lt;li&gt;Replace the 2.6 binary with 3.0 binary.&lt;/li&gt;
&lt;li&gt;Start the instance with the same options it had.&lt;/li&gt;
&lt;li&gt;We must wait for the upgraded node to catch up before beginning with other secondary. Looking at the optimeDate value returned by the rs.status() command we can know if the replication is over (it must be equal for all the Replica Set members).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Steps to follow with the primary node&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We close existing connections to drivers, convert our primary to secondary and force an election. We get all this with the rs.stepDown() command (step down the primary and force the set to failover). The drivers will automatically establish connections with the new primary node and with little time penalty. We remember that a Replica Set failover is not instantaneous and, while complete, writes are not supported.&lt;br /&gt;
&lt;pre class=&quot;crayon-plain-tag&quot;&gt;a:PRIMARY&gt; rs.stepDown()
2014-12-10T00:39:07.094+0100 I NETWORK DBClientCursor::init call() failed
2014-12-10T00:39:07.099+0100 I QUERY Error: error doing query: failed
at DBQuery._exec (src/mongo/shell/query.js:83:36)
at DBQuery.hasNext (src/mongo/shell/query.js:240:10)
at DBCollection.findOne (src/mongo/shell/collection.js:185:19)
at DB.runCommand (src/mongo/shell/db.js:58:41)
at DB.adminCommand (src/mongo/shell/db.js:66:41)
at Function.rs.stepDown (src/mongo/shell/utils.js:998:43)
at (shell):1:4 at src/mongo/shell/query.js:83
2014-12-10T00:39:07.101+0100 I NETWORK trying reconnect to 127.0.0.1:27000 (127.0.0.1) failed
2014-12-10T00:39:07.101+0100 I NETWORK reconnect 127.0.0.1:27000 (127.0.0.1) ok
a:SECONDARY&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;Now, all we need is to apply the four steps for the secondaries.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Upgrading a Sharded Cluster&lt;/h4&gt;
&lt;p&gt;All members of a cluster must be upgraded to 2.6 version in order to upgrade it to 3.0 version.&lt;/p&gt;
&lt;p&gt;A Sharded Cluster is made up of Replica Sets but, also, it has:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Config Servers (they keep the database which tells us the shard where our data is stored)&lt;/li&gt;
&lt;li&gt;mongos processes (they enroute the client requests to the shard returned by the config server)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Recomendations&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Please, make sure you have got a backup of the ‘config’ database before upgrading the cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Requirements&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;While we are upgrading the cluster, we must be sure that no client is updating the meta-data (config database).&lt;/li&gt;
&lt;li&gt;First we will upgrade the cluster&amp;#8217;s metadata, then the mongos processes and, finally, the mongod’s.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Steps to follow&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Disable the balancer (if there are operations in progress MongoDB will wait until they are finished).&lt;br /&gt;
&lt;pre class=&quot;crayon-plain-tag&quot;&gt;mongos&gt; sh.stopBalancer()
Waiting for active hosts...
Waiting for the balancer lock...
Waiting again for active hosts after balancer is off...
mongos&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;Upgrade the cluster’s metadata.
&lt;ol&gt;
&lt;li&gt;Upgrade one mongos to 3.0 version.&lt;/li&gt;
&lt;li&gt;Start this mongos with the same options it had and with this new one &amp;#8211;upgrade (this execution will be finished when the upgrade is over). MongoDB will not make splits nor chunk moves while this execution is in progress. This message will be sent out when the metadata upgrade is well finished:&lt;br /&gt;
&lt;pre class=&quot;crayon-plain-tag&quot;&gt;upgrade of config server to v6 successful
Config database is at version v6&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;Upgrade to 3.0 version the remaining mongos instances.&lt;/li&gt;
&lt;li&gt;Start the three mongos without the &amp;#8211;upgrade option.&lt;/li&gt;
&lt;li&gt;Upgrade the three config servers, one by one, like they were mongod standalone nodes (shutdown, upgrade and start). We know that in a production environment is recommendable to have three config servers. They keep key information, in a proprietary database, related to the data contained in each shard, therefore, before upgrading them is very recommendable to make a backup. All of them store the same information, so, you only have to stop one service and make the backup from it. This is possible because if one of them is off, automatically the metadata turns to read-only mode.&lt;/li&gt;
&lt;li&gt;Upgrade all shard secondary nodes. The secondaries of a shard must be upgraded one by one, however, we can upgrade them at once if they belong to distinct shards.&lt;/li&gt;
&lt;li&gt;Upgrade, one by one, all shard primary nodes. It is not recommended to upgrade at a time because the mongos’s will enroute all writes to the new primaries and the system will be busy establishing new connections. Remember that a stepDown over a primary destroy all existing connections and new ones are established.&lt;/li&gt;
&lt;li&gt;Re-enable the balancer.&lt;br /&gt;
&lt;pre class=&quot;crayon-plain-tag&quot;&gt;mongos&gt;sh.setBalancerState(true)&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If we try to upgrade the &amp;#8216;config&amp;#8217; database before stopping the balancer MongoDB return us an error.&lt;/p&gt;
&lt;p&gt;Both mongodbspain.com like myself disclaim all responsibility for any problems that may arise in updating your MongoDB deployments. This article is written for information and educational purposes only. Naturally, you always must read the official MongoDB sources.&lt;/p&gt;</description>
	<pubDate>Wed, 18 Mar 2015 19:49:55 +0000</pubDate>
</item>
<item>
	<title>MongoDB Spain: Two steps to shard a MongoDB collection</title>
	<guid>http://www.mongodbspain.com/?p=2441</guid>
	<link>http://www.mongodbspain.com/en/2015/03/03/two-steps-to-shard-a-mongodb-collection/</link>
	<description>&lt;p&gt;In the post &lt;a title=&quot;How to set up a MongoDB Sharded Cluster&quot; href=&quot;http://www.mongodbspain.com/en/2015/01/26/how-to-set-up-a-mongodb-sharded-cluster/&quot; target=&quot;_blank&quot;&gt;How to set up a MongoDB Sharded Cluster&lt;/a&gt; we studied how to set up a MongoDB Sharded Cluster. Its goal, as we already know, is to scale and to balance the workload uniformly across all our shards.&lt;/p&gt;
&lt;p&gt;Today, we are going to learn what to do in order to shard a collection and get all its documents well distributed among our shards.&lt;/p&gt;
&lt;p&gt;We must execute all administrative tasks related to shards clusters connected to a mongos:&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;$mongo
mongos&gt;&lt;/pre&gt;&lt;p&gt;MongoDB shards at a collection level. This means that, for a given database, we can have sharded collections and non-sharded collections.&lt;/p&gt;
&lt;h4&gt;First step&lt;/h4&gt;
&lt;p&gt;Sharding must be enabled in the database the collection belongs to is necessary before trying to shard it. We are going to do it by this way:&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;mongos&gt; sh.enableSharding(&quot;shardTestDB&quot;)
{ &quot;ok&quot; : 1 }
mongos&gt; sh.status()
--- Sharding Status ---
  sharding version: {
    	&quot;_id&quot; : 1,
    	&quot;version&quot; : 4,
        &quot;minCompatibleVersion&quot; : 4,
    	&quot;currentVersion&quot; : 5,
    	&quot;clusterId&quot; : ObjectId(&quot;54eb010dc6d2e5d19ca9df05&quot;)
}
  shards:
    	{  &quot;_id&quot; : &quot;shard0000&quot;,  &quot;host&quot; : &quot;PSINFW95:27000&quot; }
    	{  &quot;_id&quot; : &quot;shard0001&quot;,  &quot;host&quot; : &quot;PSINFW95:27001&quot; }
  databases:
    	{  &quot;_id&quot; : &quot;admin&quot;,  &quot;partitioned&quot; : false,  &quot;primary&quot; : &quot;config&quot; }
    	{  &quot;_id&quot; : &quot;test&quot;,  &quot;partitioned&quot; : false,  &quot;primary&quot; : &quot;shard0000&quot; }
    	{  &quot;_id&quot; : &quot;shardTestDB&quot;,  &quot;partitioned&quot; : true,  &quot;primary&quot; : &quot;shard0000&quot; }
 
mongos&gt;&lt;/pre&gt;&lt;p&gt;Collections are sharded using a field called ‘shardkey’, hence, it is very important to choose it properly. You can read about the characteristics a shard key must have at this url: &lt;a title=&quot;Choosing a shard key&quot; href=&quot;http://docs.mongodb.org/manual/tutorial/choose-a-shard-key/&quot; target=&quot;_blank&quot;&gt;Choosing a shard key&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Second step&lt;/h4&gt;
&lt;p&gt;We have this command to shard a collection (as a shardkey we use the ‘username’ field):&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;mongos&gt; sh.shardCollection(&quot;shardTestDB.users&quot;, { &quot;username&quot; : 1 } )
{ &quot;collectionsharded&quot; : &quot;shardTestDB.users&quot;, &quot;ok&quot; : 1 }
mongos&gt; sh.status()
--- Sharding Status ---
  sharding version: {
    	&quot;_id&quot; : 1,
    	&quot;version&quot; : 4,
        &quot;minCompatibleVersion&quot; : 4,
    	&quot;currentVersion&quot; : 5,
    	&quot;clusterId&quot; : ObjectId(&quot;54eb010dc6d2e5d19ca9df05&quot;)
}
  shards:
    	{  &quot;_id&quot; : &quot;shard0000&quot;,  &quot;host&quot; : &quot;PSINFW95:27000&quot; }
    	{  &quot;_id&quot; : &quot;shard0001&quot;,  &quot;host&quot; : &quot;PSINFW95:27001&quot; }
  databases:
    	{  &quot;_id&quot; : &quot;admin&quot;,  &quot;partitioned&quot; : false,  &quot;primary&quot; : &quot;config&quot; }
    	{  &quot;_id&quot; : &quot;test&quot;,  &quot;partitioned&quot; : false,  &quot;primary&quot; : &quot;shard0000&quot; }
    	{  &quot;_id&quot; : &quot;shardTestDB&quot;,  &quot;partitioned&quot; : true,  &quot;primary&quot; : &quot;shard0000&quot; }
            	shardTestDB.users
	                    shard key: { &quot;username&quot; : 1 }
                    	chunks:
                                shard0000   	1
                    	{ &quot;username&quot; : { &quot;$minKey&quot; : 1 } } --&gt;&gt; { &quot;username&quot; : { &quot;$maxKey&quot; : 1 } } on : shard0000 Timestamp(1, 0)
 
mongos&gt;&lt;/pre&gt;&lt;p&gt;When we shard a collection MongoDB creates an index on the shardkey:&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;mongos&gt; db.users.getIndexes()
[
    	{
            	&quot;v&quot; : 1,
            	&quot;key&quot; : {
                        &quot;_id&quot; : 1
            	},
            	&quot;name&quot; : &quot;_id_&quot;,
            	&quot;ns&quot; : &quot;shardTestDB.users&quot;
    	},
    	{
            	&quot;v&quot; : 1,
            	&quot;key&quot; : {
                        &quot;username&quot; : 1
            	},
            	&quot;name&quot; : &quot;username_1&quot;,
            	&quot;ns&quot; : &quot;shardTestDB.users&quot;
    	}
]
mongos&gt;&lt;/pre&gt;&lt;p&gt;If our collection has some previous data these will be distributed among the shards.&lt;/p&gt;
&lt;p&gt;Very easy, right?&lt;/p&gt;
&lt;p&gt;Let’s see MongoDB in action. We are going to insert documents in our collection to check that MongoDB distributes them among the shards. First of all we have to stop de balancer, afterwards we are going to do the inserts and all the documents must be located at the shard which the collection belongs to (shard 0 by default). We are going to continue running the balancer to check that MongoDB moves uniformly our documents across all the shards.&lt;/p&gt;
&lt;p&gt;We set the size of the chunk in 1Mb to avoid inserting too much documents.&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;mongos&gt; use config
switched to db config
mongos&gt; db.settings.find( { &quot;_id&quot; : &quot;chunksize&quot; } )
{ &quot;_id&quot; : &quot;chunksize&quot;, &quot;value&quot; : 64 }
mongos&gt; db.settings.save( { &quot;_id&quot; : &quot;chunksize&quot;, value : 1 } )
WriteResult({ &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 })
mongos&gt; db.settings.find( { &quot;_id&quot; : &quot;chunksize&quot; } )
{ &quot;_id&quot; : &quot;chunksize&quot;, &quot;value&quot; : 1 }
mongos&gt;&lt;/pre&gt;&lt;p&gt;We stop the balancer:&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;mongos&gt; sh.stopBalancer()
Waiting for active hosts...
Waiting for the balancer lock...
Waiting again for active hosts after balancer is off...
mongos&gt; sh.getBalancerState()
false
mongos&gt;&lt;/pre&gt;&lt;p&gt;We insert the documents:&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;mongos&gt; use shardTestDB
switched to db shardTestDB
 
mongos&gt; for (var i=0; i100000; i++) { ... db.users.insert( { &quot;username&quot; : &quot;user&quot;+i, &quot;created at&quot; : new Date() } ); ... } WriteResult({ &quot;nInserted&quot; : 1 }) mongos&gt;
mongos&gt; db.users.count()
100000
mongos&gt;&lt;/pre&gt;&lt;p&gt;We check that all the documents have been stored at the shard the collection belongs to.&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;mongos&gt; sh.status( { verbose : 1 } )
--- Sharding Status ---
  sharding version: {
    	&quot;_id&quot; : 1,
    	&quot;version&quot; : 4,
        &quot;minCompatibleVersion&quot; : 4,
    	&quot;currentVersion&quot; : 5,
    	&quot;clusterId&quot; : ObjectId(&quot;54eb010dc6d2e5d19ca9df05&quot;)
}
  shards:
    	{  &quot;_id&quot; : &quot;shard0000&quot;,  &quot;host&quot; : &quot;PSINFW95:27000&quot; }
    	{  &quot;_id&quot; : &quot;shard0001&quot;,  &quot;host&quot; : &quot;PSINFW95:27001&quot; }
  databases:
    	{  &quot;_id&quot; : &quot;admin&quot;,  &quot;partitioned&quot; : false,  &quot;primary&quot; : &quot;config&quot; }
    	{  &quot;_id&quot; : &quot;test&quot;,  &quot;partitioned&quot; : false,  &quot;primary&quot; : &quot;shard0000&quot; }
    	{  &quot;_id&quot; : &quot;shardTestDB&quot;,  &quot;partitioned&quot; : true,  &quot;primary&quot; : &quot;shard0000&quot; }
            	shardTestDB.users
                    	shard key: { &quot;username&quot; : 1 }
  	                  chunks:
                                shard0000   	21
                    	{ &quot;username&quot; : { &quot;$minKey&quot; : 1 } } --&gt;&gt; { &quot;username&quot; : &quot;user0&quot; } on : shard0000 Timestamp(1, 1)
                    	{ &quot;username&quot; : &quot;user0&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user1421&quot; } on : shard0000 Timestamp(1, 7)
                    	{ &quot;username&quot; : &quot;user1421&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user18423&quot; } on : shard0000 Timestamp(1, 9)
                    	{ &quot;username&quot; : &quot;user18423&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user22636&quot; } on : shard0000 Timestamp(1, 11)
                    	{ &quot;username&quot; : &quot;user22636&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user3131&quot; } on : shard0000 Timestamp(1, 12)
               	     { &quot;username&quot; : &quot;user3131&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user35523&quot; } on : shard0000 Timestamp(1, 15)
                    	{ &quot;username&quot; : &quot;user35523&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user39737&quot; } on : shard0000 Timestamp(1, 17)
                    	{ &quot;username&quot; : &quot;user39737&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user4395&quot; } on : shard0000 Timestamp(1, 19)
                    	{ &quot;username&quot; : &quot;user4395&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user48162&quot; } on : shard0000 Timestamp(1, 21)
                    	{ &quot;username&quot; : &quot;user48162&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user5314&quot; } on : shard0000 Timestamp(1, 22)
                    	{ &quot;username&quot; : &quot;user5314&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user57353&quot; } on : shard0000 Timestamp(1, 23)
                    	{ &quot;username&quot; : &quot;user57353&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user61566&quot; } on : shard0000 Timestamp(1, 25)
                    	{ &quot;username&quot; : &quot;user61566&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user6578&quot; } on : shard0000 Timestamp(1, 27)
                    	{ &quot;username&quot; : &quot;user6578&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user69993&quot; } on : shard0000 Timestamp(1, 29)
                    	{ &quot;username&quot; : &quot;user69993&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user74204&quot; } on : shard0000 Timestamp(1, 31)
                    	{ &quot;username&quot; : &quot;user74204&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user78418&quot; } on : shard0000 Timestamp(1, 33)
                    	{ &quot;username&quot; : &quot;user78418&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user82630&quot; } on : shard0000 Timestamp(1, 35)
                    	{ &quot;username&quot; : &quot;user82630&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user86844&quot; } on : shard0000 Timestamp(1, 37)
           	         { &quot;username&quot; : &quot;user86844&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user91056&quot; } on : shard0000 Timestamp(1, 39)
                    	{ &quot;username&quot; : &quot;user91056&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user999&quot; } on : shard0000 Timestamp(1, 40)
                    	{ &quot;username&quot; : &quot;user999&quot; } --&gt;&gt; { &quot;username&quot; : { &quot;$maxKey&quot; : 1 } } on : shard0000 Timestamp(1, 4)
 
mongos&gt;&lt;/pre&gt;&lt;p&gt;We start the balancer (automatically MongoDB moves the documents):&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;mongos&gt; sh.startBalancer()&lt;/pre&gt;&lt;p&gt;And, finally, we can check that all the documents have been moved as we expected.&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;mongos&gt; sh.status( { verbose : 1 } )
--- Sharding Status ---
  sharding version: {
    	&quot;_id&quot; : 1,
    	&quot;version&quot; : 4,
        &quot;minCompatibleVersion&quot; : 4,
    	&quot;currentVersion&quot; : 5,
    	&quot;clusterId&quot; : ObjectId(&quot;54eb010dc6d2e5d19ca9df05&quot;)
}
  shards:
    	{  &quot;_id&quot; : &quot;shard0000&quot;,  &quot;host&quot; : &quot;PSINFW95:27000&quot; }
    	{  &quot;_id&quot; : &quot;shard0001&quot;,  &quot;host&quot; : &quot;PSINFW95:27001&quot; }
  databases:
    	{  &quot;_id&quot; : &quot;admin&quot;,  &quot;partitioned&quot; : false,  &quot;primary&quot; : &quot;config&quot; }
    	{  &quot;_id&quot; : &quot;test&quot;,  &quot;partitioned&quot; : false,  &quot;primary&quot; : &quot;shard0000&quot; }
    	{  &quot;_id&quot; : &quot;shardTestDB&quot;,  &quot;partitioned&quot; : true,  &quot;primary&quot; : &quot;shard0000&quot; }
            	shardTestDB.users
                    	shard key: { &quot;username&quot; : 1 }
                    	chunks:
                                shard0001   	2
                                shard0000   	19
                    	{ &quot;username&quot; : { &quot;$minKey&quot; : 1 } } --&gt;&gt; { &quot;username&quot; : &quot;user0&quot; } on : shard0001 Timestamp(2, 0)
                    	{ &quot;username&quot; : &quot;user0&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user1421&quot; } on : shard0001 Timestamp(3, 0)
                    	{ &quot;username&quot; : &quot;user1421&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user18423&quot; } on : shard0000 Timestamp(3, 1)
                    	{ &quot;username&quot; : &quot;user18423&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user22636&quot; } on : shard0000 Timestamp(1, 11)
                    	{ &quot;username&quot; : &quot;user22636&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user3131&quot; } on : shard0000 Timestamp(1, 12)
                    	{ &quot;username&quot; : &quot;user3131&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user35523&quot; } on : shard0000 Timestamp(1, 15)
                    	{ &quot;username&quot; : &quot;user35523&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user39737&quot; } on : shard0000 Timestamp(1, 17)
                    	{ &quot;username&quot; : &quot;user39737&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user4395&quot; } on : shard0000 Timestamp(1, 19)
                    	{ &quot;username&quot; : &quot;user4395&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user48162&quot; } on : shard0000 Timestamp(1, 21)
             	       { &quot;username&quot; : &quot;user48162&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user5314&quot; } on : shard0000 Timestamp(1, 22)
                    	{ &quot;username&quot; : &quot;user5314&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user57353&quot; } on : shard0000 Timestamp(1, 23)
                    	{ &quot;username&quot; : &quot;user57353&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user61566&quot; } on : shard0000 Timestamp(1, 25)
                    	{ &quot;username&quot; : &quot;user61566&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user6578&quot; } on : shard0000 Timestamp(1, 27)
                    	{ &quot;username&quot; : &quot;user6578&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user69993&quot; } on : shard0000 Timestamp(1, 29)
                    	{ &quot;username&quot; : &quot;user69993&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user74204&quot; } on : shard0000 Timestamp(1, 31)
                    	{ &quot;username&quot; : &quot;user74204&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user78418&quot; } on : shard0000 Timestamp(1, 33)
                    	{ &quot;username&quot; : &quot;user78418&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user82630&quot; } on : shard0000 Timestamp(1, 35)
                    	{ &quot;username&quot; : &quot;user82630&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user86844&quot; } on : shard0000 Timestamp(1, 37)
                    	{ &quot;username&quot; : &quot;user86844&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user91056&quot; } on : shard0000 Timestamp(1, 39)
              	      { &quot;username&quot; : &quot;user91056&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user999&quot; } on : shard0000 Timestamp(1, 40)
                    	{ &quot;username&quot; : &quot;user999&quot; } --&gt;&gt; { &quot;username&quot; : { &quot;$maxKey&quot; : 1 } } on : shard0000 Timestamp(1, 4)
 
mongos&gt; sh.status( { verbose : 1 } )
--- Sharding Status ---
  sharding version: {
    	&quot;_id&quot; : 1,
    	&quot;version&quot; : 4,
        &quot;minCompatibleVersion&quot; : 4,
    	&quot;currentVersion&quot; : 5,
    	&quot;clusterId&quot; : ObjectId(&quot;54eb010dc6d2e5d19ca9df05&quot;)
}
  shards:
    	{  &quot;_id&quot; : &quot;shard0000&quot;,  &quot;host&quot; : &quot;PSINFW95:27000&quot; }
    	{  &quot;_id&quot; : &quot;shard0001&quot;,  &quot;host&quot; : &quot;PSINFW95:27001&quot; }
  databases:
    	{  &quot;_id&quot; : &quot;admin&quot;,  &quot;partitioned&quot; : false,  &quot;primary&quot; : &quot;config&quot; }
    	{  &quot;_id&quot; : &quot;test&quot;,  &quot;partitioned&quot; : false,  &quot;primary&quot; : &quot;shard0000&quot; }
    	{  &quot;_id&quot; : &quot;shardTestDB&quot;,  &quot;partitioned&quot; : true,  &quot;primary&quot; : &quot;shard0000&quot; }
            	shardTestDB.users
                    	shard key: { &quot;username&quot; : 1 }
                    	chunks:
                                shard0001  	 9
                                shard0000   	12
                    	{ &quot;username&quot; : { &quot;$minKey&quot; : 1 } } --&gt;&gt; { &quot;username&quot; : &quot;user0&quot; } on : shard0001 Timestamp(2, 0)
                    	{ &quot;username&quot; : &quot;user0&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user1421&quot; } on : shard0001 Timestamp(3, 0)
                    	{ &quot;username&quot; : &quot;user1421&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user18423&quot; } on : shard0001 Timestamp(4, 0)
                    	{ &quot;username&quot; : &quot;user18423&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user22636&quot; } on : shard0001 Timestamp(5, 0)
                    	{ &quot;username&quot; : &quot;user22636&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user3131&quot; } on : shard0001 Timestamp(6, 0)
                 	   { &quot;username&quot; : &quot;user3131&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user35523&quot; } on : shard0001 Timestamp(7, 0)
                    	{ &quot;username&quot; : &quot;user35523&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user39737&quot; } on : shard0001 Timestamp(8, 0)
                    	{ &quot;username&quot; : &quot;user39737&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user4395&quot; } on : shard0001 Timestamp(9, 0)
                    	{ &quot;username&quot; : &quot;user4395&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user48162&quot; } on : shard0001 Timestamp(10, 0)
                    	{ &quot;username&quot; : &quot;user48162&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user5314&quot; } on : shard0000 Timestamp(10, 1)
                    	{ &quot;username&quot; : &quot;user5314&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user57353&quot; } on : shard0000 Timestamp(1, 23)
                    	{ &quot;username&quot; : &quot;user57353&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user61566&quot; } on : shard0000 Timestamp(1, 25)
                    	{ &quot;username&quot; : &quot;user61566&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user6578&quot; } on : shard0000 Timestamp(1, 27)
                    	{ &quot;username&quot; : &quot;user6578&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user69993&quot; } on : shard0000 Timestamp(1, 29)
                    	{ &quot;username&quot; : &quot;user69993&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user74204&quot; } on : shard0000 Timestamp(1, 31)
                    	{ &quot;username&quot; : &quot;user74204&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user78418&quot; } on : shard0000 Timestamp(1, 33)
                        { &quot;username&quot; : &quot;user78418&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user82630&quot; } on : shard0000 Timestamp(1, 35)
                    	{ &quot;username&quot; : &quot;user82630&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user86844&quot; } on : shard0000 Timestamp(1, 37)
                 	   { &quot;username&quot; : &quot;user86844&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user91056&quot; } on : shard0000 Timestamp(1, 39)
                    	{ &quot;username&quot; : &quot;user91056&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user999&quot; } on : shard0000 Timestamp(1, 40)
                    	{ &quot;username&quot; : &quot;user999&quot; } --&gt;&gt; { &quot;username&quot; : { &quot;$maxKey&quot; : 1 } } on : shard0000 Timestamp(1, 4)
 
mongos&gt; sh.status( { verbose : 1 } )
--- Sharding Status ---
  sharding version: {
    	&quot;_id&quot; : 1,
    	&quot;version&quot; : 4,
        &quot;minCompatibleVersion&quot; : 4,
    	&quot;currentVersion&quot; : 5,
    	&quot;clusterId&quot; : ObjectId(&quot;54eb010dc6d2e5d19ca9df05&quot;)
}
  shards:
    	{  &quot;_id&quot; : &quot;shard0000&quot;,  &quot;host&quot; : &quot;PSINFW95:27000&quot; }
    	{  &quot;_id&quot; : &quot;shard0001&quot;,  &quot;host&quot; : &quot;PSINFW95:27001&quot; }
  databases:
    	{  &quot;_id&quot; : &quot;admin&quot;,  &quot;partitioned&quot; : false,  &quot;primary&quot; : &quot;config&quot; }
    	{  &quot;_id&quot; : &quot;test&quot;,  &quot;partitioned&quot; : false,  &quot;primary&quot; : &quot;shard0000&quot; }
    	{  &quot;_id&quot; : &quot;shardTestDB&quot;,  &quot;partitioned&quot; : true,  &quot;primary&quot; : &quot;shard0000&quot; }
            	shardTestDB.users
                    	shard key: { &quot;username&quot; : 1 }
                    	chunks:
                                shard0001   	10
                                shard0000   	11
                    	{ &quot;username&quot; : { &quot;$minKey&quot; : 1 } } --&gt;&gt; { &quot;username&quot; : &quot;user0&quot; } on : shard0001 Timestamp(2, 0)
                    	{ &quot;username&quot; : &quot;user0&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user1421&quot; } on : shard0001 Timestamp(3, 0)
                    	{ &quot;username&quot; : &quot;user1421&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user18423&quot; } on : shard0001 Timestamp(4, 0)
                    	{ &quot;username&quot; : &quot;user18423&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user22636&quot; } on : shard0001 Timestamp(5, 0)
                    	{ &quot;username&quot; : &quot;user22636&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user3131&quot; } on : shard0001 Timestamp(6, 0)
                    	{ &quot;username&quot; : &quot;user3131&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user35523&quot; } on : shard0001 Timestamp(7, 0)
                    	{ &quot;username&quot; : &quot;user35523&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user39737&quot; } on : shard0001 Timestamp(8, 0)
                    	{ &quot;username&quot; : &quot;user39737&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user4395&quot; } on : shard0001 Timestamp(9, 0)
                    	{ &quot;username&quot; : &quot;user4395&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user48162&quot; } on : shard0001 Timestamp(10, 0)
                 	   { &quot;username&quot; : &quot;user48162&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user5314&quot; } on : shard0001 Timestamp(11, 0)
                    	{ &quot;username&quot; : &quot;user5314&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user57353&quot; } on : shard0000 Timestamp(11, 1)
                    	{ &quot;username&quot; : &quot;user57353&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user61566&quot; } on : shard0000 Timestamp(1, 25)
                    	{ &quot;username&quot; : &quot;user61566&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user6578&quot; } on : shard0000 Timestamp(1, 27)
                    	{ &quot;username&quot; : &quot;user6578&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user69993&quot; } on : shard0000 Timestamp(1, 29)
                    	{ &quot;username&quot; : &quot;user69993&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user74204&quot; } on : shard0000 Timestamp(1, 31)
                    	{ &quot;username&quot; : &quot;user74204&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user78418&quot; } on : shard0000 Timestamp(1, 33)
                    	{ &quot;username&quot; : &quot;user78418&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user82630&quot; } on : shard0000 Timestamp(1, 35)
                    	{ &quot;username&quot; : &quot;user82630&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user86844&quot; } on : shard0000 Timestamp(1, 37)
                    	{ &quot;username&quot; : &quot;user86844&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user91056&quot; } on : shard0000 Timestamp(1, 39)
                    	{ &quot;username&quot; : &quot;user91056&quot; } --&gt;&gt; { &quot;username&quot; : &quot;user999&quot; } on : shard0000 Timestamp(1, 40)
                    	{ &quot;username&quot; : &quot;user999&quot; } --&gt;&gt; { &quot;username&quot; : { &quot;$maxKey&quot; : 1 } } on : shard0000 Timestamp(1, 4)
 
mongos&gt;&lt;/pre&gt;&lt;p&gt;If you are asking yourself how do I know the shard in which the data I need is stored?, do not worry, you only have to request it to the mongos and it will retrieve it for you.&lt;/p&gt;
&lt;p&gt;This is the end of the post, I wish that you have understood all the steps and you can get the most of your MongoDB Sharded Cluster.&lt;/p&gt;</description>
	<pubDate>Tue, 03 Mar 2015 11:00:19 +0000</pubDate>
</item>
<item>
	<title>Tim Callaghan: Bad Benchmarketing and the Bar Chart</title>
	<guid>tag:blogger.com,1999:blog-8043938871710850997.post-9200339677114730708</guid>
	<link>http://www.acmebenchmarking.com/2015/03/benchmarketing-charts-hurt-us-all.html</link>
	<description>&lt;span&gt;Technical conferences are flooded with visual [mis]representations of a particular product's performance, compression, cost effectiveness, micro-transactions per flux-capacitor, or whatever two-axis comparison someone dreams up. &lt;/span&gt;&lt;span&gt;Lets be honest, benchmarketers like to believe we all suffer from &lt;a href=&quot;http://www.merriam-webster.com/dictionary/innumeracy&quot;&gt;innumeracy&lt;/a&gt;. &lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;The &lt;a href=&quot;http://www.merriam-webster.com/&quot;&gt;Merriam-Webster dictionary&lt;/a&gt; defines innumeracy as follows:&lt;/span&gt;&lt;br /&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;span&gt;&lt;i&gt;&lt;span class=&quot;ssens&quot;&gt;innumeracy (noun): marked by an ignorance of mathematics and the scientific approach &lt;/span&gt;&lt;/i&gt;&lt;/span&gt;&lt;/blockquote&gt;&lt;span&gt;&lt;a href=&quot;http://smalldatum.blogspot.com/&quot;&gt;Mark Callaghan&lt;/a&gt; has been a long time advocate of &lt;a href=&quot;http://smalldatum.blogspot.com/2014/06/benchmarketing.html&quot;&gt;explaining benchmark results&lt;/a&gt;, but that's not the point of the bar chart. Oh no, the bar chart only exists to catch your eye and draw you into the booth for further conversation.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;I was attending a large name-brand conference in 2014. A well-known hardware vendor was presenting one of the keynotes. A few slides into the deck and it was &quot;Benchmark Time!&quot;, so up came the following bar chart.&lt;/span&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://2.bp.blogspot.com/-keoyMEm27u4/VPRm_h33YJI/AAAAAAAAB-E/Q89pNWP-CuE/s1600/benchmarketing-graphs-01-original.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://2.bp.blogspot.com/-keoyMEm27u4/VPRm_h33YJI/AAAAAAAAB-E/Q89pNWP-CuE/s1600/benchmarketing-graphs-01-original.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;span&gt;The visualization of their data is quite dramatic. Their product, the &quot;us&quot; bar, delivering a substantially higher &quot;% Improvement&quot; over their competitions, the &quot;them&quot; bar. On a quick glance your mind tells you, &quot;wow, their product is almost 3x better than the competition&quot;. And a quick glance is all you get because presenters typically spend less than 60 seconds per slide, even less in a keynote. I've been waiting to catch this type of benchmarketer in the wild, so I quickly pulled out my phone and took a picture.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Lets break down the events that led to the above bar chart. Long before the keynote the vendor in question asked someone on their technical staff to create a scenario (benchmark) comparing their product to the competition. The request usually includes something like the following, &quot;Make sure we are measurably better than them, but not so much that people won't believe it&quot;. So the technical resource goes away, creates the benchmark, executes it, and presents the following data to the Marketing department.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code&gt;us     58.5&lt;br /&gt;them   49.0&lt;/code&gt;&lt;/pre&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Now keep in mind, the &quot;us&quot; number of 58.5 &lt;i&gt;&lt;b&gt;is only 19.3% higher&lt;/b&gt;&lt;/i&gt; than the &quot;them&quot; number of 49.0. A 20% improvement in an important system metric might be huge for certain use-cases, but its not that compelling for general consumption, especially during a keynote. So marketing gets to work with the &quot;data&quot;, &lt;i&gt;which almost seems silly given that the data consists of exactly two numbers&lt;/i&gt;.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Now any good Marketer will generally fire up Microsoft Excel and see what they can do with this data. &lt;i&gt;Indeed it is almost comical to call it data, this scenario is actually just 2 values.&lt;/i&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;First up is what I call the purely scientific graph. Setting the y-axis range to the possible values (lets use 0 to 100 for this scenario) creates the following graph.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://3.bp.blogspot.com/-LC5J9w8kREw/VPRsIIDsIaI/AAAAAAAAB-U/RfjhYNNHQiY/s1600/benchmarketing-graphs-04-0-to-100.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://3.bp.blogspot.com/-LC5J9w8kREw/VPRsIIDsIaI/AAAAAAAAB-U/RfjhYNNHQiY/s1600/benchmarketing-graphs-04-0-to-100.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;span&gt;Needless to say, this chart doesn't make the cut. There is too little visual difference between the two bars.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;So next up is an attempt to re-chart the data with a still scientific approach, what I like to call the &quot;we are the best&quot; chart.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://2.bp.blogspot.com/-BiDfiuIf9lQ/VPRsvT78dII/AAAAAAAAB-c/ahUCfGgqIqQ/s1600/benchmarketing-graphs-03-0-to-65.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://2.bp.blogspot.com/-BiDfiuIf9lQ/VPRsvT78dII/AAAAAAAAB-c/ahUCfGgqIqQ/s1600/benchmarketing-graphs-03-0-to-65.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;span&gt;The goal of this chart is to set something slightly larger than our value as the maximum y value but keep the minimum value at 0, thus making the difference between &quot;us&quot; and &quot;them&quot; more apparent. As with the prior graph, this one is rejected as our awesomeness is not properly conveyed.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;So it's time to get extreme, and create the &quot;world domination&quot; graph. I've never seen one of these in the wild, but it's just a matter of time.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://1.bp.blogspot.com/-B4mai6IhVwg/VPRtXt6bY4I/AAAAAAAAB-k/IeM-bhsssBw/s1600/benchmarketing-graphs-02-48-to-59.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://1.bp.blogspot.com/-B4mai6IhVwg/VPRtXt6bY4I/AAAAAAAAB-k/IeM-bhsssBw/s1600/benchmarketing-graphs-02-48-to-59.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;span&gt;This graph uses a value slightly larger than the &quot;us&quot; as the maximum y-axis value and something slightly smaller than the &quot;them&quot; as the minimum y-axis value. The results are stunning, we're talking order-of-magnitude improvements now. &lt;b&gt;Well done!&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;At this point the presentation starts coming together with with the above slide. Inevitably someone in engineering walks by a printer, sees the chart, and freaks out. Engineering and Marketing negotiate a peaceful settlement and we end up with the chart at the top of this blog. Not ideal, but certainly better than what might have been presented. Benchmarketing for-the-win!&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;So that's it, hopefully that explains the process. Perhaps you're now a little better prepared to question what you see, and question you should. Don't be innumerate.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;I want to create a page on AcmeBenchmarking with a Benchmarketing Hall of Fame, so please send along any pictures or URLs of the bad benchmarketing &lt;/span&gt;&lt;span&gt;you've seen.&amp;nbsp;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;Also get involved in the comments. Any Marketers have a contrary opinion?&lt;/span&gt;</description>
	<pubDate>Mon, 02 Mar 2015 13:00:06 +0000</pubDate>
	<author>noreply@blogger.com (Tim Callaghan)</author>
</item>
<item>
	<title>MongoDB Management Service: Introduction to Import Existing Deployment for Automation</title>
	<guid>http://blog.mms.mongodb.com/post/111860709590</guid>
	<link>http://blog.mms.mongodb.com/post/111860709590</link>
	<description>&lt;h2&gt;Why Import for Automation?&lt;/h2&gt;

&lt;p&gt;Until today, it was only possible to manage new deployments with MMS Automation. Starting today, you can get the benefits of Automation on the vast majority of your pre-existing deployments. Automation allows you to turn long and complicated manual processes into single click operations with MMS.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve tried to minimize the amount of work you&amp;rsquo;ll need to do in order to attach MMS to your pre-existing deployments. In some cases some re-configuration will be necessary, and some deployment types aren&amp;rsquo;t yet supported, but stay tuned.&lt;/p&gt;

&lt;h2&gt;Getting Started&lt;/h2&gt;

&lt;p&gt;To successfully import, please check your deployment meets the following requirements:&lt;/p&gt;

&lt;ol&gt;&lt;li&gt;You have a Monitoring Agent and your deployment is monitored. If you have no MMS Agents running:
    &lt;ul&gt;&lt;li&gt;Install an Automation Agent on every server in the deployment. Instructions for installing Automation Agents can be found at Administration -&amp;gt; Agents&lt;/li&gt;
      &lt;li&gt;Use MMS Automation to deploy a Monitoring Agent. &lt;a href=&quot;https://docs.mms.mongodb.com/tutorial/move-agent-to-new-server/&quot; target=&quot;_blank&quot;&gt;See docs here.&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;&lt;/li&gt;

  &lt;li&gt;The fully qualified domain name (&amp;ldquo;hostname -f&amp;rdquo;) for each server in the deployment must be resolvable from every other server. For example, if you login to Server A, and &amp;ldquo;hostname -f&amp;rdquo; returns a.example.com, then this address must be resolvable from Server B, C and D as well.&lt;/li&gt;

  &lt;li&gt;If you are using MongoDB authentication, you must configure a MongoDB user account that the Automation Agents will use to authenticate to your deployment.
    &lt;pre class=&quot;“prettyprint” prettyprinted&quot;&gt;
      use admin
      db.createUser({user: &quot;mms-automation&quot;, pwd: &quot;yourpassword&quot;, roles: ['clusterAdmin', 'dbAdminAnyDatabase', 'readWriteAnyDatabase', 'userAdminAnyDatabase']})
    &lt;/pre&gt;
    &lt;em&gt;Please note that if you are importing a sharded cluster, this user must be created via the mongos, as well as on every shard.  The same username and password must be used throughout.&lt;/em&gt;
  &lt;/li&gt;

  &lt;li&gt;Your deployment makes use only of supported MongoDB configuration file options. &lt;a href=&quot;https://docs.mms.mongodb.com/reference/deployment-advanced-options/&quot; target=&quot;_blank&quot;&gt;See docs here.&lt;/a&gt;&lt;/li&gt;

  &lt;li&gt;The MongoDB processes that are being imported must be running as the same system user as the Automation Agent.  For example, if your MongoDB process is running as the &amp;ldquo;mongod&amp;rdquo; user, the Automation Agent must also be run as the &amp;ldquo;mongod&amp;rdquo; user.&lt;/li&gt;
  
  &lt;li&gt;If you wish to import multiple replica sets or clusters that are using MongoDB authentication, they must all use the same keyFile.&lt;/li&gt;

&lt;/ol&gt;&lt;h2&gt;How It Works&lt;/h2&gt;

&lt;p&gt;To start the process, click on the green Add button and choose Import Existing for Automation.&lt;/p&gt;

&lt;p&gt;The first step is to choose the item you would like to import.  You will be presented with a list of all the sharded clusters, replica sets and standalones that you are currently monitoring in MMS. In this example, we&amp;rsquo;re going to import a replica set called &amp;ldquo;leaf&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://41.media.tumblr.com/a0bd924f894624814ed9a20d7a236e2b/tumblr_nk8es2j7lW1sdaytmo3_1280.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://41.media.tumblr.com/a0bd924f894624814ed9a20d7a236e2b/tumblr_nk8es2j7lW1sdaytmo3_1280.png&quot; height=&quot;358&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you are using MongoDB authentication, you will additionally be asked to enter the username and password for the MongoDB user account you prepared for the Automation Agent user.&lt;/p&gt;

&lt;p&gt;Once you click Start Import, the Automation Agents will query each process for detailed information about its current state.  The Automation Agents will also gather information about any users and custom roles defined in your deployment.  This information is sent back to MMS, which uses this information to construct a detailed map of your deployment.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://40.media.tumblr.com/c489d5118a5d7f4da53e88b5932d51e5/tumblr_nk8es2j7lW1sdaytmo4_1280.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://40.media.tumblr.com/c489d5118a5d7f4da53e88b5932d51e5/tumblr_nk8es2j7lW1sdaytmo4_1280.png&quot; height=&quot;358&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;When this process is complete, you will be invited to view the draft deployment:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://40.media.tumblr.com/ceae6850e6656025a6264faacabbc865/tumblr_nk8es2j7lW1sdaytmo5_1280.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://40.media.tumblr.com/ceae6850e6656025a6264faacabbc865/tumblr_nk8es2j7lW1sdaytmo5_1280.png&quot; height=&quot;358&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The draft deployment might look like this:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://40.media.tumblr.com/34f63e994678886ff6434b3d65757b7f/tumblr_nk8es2j7lW1sdaytmo6_1280.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://40.media.tumblr.com/34f63e994678886ff6434b3d65757b7f/tumblr_nk8es2j7lW1sdaytmo6_1280.png&quot; height=&quot;358&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The final step is to &amp;ldquo;Review &amp;amp; Deploy&amp;rdquo; your changes.  After clicking this you will be given one last preview, and then configuration will be broadcast to the Automation Agents.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://40.media.tumblr.com/dad2132425aeddfbbcdd9a2f4fc9b631/tumblr_nk8es2j7lW1sdaytmo1_1280.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://40.media.tumblr.com/dad2132425aeddfbbcdd9a2f4fc9b631/tumblr_nk8es2j7lW1sdaytmo1_1280.png&quot; height=&quot;358&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;When the Automation Agents receive the new configuration, they will initiate a rolling restart of the new processes.  When this completes, your deployment will be in &amp;ldquo;Goal state&amp;rdquo;.&lt;/p&gt;

&lt;h2&gt;Uh-Oh, It Doesn&amp;rsquo;t Seem To Be Working&lt;/h2&gt;

&lt;p&gt;When the Automation Agents gather information about the state of your deployment, we also perform extensive validation to ensure that your deployment is suitable for import.&lt;/p&gt;

&lt;p&gt;However, if we missed something, and things don&amp;rsquo;t seem to be going well, you can always Unmanage any item from your deployment.  When you Unmanage the item, it is removed from the configuration for the Automation Agents.  The Automation Agents will not shutdown the processes, they will simply stop interacting with them.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://41.media.tumblr.com/7aa8a4845e2d273c0c64c6b371cd959e/tumblr_nk8es2j7lW1sdaytmo2_1280.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://41.media.tumblr.com/7aa8a4845e2d273c0c64c6b371cd959e/tumblr_nk8es2j7lW1sdaytmo2_1280.png&quot; height=&quot;358&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/p&gt;</description>
	<pubDate>Mon, 23 Feb 2015 14:25:00 +0000</pubDate>
</item>
<item>
	<title>Tim Callaghan: How to benchmark MongoDB</title>
	<guid>tag:blogger.com,1999:blog-8043938871710850997.post-5740379561084135333</guid>
	<link>http://www.acmebenchmarking.com/2015/02/how-to-benchmark-mongodb.html</link>
	<description>&lt;span&gt;There are generally three components to any benchmark project:&lt;/span&gt;&lt;br /&gt;&lt;ol&gt;&lt;li&gt;&lt;span&gt;Create the benchmark application&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Execute it&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Publish your results&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;&lt;span&gt;I assume many people think they want to run more benchmarks but give up since step 2 is extremely consuming as you expand the number of different configurations/scenarios.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;I'm hoping that this blog post will encourage more people to dive-in and participate, as I'll be sharing the bash script I used to test the &lt;a href=&quot;http://www.acmebenchmarking.com/2015/02/mongodb-v30-compression-benchmarks.html&quot; target=&quot;_blank&quot;&gt;various compression options coming in the MongoDB 3.0 storage engines&lt;/a&gt;. It enabled me to run a few different tests against 8 different configurations, recording insertion speed and size-on-disk for each one.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;If you're into this sort of thing, please read on and provide any feedback or improvements you can think of. You also might want to grab a Snickers, as there is a lot to cover. I've commented along the way so hopefully it is an interesting read. Also, links to the full script and configuration files are at the bottom of the blog. Lets get started!&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code&gt;#!/bin/bash&lt;br /&gt;&lt;br /&gt;# remember the directory we are starting from&lt;br /&gt;#   the script expects the MongoDB configuration files&lt;br /&gt;export homeDirectory=$PWD&lt;br /&gt;&lt;br /&gt;# directory where MongoDB/TokuMX tarballs are located&lt;br /&gt;export tarDirectory=${BACKUP_DIR}/mongodb&lt;br /&gt;&lt;br /&gt;# directory used for MongoDB server binaries and data folder&lt;br /&gt;export MONGO_DIR=~/temp&lt;br /&gt;&lt;br /&gt;# perform some sanity checks&lt;br /&gt;&lt;br /&gt;# check that $MONGO_DIR is defined&lt;br /&gt;if [ -z &quot;$MONGO_DIR&quot; ]; then&lt;br /&gt;    echo &quot;Need to set MONGO_DIR&quot;&lt;br /&gt;    exit 1&lt;br /&gt;fi&lt;br /&gt;&lt;br /&gt;# check that $MONGO_DIR exists&lt;br /&gt;if [ ! -d &quot;$MONGO_DIR&quot; ]; then&lt;br /&gt;    echo &quot;Need to create directory $MONGO_DIR&quot;&lt;br /&gt;    exit 1&lt;br /&gt;fi&lt;br /&gt;&lt;br /&gt;# check that $MONGO_DIR is empty&lt;br /&gt;#   force manual cleanup before starting&lt;br /&gt;if [ &quot;$(ls -A ${MONGO_DIR})&quot; ]; then&lt;br /&gt;   echo &quot;Directory $MONGO_DIR must be empty before starting&quot;&lt;br /&gt;   exit 1&lt;br /&gt;fi&lt;/code&gt;&lt;/pre&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;I'm a big fan of two things at the top of all my scripts: directory locations and sanity checks. The three directories needed for this particular benchmark run are as follows:&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;homeDirectory = The directory from where we are executing the script.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;tarDirectory = The directory where the tar files exist for the various MongoDB flavors/versions that we are benchmarking. You'll likely need to change this for your benchmarks.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;MONGO_DIR = The directory where we'll be unpacking the tar files (to execute the mongod binary) as well as creating a directory for storing the data for the benchmark. Make sure this is on decent storage is you are running a performance benchmark, a single SATA drive isn't fast. You'll likely need to change this for your benchmarks.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;The sanity checks follow, we want to make sure that $MONGO_DIR is defined (just in case), the the $MONGO_DIR directory exists, and that the $MONGO_DIR directory is empty. The empty check is something I think is important, you might have something interesting in that directory and should manually clear it out before starting the benchmark.&lt;/span&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;pre&gt;&lt;code&gt;# decide which tarballs and configurations we want to benchmark&lt;br /&gt;#   use semi-colon list of &quot;tarball;id;config;mongo_type&quot;&lt;br /&gt;#     tarball     : MongoDB or TokuMX tarball&lt;br /&gt;#     id          : Short hand description of this particular benchmark run, ends up in the log file and the summary log&lt;br /&gt;#     config      : YAML configuration file to use for the this benchmark run&lt;br /&gt;#     mongo_type  : Identifies which &quot;type&quot; of MongoDB, tokumx|mxse|wt|mongo&lt;br /&gt;export benchmarkList=&quot;&quot;&lt;br /&gt;export benchmarkList=&quot;${benchmarkList} mongodb-linux-x86_64-tokumxse-1.0.0-rc.2.tgz;mxse_100rc2_none;tokumxse-uncompressed.conf;mxse&quot;&lt;br /&gt;export benchmarkList=&quot;${benchmarkList} mongodb-linux-x86_64-tokumxse-1.0.0-rc.2.tgz;mxse_100rc2_quicklz;tokumxse-quicklz.conf;mxse&quot;&lt;br /&gt;export benchmarkList=&quot;${benchmarkList} mongodb-linux-x86_64-tokumxse-1.0.0-rc.2.tgz;mxse_100rc2_zlib;tokumxse-zlib.conf;mxse&quot;&lt;br /&gt;export benchmarkList=&quot;${benchmarkList} mongodb-linux-x86_64-tokumxse-1.0.0-rc.2.tgz;mxse_100rc2_lzma;tokumxse-lzma.conf;mxse&quot;&lt;br /&gt;export benchmarkList=&quot;${benchmarkList} mongodb-linux-x86_64-3.0.0-rc8.tgz;mmapv1_300rc8;mmapv1.conf;mongo&quot;&lt;br /&gt;export benchmarkList=&quot;${benchmarkList} mongodb-linux-x86_64-3.0.0-rc8.tgz;wt_300rc8_none;wiredtiger-uncompressed.conf;wt&quot;&lt;br /&gt;export benchmarkList=&quot;${benchmarkList} mongodb-linux-x86_64-3.0.0-rc8.tgz;wt_300rc8_snappy;wiredtiger-snappy.conf;wt&quot;&lt;br /&gt;export benchmarkList=&quot;${benchmarkList} mongodb-linux-x86_64-3.0.0-rc8.tgz;wt_300rc8_zlib;wiredtiger-zlib.conf;wt&quot;&lt;/code&gt;&lt;/pre&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Benchmarking is usually a single test run against multiple scenarios, and this is the section where we define those scenarios. The benchmarkList variable starts empty and is then appended with one or more scenarios. The scenario information is broken down into 4 segments, each delimited by a semi-colon. The comment above it is self-explanatory but worth explaining is the fourth segment, mongo_type. This script doesn't care what specific &quot;MongoDB&quot; you are running, but others I've created do so I always define it should I want it somewhere else.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code&gt;# make sure we have valid tarballs and config scripts for this benchmark run&lt;br /&gt;echo &quot;checking that all needed tarballs exist.&quot;&lt;br /&gt;for thisBenchmark in ${benchmarkList}; do&lt;br /&gt;    TARBALL=$(echo &quot;${thisBenchmark}&quot; | cut -d';' -f1)&lt;br /&gt;    MONGOD_CONFIG=$(echo &quot;${thisBenchmark}&quot; | cut -d';' -f3)&lt;br /&gt;&lt;br /&gt;    if [ -e ${tarDirectory}/${TARBALL} ]; then&lt;br /&gt;        echo &quot;  located ${tarDirectory}/${TARBALL}&quot;&lt;br /&gt;    else&lt;br /&gt;        echo &quot;  unable to locate ${tarDirectory}/${TARBALL}, exiting.&quot;&lt;br /&gt;        exit 1&lt;br /&gt;    fi&lt;br /&gt;&lt;br /&gt;    if [ -e ${MONGOD_CONFIG} ]; then&lt;br /&gt;        echo &quot;  located ${MONGOD_CONFIG}&quot;&lt;br /&gt;    else&lt;br /&gt;        echo &quot;  unable to locate ${MONGOD_CONFIG}, exiting.&quot;&lt;br /&gt;        exit 1&lt;br /&gt;    fi&lt;br /&gt;done&lt;/code&gt;&lt;/pre&gt;&lt;br /&gt;&lt;span&gt;More sanity checking here. Before running any benchmarks we want to make sure that all the tar files and configuration files actually exist on the server. Nothing is more disappointing than starting a long running series of benchmarks only to come back in a day and find that some of them failed because of a type or missing file.&lt;/span&gt; &lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code&gt;export DB_NAME=test&lt;br /&gt;export NUM_CLIENTS=2&lt;br /&gt;export DOCS_PER_CLIENT=$((512 * 80000))&lt;br /&gt;export NUM_INSERTS=$((NUM_CLIENTS * DOCS_PER_CLIENT))&lt;br /&gt;export SUMMARY_LOG_NAME=summary.log&lt;br /&gt;rm -f ${SUMMARY_LOG_NAME}&lt;/code&gt;&lt;/pre&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;This section allows some control over the benchmark itself, plus gives us information needed for interpreting some of the results.&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;DB_NAME = The MongoDB database we'll be inserting into.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;NUM_CLIENTS = The number of simultaneous insert clients. You can set this to any value &amp;gt;= 1, if you set it to &amp;lt; 1 you'll still get a single insert client.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;DOCS_PER_CLIENT = The number of documents a single client will insert. This is multiplied by NUM_CLIENTS to find the total number of inserts (NUM_INSERTS), and is needed to calculate inserts per second later in the script. This value of 512 * 80000 is taken directly from the Javascript code, I'd normally inject it for the benchmark but didn't due to a lack of time.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;NUM_INSERTS = Total number of inserts for the benchmark, a cooler way to do this would be to get a count from the collection itself, but that might take a while if an exact count is important and the particular storage engine supports document level locking. And remember, benchmarking isn't always about being cool, efficiency counts too.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;SUMMARY_LOG_NAME = A single log file that will contain all results, summarized. And yes, delete it if it exists.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;&lt;pre&gt;&lt;code&gt;for thisBenchmark in ${benchmarkList}; do&lt;br /&gt;    export TARBALL=$(echo &quot;${thisBenchmark}&quot; | cut -d';' -f1)&lt;br /&gt;    export MINI_BENCH_ID=$(echo &quot;${thisBenchmark}&quot; | cut -d';' -f2)&lt;br /&gt;    export MONGOD_CONFIG=$(echo &quot;${thisBenchmark}&quot; | cut -d';' -f3)&lt;br /&gt;    export MONGO_TYPE=$(echo &quot;${thisBenchmark}&quot; | cut -d';' -f4)&lt;br /&gt;&lt;br /&gt;    echo &quot;benchmarking tarball = ${TARBALL}&quot;&lt;/code&gt;&lt;/pre&gt;&lt;br /&gt;&lt;span&gt;Start the loop where we benchmark each scenario by grabbing each one and cutting it into the four components. Give the user a heads up as to which TARBALL we're benchmarking this time.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code&gt;    # clean up + start the new server&lt;br /&gt;    &lt;br /&gt;    pushd ${MONGO_DIR}&lt;br /&gt;    if [ &quot;$?&quot; -eq 1 ]; then&lt;br /&gt;        echo &quot;Unable to pushd $MONGO_DIR, exiting.&quot;&lt;br /&gt; exit 1&lt;br /&gt;    fi&lt;br /&gt;    &lt;br /&gt;    # erase any files from the previous run&lt;br /&gt;    rm -rf *&lt;br /&gt;    &lt;br /&gt;    # untar server binaries to here&lt;br /&gt;    tar xzvf ${tarDirectory}/${TARBALL} --strip 1&lt;br /&gt;    &lt;br /&gt;    # create the &quot;data&quot; directory&lt;br /&gt;    mkdir data&lt;br /&gt;    bin/mongod --config ${homeDirectory}/${MONGOD_CONFIG}&lt;br /&gt;    popd&lt;/code&gt;&lt;/pre&gt;&lt;br /&gt;&lt;span&gt;Did I mention how defensive I try to write these benchmarking scripts? Maybe paranoid is a better term. Earlier we confirmed that MONGO_DIR is defined, exists as a directory, and is empty. Guess what? Something might go terribly wrong during the benchmark and that might no longer be the case. So right after changing to the MONGO_DIR directory using pushd, check that pushd succeeded. Erase any existing files in the directory, untar the current benchmark's tarball, create a data folder, start MongoDB with the current scenario's configuration file, and popd back to our starting directory.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code&gt;    # wait for mongo to start&lt;br /&gt;    while [ 1 ]; do&lt;br /&gt;        $MONGO_DIR/bin/mongostat -n 1 &amp;gt; /dev/null 2&amp;gt;&amp;amp;1&lt;br /&gt;        if [ &quot;$?&quot; -eq 0 ]; then&lt;br /&gt;            break&lt;br /&gt;        fi    &lt;br /&gt;        sleep 5&lt;br /&gt;    done&lt;br /&gt;    sleep 5&lt;/code&gt;&lt;/pre&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;We are starting mongod forked, so the MongoDB server isn't yet available. This code executes until the mongostat utility returns data, letting us know that the server is running.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Any ideas on a cleaner way to do this?&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code&gt;    # log for this run&lt;br /&gt;    export LOG_NAME=${MINI_BENCH_ID}-${NUM_CLIENTS}-${NUM_INSERTS}.log&lt;br /&gt;    rm -f ${LOG_NAME}&lt;/code&gt;&lt;/pre&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Create a custom log file for this particular scenario.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code&gt;    # TODO : log server performance with mongostat&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;If you've ever attending one of my benchmark presentations you've likely heard me say that benchmarking is never done, there is always more to measure and analyze. This script currently records overall (cumulative) inserts per second, catching mongostat output along the way would allow for creating a pretty graph over time. I highly recommend picking a way to add &quot;to-do&quot; tasks to your scripts and code, mine is as simple as &quot;TODO : &quot;.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code&gt;    # start the first inserter&lt;br /&gt;    T=&quot;$(date +%s)&quot;&lt;br /&gt;    echo &quot;`date` | starting insert client 1&quot; | tee -a ${LOG_NAME}&lt;br /&gt;    $MONGO_DIR/bin/mongo ${DB_NAME} --eval 'load(&quot;./compress_test.js&quot;)' &amp;amp;&lt;br /&gt;    sleep 5&lt;/code&gt;&lt;/pre&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;This particular benchmark is simple Javascript, so we execute it using the mongo shell. Prior to starting the client we grab the current time (probably the number of seconds since the epoch) so we can calculate the total inserts per second. I include a &quot;sleep 5&quot; after this first client since it might take a bit of time for the collection to get created, I've found it's always safest to let the first insert client get started on it's own.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;Again, thanks to &lt;a href=&quot;https://twitter.com/comerford&quot;&gt;Adam&lt;/a&gt; &lt;a href=&quot;http://comerford.cc/&quot;&gt;Comerford&lt;/a&gt; for sharing &lt;a href=&quot;https://comerford.cc/2015/02/04/mongodb-3-0-testing-compression/&quot;&gt;this benchmark&lt;/a&gt;. &lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code&gt;    # start the additional insert clients&lt;br /&gt;    clientNumber=2&lt;br /&gt;    while [ ${clientNumber} -le ${NUM_CLIENTS} ]; do&lt;br /&gt;        echo &quot;`date` | starting insert client ${clientNumber}&quot; | tee -a ${LOG_NAME}&lt;br /&gt;        $MONGO_DIR/bin/mongo ${DB_NAME} --eval 'load(&quot;./compress_test.js&quot;)' &amp;amp;&lt;br /&gt;        let clientNumber=clientNumber+1&lt;br /&gt;    done&lt;/code&gt;&lt;/pre&gt;&lt;br /&gt;&lt;span&gt;If we are running 2 or more insert clients then each gets started with this loop.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code&gt;    # wait for all of the client(s) to finish&lt;br /&gt;    wait&lt;/code&gt;&lt;/pre&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;I only learned about the wait command a few months ago, and it is extremely useful. It causes our script to pause (wait) until any children processes we created are finished. So for this example each of the insert clients will finish before the script continues.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code&gt;    # report insert performance&lt;br /&gt;    T=&quot;$(($(date +%s)-T))&quot;&lt;br /&gt;    printf &quot;`date` | insert duration = %02d:%02d:%02d:%02d\n&quot; &quot;$((T/86400))&quot; &quot;$((T/3600%24))&quot; &quot;$((T/60%60))&quot; &quot;$((T%60))&quot; | tee -a ${LOG_NAME}&lt;br /&gt;    DOCS_PER_SEC=`echo &quot;scale=0; ${NUM_INSERTS}/${T}&quot; | bc `&lt;br /&gt;    echo &quot;`date` | inserts per second = ${DOCS_PER_SEC}&quot; | tee -a ${LOG_NAME}&lt;/code&gt;&lt;/pre&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Now that the inserts are finished we find the number of elapsed seconds by subtracting the current seconds (from the epoch) from our starting time. Calculating inserts per second is a simple as dividing the number of inserts by the number of seconds.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code&gt;    # stop the server&lt;br /&gt;    T=&quot;$(date +%s)&quot;&lt;br /&gt;    echo &quot;`date` | shutting down the server&quot; | tee -a ${LOG_NAME}&lt;br /&gt;    $MONGO_DIR/bin/mongo admin --eval &quot;db.shutdownServer({force: true})&quot;&lt;br /&gt;&lt;br /&gt;    # wait for the MongoDB server to shutdown&lt;br /&gt;    while [ 1 ]; do&lt;br /&gt;        pgrep -U $USER mongod &amp;gt; /dev/null 2&amp;gt;&amp;amp;1&lt;br /&gt;        if [ &quot;$?&quot; -eq 1 ]; then&lt;br /&gt;            break&lt;br /&gt;        fi    &lt;br /&gt;        sleep 5&lt;br /&gt;    done&lt;br /&gt;    T=&quot;$(($(date +%s)-T))&quot;&lt;br /&gt;    printf &quot;`date` | shutdown duration = %02d:%02d:%02d:%02d\n&quot; &quot;$((T/86400))&quot; &quot;$((T/3600%24))&quot; &quot;$((T/60%60))&quot; &quot;$((T%60))&quot; | tee -a ${LOG_NAME}&lt;/code&gt;&lt;/pre&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Prior to calculating size on disk I like to stop the server, since that allows each storage engine to perform cleanup, flush old log files, and shut down cleanly. I also like to time the operation. It's always bothered me that the MongoDB server shutdown process is asynchronous, the client requesting the shutdown is immediately disconnected with an unfriendly warning message (which one might mistake for an error).&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;In any event, the loop immediately following the db.shutdownServer() call is there to wait for the mongod process to disappear. Until it does, MongoDB is not really stopped.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Any ideas on how to improve this?&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code&gt;    # report size on disk&lt;br /&gt;    SIZE_BYTES=`du -c --block-size=1 ${MONGO_DIR}/data | tail -n 1 | cut -f1`&lt;br /&gt;    SIZE_MB=`echo &quot;scale=2; ${SIZE_BYTES}/(1024*1024)&quot; | bc `&lt;br /&gt;    echo &quot;`date` | post-load sizing (SizeMB) = ${SIZE_MB}&quot; | tee -a ${LOG_NAME}&lt;/code&gt;&lt;/pre&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Find and report the total megabytes of the data directory (dbPath). I usually only report on the specific collection and it's indexes, this is simpler in that it includes the entire data directory.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;code&gt;    # put all the information into the summary log file&lt;br /&gt;    echo &quot;`date` | tech = ${MINI_BENCH_ID} | ips = ${DOCS_PER_SEC} | sizeMB = ${SIZE_MB}&quot; | tee -a ${SUMMARY_LOG_NAME}&lt;br /&gt;done&lt;/code&gt;&lt;/pre&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Having all the results go to a single summary log file make it easy to interpret and graph your results.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;So there you have it. Download the script and configuration files, make some changes, and run a few tests for yourself. Oh, give me some feedback if you can think of areas I can improve the above.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;&lt;span&gt;You are well on your way to your benchmarking black belt!&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;i&gt;&lt;b&gt;&lt;span&gt;&lt;span&gt;Links to everything you'll need to try this at home.&lt;/span&gt;&lt;/span&gt;&lt;/b&gt;&lt;/i&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;span&gt;&lt;a href=&quot;https://gist.github.com/tmcallaghan/4d15c18f9e34e1ea8a21&quot;&gt;run.benchmark.bash&lt;/a&gt; (the script we picked apart in this blog)&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://gist.github.com/tmcallaghan/d6690529c62756ce32d7&quot;&gt;&lt;span&gt;&lt;span&gt;mmapv1.conf&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://gist.github.com/tmcallaghan/68aa2ab4fb07cd138537&quot;&gt;&lt;span&gt;&lt;span&gt;wiredtiger-uncompressed.conf&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://gist.github.com/tmcallaghan/3591fa29f8d9ce1e0178&quot;&gt;&lt;span&gt;&lt;span&gt;wiredtiger-snappy.conf&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://gist.github.com/tmcallaghan/f068e414f6018185e92b&quot;&gt;&lt;span&gt;&lt;span&gt;wiredtiger-zlib.conf&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://gist.github.com/tmcallaghan/e3fe0584263585cfaeac&quot;&gt;&lt;span&gt;&lt;span&gt;tokumxse-uncompressed.conf&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://gist.github.com/tmcallaghan/81fef36f8ad94e462803&quot;&gt;&lt;span&gt;&lt;span&gt;tokumxse-quicklz.conf&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://gist.github.com/tmcallaghan/a2aba6930e88e70a3ef6&quot;&gt;&lt;span&gt;&lt;span&gt;tokumxse-zlib.conf&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://gist.github.com/tmcallaghan/d3964a28a775c57d3720&quot;&gt;&lt;span&gt;&lt;span&gt;tokumxse-lzma.conf&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://gist.github.com/comerford/e5417b57d8b4691dc55c&quot;&gt;&lt;span&gt;&lt;span&gt;compress_test.js&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description>
	<pubDate>Mon, 23 Feb 2015 14:05:58 +0000</pubDate>
	<author>noreply@blogger.com (Tim Callaghan)</author>
</item>
<item>
	<title>Tim Callaghan: MongoDB v3.0 Compression Benchmarks</title>
	<guid>tag:blogger.com,1999:blog-8043938871710850997.post-4398517351647018440</guid>
	<link>http://www.acmebenchmarking.com/2015/02/mongodb-v30-compression-benchmarks.html</link>
	<description>&lt;span&gt;In my &lt;a href=&quot;http://www.acmebenchmarking.com/2015/02/mongodb-storage-engine-shootout-round-1.html&quot; target=&quot;_blank&quot;&gt;last&lt;/a&gt; &lt;a href=&quot;http://www.acmebenchmarking.com/2015/02/mongodb-v3-se-shootout-1a.html&quot; target=&quot;_blank&quot;&gt;two&lt;/a&gt; blogs, I compared the indexed insertion performance of the various &lt;a href=&quot;http://www.mongodb.com/&quot; target=&quot;_blank&quot;&gt;MongoDB&lt;/a&gt; v3.0 storage engines. It was interesting to see how they stacked up against each other, especially looking at the performance variability in each. Based on those results I expect the &lt;a href=&quot;http://www.wiredtiger.com/&quot; target=&quot;_blank&quot;&gt;WiredTiger&lt;/a&gt; and &lt;a href=&quot;http://www.tokutek.com/&quot; target=&quot;_blank&quot;&gt;Tokutek&lt;/a&gt; developers to continue improving their respective technologies throughout 2015, there is much to be done.&lt;/span&gt;&lt;br /&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;But enough about those benchmarks, it's time for a good-old-fashioned compression test. I enjoy testing compression since it is extremely scientific. You simply insert a bunch of data then measure the size on disk. I decided to do two separate compression tests: one from &lt;a href=&quot;http://comerford.cc/&quot; target=&quot;_blank&quot;&gt;Adam&lt;/a&gt; &lt;a href=&quot;https://twitter.com/comerford&quot; target=&quot;_blank&quot;&gt;Comerford&lt;/a&gt; of MongoDB and the other a simple mongoimport test of an easily accessible data set.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;First up is a benchmark that was originally created and presented on &lt;a href=&quot;http://comerford.cc/wordpress/2015/02/04/mongodb-3-0-testing-compression/&quot; target=&quot;_blank&quot;&gt;Adam Comerford's blog&lt;/a&gt;. &lt;i&gt;I owe Adam a word for thanks as I didn't realize that MongoDB supports YAML based configuration files until I read his blog, this made my testing so much simpler.&lt;/i&gt;&amp;nbsp;Adam created a &lt;a href=&quot;https://gist.github.com/comerford/e5417b57d8b4691dc55c&quot; target=&quot;_blank&quot;&gt;small amount of Javascript&lt;/a&gt; to insert the data and recorded both size on disk and overall insert performance for his tests.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;Here are the results executed on my benchmark machine, now including the results for the TokuMXse storage engine as well.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://4.bp.blogspot.com/-vCqIGlS1Ar4/VNkC2IeMsJI/AAAAAAAAB78/7E-Ulo6b_WM/s1600/se-shootout-02-adamc-compression-size.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://4.bp.blogspot.com/-vCqIGlS1Ar4/VNkC2IeMsJI/AAAAAAAAB78/7E-Ulo6b_WM/s1600/se-shootout-02-adamc-compression-size.png&quot; height=&quot;426&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;Size-wise, TokuMXse is 8.2% smaller than WiredTiger using zlib compression. This is interesting, especially since WiredTiger is using zlib level 6, versus TokuMXse using zlib level 5. The higher the number, and it goes up to 9, the more aggressive the compression. TokuMXse with lzma compression is far and away the winner here, as the on-disk size is 31.1% smaller than WiredTiger/zlib.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;span&gt;&lt;i&gt;&lt;b&gt;Note&lt;/b&gt;: it's important to keep in mind the balance between compression and performance, especially when it comes to query performance. The more aggressive the compression, like lzma versus zlib, or the more aggressive the zlib level, the longer it takes to decompress the data for reads/updates.&lt;/i&gt;&lt;/span&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;span&gt;And for that same benchmark, here are the insert performance numbers measured in documents inserted per second.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://1.bp.blogspot.com/-37s7efPfxVs/VNn_WRixrxI/AAAAAAAAB8s/lYij_8g_gLs/s1600/se-shootout-02-adamc-compression-speed.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://1.bp.blogspot.com/-37s7efPfxVs/VNn_WRixrxI/AAAAAAAAB8s/lYij_8g_gLs/s1600/se-shootout-02-adamc-compression-speed.png&quot; height=&quot;426&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot;&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;The only surprise here is that WiredTiger with zlib is so much slower than WiredTiger with snappy, specifically 35.1%. I'm curious to understand why that is the case. Anyone?&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;i&gt;&lt;b&gt;&lt;span&gt;Edit : 2015-02-10 : As Adam Comerford pointed out in Twitter and in a comment to this blog, the performance issue is in WiredTiger's zlib compression on the journal. A repeat test without journal compression showed good insert performance (similar to none and snappy).&lt;/span&gt;&lt;/b&gt;&lt;/i&gt;&lt;/blockquote&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;My second compression benchmark uses a data set available from &lt;a href=&quot;https://twitter.com/andy_pavlo&quot; target=&quot;_blank&quot;&gt;Andy Pavlo's&lt;/a&gt; &lt;a href=&quot;http://www.cs.cmu.edu/~pavlo/datasets/index.html&quot; target=&quot;_blank&quot;&gt;Collected Data Sets&lt;/a&gt;, specifically the &lt;a href=&quot;http://www.cs.brown.edu/~pavlo/torrent/peersnapshots-01.csv.gz&quot; target=&quot;_blank&quot;&gt;BitTorrent Peer Snapshot Part 1&lt;/a&gt;. For each test I start with an empty MongoDB server with the appropriate storage engine and, if applicable, compression settings. I think this data set is more real-world than Adam's Javascript data generator.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;First up are the compression numbers.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://1.bp.blogspot.com/-AP6ez18phq0/VNkFtBph-9I/AAAAAAAAB8U/51I0Xn3pDqs/s1600/se-shootout-02-andyp-compression-size.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://1.bp.blogspot.com/-AP6ez18phq0/VNkFtBph-9I/AAAAAAAAB8U/51I0Xn3pDqs/s1600/se-shootout-02-andyp-compression-size.png&quot; height=&quot;426&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;Again, the TokuMXse/zlib size on disk was smaller than WiredTiger/zlib, this time at 7%. TokuMXse/lzma was 28% smaller than WiredTiger/zlib.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;Most interesting to me were the performance results for this test, again measured in number of documents inserted per second.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot;&gt;&lt;/div&gt;&lt;div&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://1.bp.blogspot.com/-O9C4tLKdaBY/VNn_fjNMa6I/AAAAAAAAB80/8LrxBIF6TTg/s1600/se-shootout-02-andyp-compression-speed.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://1.bp.blogspot.com/-O9C4tLKdaBY/VNn_fjNMa6I/AAAAAAAAB80/8LrxBIF6TTg/s1600/se-shootout-02-andyp-compression-speed.png&quot; height=&quot;426&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;In this test WiredTiger was 55% slower using zlib than snappy. TokuMXse performance was relatively unchanged throughout the test.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;To recap from these benchmarks:&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;TokuMXse is around 7% smaller than WiredTiger with zlib, and substantially smaller with lzma.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;WiredTiger insertion performance is significantly slower with zlib than snappy. Side note, I see nothing in the &lt;a href=&quot;https://jira.mongodb.org/&quot; target=&quot;_blank&quot;&gt;MongoDB Jira&lt;/a&gt; in regards to this issue.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;MMAPv1 insert performance was the best of the bunch, but it's size-on-disk is just awful.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;&lt;span&gt;Next week I'm going to publish and explain the &lt;i&gt;&lt;u&gt;simple&lt;/u&gt;&lt;/i&gt; bash scripts I created for this round of benchmarks.&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;</description>
	<pubDate>Tue, 10 Feb 2015 15:30:40 +0000</pubDate>
	<author>noreply@blogger.com (Tim Callaghan)</author>
</item>
<item>
	<title>Tim Callaghan: MongoDB v3.0 Storage Engine Shootout : Round 1a : WiredTiger and directIO</title>
	<guid>tag:blogger.com,1999:blog-8043938871710850997.post-8661553023177415380</guid>
	<link>http://www.acmebenchmarking.com/2015/02/mongodb-v3-se-shootout-1a.html</link>
	<description>&lt;span&gt;In &lt;a href=&quot;http://www.acmebenchmarking.com/2015/02/mongodb-storage-engine-shootout-round-1.html&quot; target=&quot;_blank&quot;&gt;round 1&lt;/a&gt; of the MongoDB v3.0 Storage Engine Shootout I noticed that &lt;a href=&quot;http://www.wiredtiger.com/&quot; target=&quot;_blank&quot;&gt;WiredTiger&lt;/a&gt; did something totally unexpected, it performed better with the OpLog enabled than it did with the OpLog disabled. This is peculiar, as enabling the OpLog forces MongoDB to maintain an additional collection of all insert/update/delete operations on the server, plus the collection is capped which is a no simple feat for WiredTiger's B-tree (it's a similarly painful for &lt;a href=&quot;http://www.tokutek.com/&quot; target=&quot;_blank&quot;&gt;Tokutek's&lt;/a&gt; &lt;a href=&quot;http://en.wikipedia.org/wiki/Fractal_tree_index&quot; target=&quot;_blank&quot;&gt;Fractal Tree&lt;/a&gt;).&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;My benchmark server has 32GB of RAM. Since these first few benchmark rounds are not intended to be in-memory benchmarks I need to make sure the working set of data far exceeds the cache size so I can induce IO. The easiest way to do that on storage engines that support a defined cache size is to use directIO. With directIO the filesystem writes are not cached in the OS buffers, so setting a small cache for the particular storage engine works fine. In my testing I use 8GB as the cache size. Unfortunately the MMAPv1 storage engine uses memory mapped files and doesn't have a specific parameter to limit the amount of memory it uses, it just uses all available memory in your server. I get around this by executing a program prior to my benchmark that exclusively grabs a set amount of memory on the server, leaving just 12GB available to the MMAPv1 engine. My long term plan is to measure the performance loss when using something like &lt;a href=&quot;http://www.docker.com/&quot; target=&quot;_blank&quot;&gt;Docker&lt;/a&gt; and run the servers in containers.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;The WiredTiger storage engine in MongoDB v3.0 is highly configurable. A small number of these configuration options have been exposed and are easily set via the command line when starting up the server. Enabling directIO requires passing a specific configuration string that is not exposed as one of these options, specifically &quot;--wiredTigerEngineConfigString direct_io=[data]&quot;.&lt;/span&gt;&lt;br /&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;i&gt;&lt;span&gt;&lt;b&gt;My current advice to everyone is that if you can't set a WiredTiger parameter with a simple defined command line argument, then don't do it. &lt;u&gt;They are hard to set for a reason&lt;/u&gt;, leave the string based arguments alone!&lt;/b&gt;&lt;/span&gt;&lt;/i&gt;&lt;/blockquote&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;I've changed my benchmarking methodology. Going forward I'm booting my server with the &quot;mem=&quot; argument, thus limiting the server to 12GB of total memory. Also, all go-forward benchmarks with WiredTiger will be using bufferedIO. At some point the MongoDB/WiredTiger team will improve the directIO implementation and I'll rerun my tests and share the results.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;So here are the results of the rerun. First up is WiredTiger with the OpLog on and off. Results are now more explainable than before, but still odd. With directIO, WiredTiger was faster with the OpLog on than off. Now the performance is similar with the OpLog enabled and disabled. Again, not exactly what I expected but better than before.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://4.bp.blogspot.com/-vNBhpsRRNxY/VNN1NA_wEuI/AAAAAAAAB7M/zI-R_oYnCy8/s1600/iibench-tps-se-shootout-01a-wiredtiger-12-0-all.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://4.bp.blogspot.com/-vNBhpsRRNxY/VNN1NA_wEuI/AAAAAAAAB7M/zI-R_oYnCy8/s1600/iibench-tps-se-shootout-01a-wiredtiger-12-0-all.png&quot; height=&quot;480&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;So replacing the prior bufferedIO run we have a new comparison graph showing TokuMXse vs. WiredTiger vs. MMAPv1.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://4.bp.blogspot.com/-VYYXN9KNzjI/VNN1cV6K3II/AAAAAAAAB7U/A8uDCLMYTY4/s1600/iibench-tps-se-shootout-01a-all.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://4.bp.blogspot.com/-VYYXN9KNzjI/VNN1cV6K3II/AAAAAAAAB7U/A8uDCLMYTY4/s1600/iibench-tps-se-shootout-01a-all.png&quot; height=&quot;480&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Still a convincing victory for TokuMXse. The low points in TokuMXse are above the peak performance of WiredTiger, and MMAPv1 is just a mess (there are 10 second intervals where MMAPv1 inserted less than 100 documents).&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;A &lt;a href=&quot;http://comerford.cc/wordpress/2015/02/04/mongodb-3-0-testing-compression/&quot; target=&quot;_blank&quot;&gt;recent blog on compression size and performance&lt;/a&gt; by &lt;a href=&quot;https://twitter.com/comerford&quot; target=&quot;_blank&quot;&gt;Adam Comerford&lt;/a&gt; of MongoDB has distracted me, so round 2 will commence on Monday and focus on compression.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;</description>
	<pubDate>Thu, 05 Feb 2015 10:57:40 +0000</pubDate>
	<author>noreply@blogger.com (Tim Callaghan)</author>
</item>
<item>
	<title>Tim Callaghan: MongoDB Storage Engine Shootout : Round 1 : Indexed Insertion</title>
	<guid>tag:blogger.com,1999:blog-8043938871710850997.post-264051219135946328</guid>
	<link>http://www.acmebenchmarking.com/2015/02/mongodb-storage-engine-shootout-round-1.html</link>
	<description>&lt;div class=&quot;separator&quot;&gt;&lt;span&gt;&lt;a href=&quot;http://1.bp.blogspot.com/-CgPt9dgZ9LA/VNDVgbvT4sI/AAAAAAAAB50/QcWVFjcVZgk/s1600/storage-engine-wars.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://1.bp.blogspot.com/-CgPt9dgZ9LA/VNDVgbvT4sI/AAAAAAAAB50/QcWVFjcVZgk/s1600/storage-engine-wars.png&quot; height=&quot;163&quot; width=&quot;320&quot; /&gt;&lt;/a&gt;&lt;/span&gt;&lt;/div&gt;&lt;span&gt;The next release of MongoDB includes the ability to select a storage engine, the goal being that different storage engines will have different capabilities/advantages, and user's can select the one most beneficial to their particular use-case. &lt;i&gt;&lt;b&gt;Storage engines are cool.&lt;/b&gt;&lt;/i&gt; MySQL has offered them for quite a while. One very big difference between the MySQL and MongoDB implementations is that in MySQL the user gets to select a particular storage engine for each table, whereas in MongoDB it's a choice made at server startup. You get a single storage engine for everything on the particular mongod instance. I see pros and cons to each decision, but that's a blog for another day.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;In &lt;a href=&quot;http://www.mongodb.com/blog/post/renaming-our-upcoming-release-mongodb-30&quot;&gt;MongoDB 3.0&lt;/a&gt; (yes it was going to be 2.8 but now it's 3.0, &lt;i&gt;get over it&lt;/i&gt;) the existing storage engine technology, named MMAPv1, is the default but MongoDB will support the &lt;a href=&quot;http://www.mongodb.com/press/wired-tiger&quot;&gt;acquired WiredTiger&lt;/a&gt; storage engine to a limited degree. As I understand it the B-tree implementation is supported, but LSM is not. Eliot has publicly stated that the long term goal is for WiredTiger to be fully supported and the default, and eventually MMAPv1 will be deprecated. The only other currently viable storage engine is &lt;a href=&quot;http://www.tokutek.com/&quot;&gt;Tokutek's&lt;/a&gt; &lt;a href=&quot;http://www.tokutek.com/2015/01/announcing-tokumxse-v1-0-0-rc-0/&quot;&gt;TokuMXse&lt;/a&gt;, which must be obtained directly from Tokutek. The &lt;a href=&quot;https://github.com/facebook/rocksdb&quot;&gt;RocksDB&lt;/a&gt; team at &lt;a href=&quot;http://www.facebook.com/&quot;&gt;Facebook&lt;/a&gt; is also working on a storage engine, but the release date doesn't seem to be any time soon.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;But enough of this overview, this series of blogs is all about picking a benchmark, explaining what it's doing, running it on all storage engines, and selecting a winner for the &quot;round&quot;.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;First up is iiBench, the indexed insertion benchmark. It's easy to grab and run the code for yourself via &lt;a href=&quot;https://github.com/tmcallaghan/iibench-mongodb&quot;&gt;GitHub&lt;/a&gt;. The benchmark itself was created to show how well systems perform in a pure insertion workload to a single collection with 3 secondary indexes, all of which are on fields with random insertion patterns.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;I ran iiBench with defaults except for the following, all of which can be easily modified via the run.simple.bash script:&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;4 concurrent insert threads (WiredTiger and TokuMXse support document level locking so they should have an advantage here)&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;250 documents per inserted batch&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;benchmark duration = 60 minutes&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;1 additional character field, 1000 bytes, 75% compressible&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Every 10 second interval the number of completed inserts is reported &lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;All benchmarks were run on AcmeBenchmarking server #1 (ab01), I'll be creating a &quot;server details&quot; page soon, but for the time being:&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;span&gt;Intel 4790K CPU (quad core + hyperthreading)&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;32GB DDR3 RAM&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;LSI 9280 RAID : 512MB cache, Samsung 830 256GB SSD&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Lots of IOPs and bandwidth &lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span&gt;Ubuntu 14.04 &lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;The goal is to show how the system behave long-term, not just for a short amount of time. The easiest way to do that is to get data &amp;gt; RAM, so...&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;For WiredTiger and TokuMXse I configured cache size to 8GB and forced directIO.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;For MMAPv1 I run an additional process on the server that uses/locks all but 12GB of RAM, so the mongod server doesn't have access to the entire 32GB. I plan on running via docker at some point so I can simply expose less memory to the server, but this technique has served me well so far.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;&amp;nbsp;And lastly, the MongoDB versions:&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;mongodb-linux-x86_64-3.0.0-rc7.tgz&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;mongodb-linux-x86_64-tokumxse-20150123e.tgz&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;b&gt;&lt;span&gt;First benchmark run, OpLog is on.&lt;/span&gt;&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://1.bp.blogspot.com/-aLNTFWf1y_g/VNDZQy2ynTI/AAAAAAAAB6A/ZJOZmTO2fiE/s1600/iibench-tps-se-shootout-01-all-oplog-on.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://1.bp.blogspot.com/-aLNTFWf1y_g/VNDZQy2ynTI/AAAAAAAAB6A/ZJOZmTO2fiE/s1600/iibench-tps-se-shootout-01-all-oplog-on.png&quot; height=&quot;480&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;span&gt;A couple of interesting things to note here:&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;The WiredTiger B-tree becomes IO bound for secondary index maintenance quickly.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;MMAPv1 comes close to the 0-line 30 minutes in. The 0-line is bad, it means there were no inserts completed in the measured 10 second interval.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;span&gt;After looking at this graph I wanted to understand the performance impact of the OpLog. For those new to MongoDB the OpLog is where completed operations are logged for replay into secondaries (replication). The OpLog can also be tailed or queried just like any other collection in MongoDB. The OpLog is a capped collection, meaning it's kept at a fixed size, which is hard for B-tree/Fractal Tree backed engined like WiredTiger and TokuMXse to maintain. So here is the rerun of the benchark with the OpLog disabled (off).&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;&lt;b&gt;&lt;span&gt;Second benchmark run, OpLog is off.&lt;/span&gt;&lt;/b&gt; &lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://1.bp.blogspot.com/-ubuAWs8tXg0/VNDeYmaDNaI/AAAAAAAAB6Q/loXS8BXv2dU/s1600/iibench-tps-se-shootout-01-all-oplog-off.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://1.bp.blogspot.com/-ubuAWs8tXg0/VNDeYmaDNaI/AAAAAAAAB6Q/loXS8BXv2dU/s1600/iibench-tps-se-shootout-01-all-oplog-off.png&quot; height=&quot;480&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;span&gt;The performance changes are interesting, and worth looking at in isolation of each engine. I've regraphed each result, showing the particular engine with the OpLog on and off, and started the graph at the 1200 second mark to eliminate the early in-memory noise.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&lt;b&gt;MMAPv1&lt;/b&gt;&amp;nbsp;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;The 0-line issues went away after disabling the OpLog. This is nice to see, but running a server without an OpLog is a really bad idea if you care about your data. With the OpLog enabled it's performance variability is concerning.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://1.bp.blogspot.com/-_UtRI9S1A4I/VNDfkG10UoI/AAAAAAAAB6c/xnxGtGMvSDc/s1600/iibench-tps-se-shootout-01-mmapv1.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://1.bp.blogspot.com/-_UtRI9S1A4I/VNDfkG10UoI/AAAAAAAAB6c/xnxGtGMvSDc/s1600/iibench-tps-se-shootout-01-mmapv1.png&quot; height=&quot;480&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&lt;b&gt;TokuMXse&lt;/b&gt;&amp;nbsp;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;As with MMAPv1, performance improved after disabling the OpLog.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://4.bp.blogspot.com/-wneYOSNRtvY/VNDhDqgDyOI/AAAAAAAAB64/aLJaSTWeAzc/s1600/iibench-tps-se-shootout-01-tokumxse(1).png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://4.bp.blogspot.com/-wneYOSNRtvY/VNDhDqgDyOI/AAAAAAAAB64/aLJaSTWeAzc/s1600/iibench-tps-se-shootout-01-tokumxse(1).png&quot; height=&quot;480&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://3.bp.blogspot.com/-0wNEr4NtTOw/VNDfsOPEGHI/AAAAAAAAB6k/b2InpKon2I0/s1600/iibench-tps-se-shootout-01-tokumxse.png&quot;&gt;&lt;br /&gt;&lt;/a&gt;&lt;/div&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;b&gt;WiredTiger&lt;/b&gt;&amp;nbsp;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;This is a real head-scratcher. Performance peaks were actually higher (almost 2x) with the OpLog enabled, even though the server needs to maintain an additional collection. Ideas?&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://2.bp.blogspot.com/-oQxcZu5vczo/VNDfwelfcQI/AAAAAAAAB6s/vi1ZSRs5uQk/s1600/iibench-tps-se-shootout-01-wiredtiger.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://2.bp.blogspot.com/-oQxcZu5vczo/VNDfwelfcQI/AAAAAAAAB6s/vi1ZSRs5uQk/s1600/iibench-tps-se-shootout-01-wiredtiger.png&quot; height=&quot;480&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;b&gt;&lt;span&gt;&lt;span&gt;Round 1 Recap&lt;/span&gt;&lt;/span&gt;&lt;/b&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;Round 1 goes&lt;/span&gt;&lt;span&gt; to TokuMXse, it's performance is substantially higher in both the OpLog enabled and OpLog disabled tests. &lt;i&gt;I'm hoping that someone can explain to me the reason WiredTiger performed better with the OpLog enabled.&lt;/i&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;In Round 2 I'll add queries to the above workload to see how that impacts the insertion performance.&lt;/span&gt;</description>
	<pubDate>Tue, 03 Feb 2015 15:12:59 +0000</pubDate>
	<author>noreply@blogger.com (Tim Callaghan)</author>
</item>
<item>
	<title>MongoDB Spain: How to set up a MongoDB Sharded Cluster</title>
	<guid>http://www.mongodbspain.com/?p=2382</guid>
	<link>http://www.mongodbspain.com/en/2015/01/26/how-to-set-up-a-mongodb-sharded-cluster/</link>
	<description>&lt;h4&gt;&lt;b&gt;Definition&lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;A sharded cluster is a set of Replica Sets (shards) whose mission is to distribute the load uniformly, so that we can scale horizontally our applications in order to work with huge amounts of data.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;&lt;b&gt;Advantages of a sharded cluster&lt;/b&gt;&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The main features of a sharded cluster are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Scalability: From a standalone server to distributed architectures of huge clusters (This allows us to shard our database transparently across all our shards. This increases the performance of our data processing).&lt;/li&gt;
&lt;li&gt;Load balancing: Automatic data movement across different shards for load balancing. The balancer decides when to migrate the data and the destination Shard, so they are evenly distributed among all servers in the cluster. Each shard stores the data for a selected range of our collection according to a partition key.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;strong&gt;&lt;b&gt;Our sharded cluster configuration&lt;/b&gt;&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;This is the configuration we will use in our sharded cluster:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Three config servers (the minimum in a production environment): the mongos will query them to know the shard to which enroute the clients requires. We will allocate them in the juan-mongodbspain machine.&lt;/li&gt;
&lt;li&gt;Two Replica Sets (a and b), each one with three nodes (enough to show how to set up the cluster). Each Replica Set in a separate machine, the first in juan-mongodbspain and the second one in juan-mongodbspain2.&lt;/li&gt;
&lt;li&gt;Two mongos (they request the information needed by the clients to the shard that contains it). Both of them will be allocated in the juan-mongodbspain machine.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;strong&gt;&lt;b&gt;Creating directories&lt;/b&gt;&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;We can not start the services if we have not created the directories in which we will store our data. We will use these ones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cfg0, cfg1 and cfg2 for the three config server&lt;/li&gt;
&lt;li&gt;a0, a1 and a2 for the nodes of the Replica Set called ‘a’&lt;/li&gt;
&lt;li&gt;b0, b1 and b2 for the nodes of the Replica Set called ‘b’&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;b&gt;Order&lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;We will follow this order to start the instances of our sharded cluster:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Config servers&lt;/li&gt;
&lt;li&gt;mongod’s&lt;/li&gt;
&lt;li&gt;mongo’s&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;b&gt;Config servers&lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;These are the commands we will use for the config servers:&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;juan@juan-mongodbspain:/var/lib/mongodb/shardedcluster$ mongod --configsvr --port 26050 --logpath /var/lib/mongodb/shardedcluster/log.cfg0 --logappend --dbpath /var/lib/mongodb/shardedcluster/cfg0 --fork
about to fork child process, waiting until server is ready for connections.
forked process: 5453
child process started successfully, parent exiting
juan@juan-mongodbspain:/var/lib/mongodb/shardedcluster$ mongod --configsvr --port 26051 --logpath /var/lib/mongodb/shardedcluster/log.cfg1 --logappend --dbpath /var/lib/mongodb/shardedcluster/cfg1 --fork
about to fork child process, waiting until server is ready for connections.
forked process: 5469
child process started successfully, parent exiting
juan@juan-mongodbspain:/var/lib/mongodb/shardedcluster$ mongod --configsvr --port 26052 --logpath /var/lib/mongodb/shardedcluster/log.cfg2 --logappend --dbpath /var/lib/mongodb/shardedcluster/cfg2 --fork
about to fork child process, waiting until server is ready for connections.
forked process: 5498
child process started successfully, parent exiting
juan@juan-mongodbspain:/var/lib/mongodb/shardedcluster$&lt;/pre&gt;&lt;p&gt;With the ‘configsvr’ option we indicate that the service will be a config service.&lt;/p&gt;
&lt;p&gt;As we can see, we allocate the three config servers in the juan-mongodbspain machine, for that reason we use different ports. With different machines we should not specify any ports.&lt;/p&gt;
&lt;h4&gt;&lt;b&gt;mongod’s&lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;In the last post we talked about how to set up a Replica Set, you can read it here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a title=&quot;In a last post we talked about how to set up a Replica Set, you can read it here:&quot; href=&quot;http://www.mongodbspain.com/how-to-set-up-a-MongoDB-Replica-Set&quot; target=&quot;_blank&quot;&gt;How to set up a MongoDB Replica Set&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now it is the turn of the six nodes of the two Replica Sets. The first three nodes belong to the first Replica Set (shard) and they will be in one machine. The other three belong to the second shard and they will be in the other machine.&lt;/p&gt;
&lt;p&gt;In the command we must specify that our mongod will belong to a sharded cluster. For this purpose we use the ‘&amp;#8211;shardsvr’ option. 27018 is the default port for these instances.&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;juan@juan-mongodbspain:/var/lib/mongodb/shardedcluster$ mongod --shardsvr --replSet a --dbpath /var/lib/mongodb/shardedcluster/a0 --logpath /var/lib/mongodb/shardedcluster/log.a0 --port 27000 --fork --logappend --smallfiles --oplogSize 50
about to fork child process, waiting until server is ready for connections.
forked process: 5515
child process started successfully, parent exiting
juan@juan-mongodbspain:/var/lib/mongodb/shardedcluster$  mongod --shardsvr --replSet a --dbpath /var/lib/mongodb/shardedcluster/a1 --logpath /var/lib/mongodb/shardedcluster/log.a1 --port 27001 --fork --logappend --smallfiles --oplogSize 50
about to fork child process, waiting until server is ready for connections.
forked process: 5562
child process started successfully, parent exiting
juan@juan-mongodbspain:/var/lib/mongodb/shardedcluster$ mongod --shardsvr --replSet a --dbpath /var/lib/mongodb/shardedcluster/a2 --logpath /var/lib/mongodb/shardedcluster/log.a2 --port 27002 --fork --logappend --smallfiles --oplogSize 50
about to fork child process, waiting until server is ready for connections.
forked process: 5609
child process started successfully, parent exiting
juan@juan-mongodbspain:/var/lib/mongodb/shardedcluster$&lt;/pre&gt;&lt;p&gt;Now the second shard, remember that it is in the other machine:&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;juan@juan-mongodbspain2:/var/lib/mongodb/shardedcluster$ mongod --shardsvr --replSet b --dbpath /var/lib/mongodb/shardedcluster/b0 --logpath /var/lib/mongodb/shardedcluster/log.b0 --port 27000 --fork --logappend --smallfiles --oplogSize 50
about to fork child process, waiting until server is ready for connections.
forked process: 5658
child process started successfully, parent exiting
juan@juan-mongodbspain2:/var/lib/mongodb/shardedcluster$ mongod --shardsvr --replSet b --dbpath /var/lib/mongodb/shardedcluster/b1 --logpath /var/lib/mongodb/shardedcluster/log.b1 --port 27001 --fork --logappend --smallfiles --oplogSize 50
about to fork child process, waiting until server is ready for connections.
forked process: 5706
child process started successfully, parent exiting
juan@juan-mongodbspain2:/var/lib/mongodb/shardedcluster$ mongod --shardsvr --replSet b --dbpath /var/lib/mongodb/shardedcluster/b2 --logpath /var/lib/mongodb/shardedcluster/log.b2 --port 27002 --fork --logappend --smallfiles --oplogSize 50
about to fork child process, waiting until server is ready for connections.
forked process: 5754
child process started successfully, parent exiting
juan@juan-mongodbspain2:/var/lib/mongodb/shardedcluster$&lt;/pre&gt;&lt;p&gt;&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;&lt;b&gt;mongo’s&lt;/b&gt;&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Except for administrative tasks, we will never connect ourselves to a mongod, neither to a config server. The apps will connect to the mongos which mission is to send the queries to the appropiate shards. For this reason we specify ports different to the standard in all of our instances. Therefore, our apps will only be able to connect to the database through a mongos.&lt;/p&gt;
&lt;p&gt;This is the command:&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;juan@juan-mongodbspain:/var/lib/mongodb/shardedcluster$ mongos --configdb juan-mongodbspain:26050,juan-mongodbspain:26051,juan-mongodbspain:26052 --fork --logappend --logpath /var/lib/mongodb/shardedcluster/log.mongos0
about to fork child process, waiting until server is ready for connections.
forked process: 5832
child process started successfully, parent exiting
juan@juan-mongodbspain:/var/lib/mongodb/shardedcluster$ mongos --configdb juan-mongodbspain:26050,juan-mongodbspain:26051,juan-mongodbspain:26052 --fork --logappend --logpath /var/lib/mongodb/shardedcluster/log.mongos1 --port 26061
about to fork child process, waiting until server is ready for connections.
forked process: 5894
child process started successfully, parent exiting
juan@juan-mongodbspain:/var/lib/mongodb/shardedcluster$&lt;/pre&gt;&lt;p&gt;As we can see, the first command does not specify any port. Hence, this mongos will listen at the port by default (27017).&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;&lt;b&gt;Checking our services&lt;/b&gt;&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Now we will check that all of our services are running:&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;juan@juan-mongodbspain:/var/lib/mongodb/shardedcluster$ ps -ef | grep mongo
root 5453 2634 0 11:08 ? 00:00:10 mongod --configsvr --port 26050 --logpath /var/lib/mongodb/shardedcluster/log.cfg0 --logappend --dbpath /var/lib/mongodb/shardedcluster/cfg0 --fork
root 5469 2634 0 11:08 ? 00:00:10 mongod --configsvr --port 26051 --logpath /var/lib/mongodb/shardedcluster/log.cfg1 --logappend --dbpath /var/lib/mongodb/shardedcluster/cfg1 --fork
root 5498 2634 0 11:09 ? 00:00:10 mongod --configsvr --port 26052 --logpath /var/lib/mongodb/shardedcluster/log.cfg2 --logappend --dbpath /var/lib/mongodb/shardedcluster/cfg2 --fork
root 5515 2634 0 11:10 ? 00:00:10 mongod --shardsvr --replSet a --dbpath /var/lib/mongodb/shardedcluster/a0 --logpath /var/lib/mongodb/shardedcluster/log.a0 --port 27000 --fork --logappend --smallfiles --oplogSize 50
root 5562 2634 0 11:10 ? 00:00:10 mongod --shardsvr --replSet a --dbpath /var/lib/mongodb/shardedcluster/a1 --logpath /var/lib/mongodb/shardedcluster/log.a1 --port 27001 --fork --logappend --smallfiles --oplogSize 50
root 5609 2634 0 11:11 ? 00:00:09 mongod --shardsvr --replSet a --dbpath /var/lib/mongodb/shardedcluster/a2 --logpath /var/lib/mongodb/shardedcluster/log.a2 --port 27002 --fork --logappend --smallfiles --oplogSize 50
root 5832 2634 0 11:34 ? 00:00:00 mongos --configdb juan-mongodbspain:26050,juan-mongodbspain:26051,juan-mongodbspain:26052 --fork --logappend --logpath /var/lib/mongodb/shardedcluster/log.mongos0
root 5894 2634 0 11:34 ? 00:00:00 mongos --configdb juan-mongodbspain:26050,juan-mongodbspain:26051,juan-mongodbspain:26052 --fork --logappend --logpath /var/lib/mongodb/shardedcluster/log.mongos1 --port 26061
juan 5954 5196 0 11:35 pts/6 00:00:00 grep --color=auto mongo
juan@juan-mongodbspain:/var/lib/mongodb/shardedcluster$&lt;/pre&gt;&lt;p&gt;In the other machine we have the mongod’s corresponding to the second shard:&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;juan@juan-mongodbspain2:/var/lib/mongodb/shardedcluster$ ps -ef | grep mongo
root 5658 2634 0 11:12 ? 00:00:09 mongod --shardsvr --replSet b --dbpath /var/lib/mongodb/shardedcluster/b0 --logpath /var/lib/mongodb/shardedcluster/log.b0 --port 27000 --fork --logappend --smallfiles --oplogSize 50
root 5706 2634 0 11:12 ? 00:00:09 mongod --shardsvr --replSet b --dbpath /var/lib/mongodb/shardedcluster/b1 --logpath /var/lib/mongodb/shardedcluster/log.b1 --port 27001 --fork --logappend --smallfiles --oplogSize 50
root 5754 2634 0 11:12 ? 00:00:09 mongod --shardsvr --replSet b --dbpath /var/lib/mongodb/shardedcluster/b2--logpath /var/lib/mongodb/shardedcluster/log.b2 --port 27002 --fork --logappend --smallfiles --oplogSize 50
juan@juan-mongodbspain2:/var/lib/mongodb/shardedcluster$&lt;/pre&gt;&lt;p&gt;&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;&lt;b&gt;Replica Set configurations&lt;/b&gt;&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Now we must configure our Replica Sets (we have explained it in an earlier post). I show it only for the first Replica Set because it is identical in both of them.&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;juan@juan-mongodbspain:/var/lib/mongodb/replicaset$ mongo --port 27000
MongoDB shell version: 2.6.6
connecting to: 127.0.0.1:27000/test
Hello Juan. Have a good day!
&amp;gt; rs.initiate()
{
	&quot;info2&quot; : &quot;no configuration explicitly specified -- making one&quot;,
	&quot;me&quot; : &quot;juan-mongodbspain:27000&quot;,
	&quot;info&quot; : &quot;Config now saved locally.  Should come online in about a minute.&quot;,
	&quot;ok&quot; : 1
}
a:PRIMARY&amp;gt; rs.add(&quot;juan-mongodbspain:27001&quot;)
{ &quot;ok&quot; : 1 }
a:PRIMARY&amp;gt; rs.add(&quot;juan-mongodbspain:27002&quot;)
{ &quot;ok&quot; : 1 }
a:PRIMARY&amp;gt;&lt;/pre&gt;&lt;p&gt;&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;&lt;b&gt;Cluster configuration&lt;/b&gt;&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The last thing we must do is to add our shards to the cluster. We will do it from the first mongos, but before doing it we can check that our cluster has not any shard yet.&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;juan@juan-mongodbspain:/var/lib/mongodb/shardedcluster$ mongo
MongoDB shell version: 2.6.6
connecting to: test
Hello Juan. Have a good day!
mongos&amp;gt; sh.status()
--- Sharding Status --- 
sharding version: {
&quot;_id&quot; : 1,
&quot;version&quot; : 4,
&quot;minCompatibleVersion&quot; : 4,
&quot;currentVersion&quot; : 5,
&quot;clusterId&quot; : ObjectId(&quot;548eb941260fb6e98e17d275&quot;)
}
shards:
databases:
{ &quot;_id&quot; : &quot;admin&quot;, &quot;partitioned&quot; : false, &quot;primary&quot; : &quot;config&quot; }

mongos&amp;gt;&lt;/pre&gt;&lt;p&gt;Ok, let’s add our two shards to the cluster:&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;mongos&amp;gt; sh.addShard(&quot;a/juan-mongodbspain:27000&quot;)
{ &quot;shardAdded&quot; : &quot;a&quot;, &quot;ok&quot; : 1 }
mongos&amp;gt; sh.addShard(&quot;b/juan-mongodbspain2:27000&quot;)
{ &quot;shardAdded&quot; : &quot;a&quot;, &quot;ok&quot; : 1 }
mongos&amp;gt;&lt;/pre&gt;&lt;p&gt;Now, our cluster is conformed by the two shards:&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;mongos&amp;gt; sh.status()
--- Sharding Status --- 
sharding version: {
&quot;_id&quot; : 1,
&quot;version&quot; : 4,
&quot;minCompatibleVersion&quot; : 4,
&quot;currentVersion&quot; : 5,
&quot;clusterId&quot; : ObjectId(&quot;548eb941260fb6e98e17d275&quot;)
}
shards:
{ &quot;_id&quot; : &quot;a&quot;, &quot;host&quot; : &quot;a/juan-mongodbspain:27000,juan-mongodbspain:27001,juan-mongodbspain:27002&quot; }
{ &quot;_id&quot; : &quot;b&quot;, &quot;host&quot; : &quot;b/juan-mongodbspain2:27000,juan-mongodbspain2:27001,juan-mongodbspain2:27002&quot; }
databases:
{ &quot;_id&quot; : &quot;admin&quot;, &quot;partitioned&quot; : false, &quot;primary&quot; : &quot;config&quot; }

mongos&amp;gt;&lt;/pre&gt;&lt;p&gt;&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;&lt;b&gt;Cluster Balancer&lt;/b&gt;&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;So we can know if the balancer is active:&lt;/p&gt;&lt;pre class=&quot;crayon-plain-tag&quot;&gt;mongos&amp;gt; sh.getBalancerState()
true
mongos&amp;gt;&lt;/pre&gt;&lt;p&gt;We have finished the setup of the sharded cluster. In a future post we will explain how to shard a collection.&lt;/p&gt;
&lt;p&gt;In the next post we will talk about how to upgrade a stand-alone node, a Replica Set and a Sharded Cluster.&lt;/p&gt;</description>
	<pubDate>Mon, 26 Jan 2015 23:24:42 +0000</pubDate>
</item>
<item>
	<title>Gustavo Niemeyer: Readying mgo for MongoDB 3.0</title>
	<guid>http://blog.labix.org/?p=2681</guid>
	<link>http://blog.labix.org/2015/01/24/readying-mgo-for-mongodb-3-0</link>
	<description>&lt;p&gt;MongoDB 3.0 (previously known as 2.8) is right around the block, and it&amp;#8217;s time to release a few fixes and improvements on the &lt;a href=&quot;http://labix.org/mgo&quot;&gt;mgo&lt;/a&gt; driver for &lt;a href=&quot;http://golang.org&quot;&gt;Go&lt;/a&gt; to ensure it works fine on that new major server version. Compatibility is being preserved both with old applications and with old servers, so updating should be a smooth experience.&lt;/p&gt;
&lt;p&gt;Release r2015.01.24 of mgo includes the following changes:&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;more-2681&quot;&gt;&lt;/span&gt;&lt;br /&gt;
&lt;strong&gt;Support ReplicaSetName in DialInfo&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;DialInfo now offers a &lt;a href=&quot;http://gopkg.in/mgo.v2#DialInfo.ReplicaSetName&quot;&gt;ReplicaSetName&lt;/a&gt; field that may contain the name of the MongoDB replica set being connected to. If set, the cluster synchronization routines will prevent communication with any server that does not report itself as part of that replica set.&lt;/p&gt;
&lt;p&gt;Feature implemented by Wisdom Omuya.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MongoDB 3.0 support for collection and index listing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;MongoDB 3.0 requires the use of commands for listing collections and indexes, and may report long results via cursors that must be iterated over. The &lt;a href=&quot;http://gopkg.in/mgo.v2#Database.CollectionNames&quot;&gt;CollectionNames&lt;/a&gt; and &lt;a href=&quot;http://gopkg.in/mgo.v2#Collection.Indexes&quot;&gt;Indexes&lt;/a&gt; methods were adapted to support both the old and the new cases.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Introduced Collection.NewIter method&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the last few releases of MongoDB, a growing number of low-level database commands are returning results that include an initial set of documents and one or more cursor ids that should be iterated over for obtaining the remaining documents. Such results defeated one of the goals in mgo&amp;#8217;s design: developers should be able to walk around the convenient pre-defined static interfaces when they must, so they don&amp;#8217;t have to patch the driver when a feature is not yet covered by the convenience layer.&lt;/p&gt;
&lt;p&gt;The introduced &lt;a href=&quot;http://gopkg.in/mgo.v2#Collection.NewIter&quot;&gt;NewIter&lt;/a&gt; method solves that problem by enabling developers to create &lt;a href=&quot;http://gopkg.in/mgo.v2#Iter&quot;&gt;normal iterators&lt;/a&gt; by providing the initial batch of documents and optionally the cursor id for obtaining the remaining documents, if any.&lt;/p&gt;
&lt;p&gt;Thanks to John Morales, Daniel Gottlieb, and Jeff Yemin, from MongoDB Inc, for their help polishing the feature.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Improved JSON unmarshaling of ObjectId&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://gopkg.in/mgo.v2/bson#ObjectId&quot;&gt;bson.ObjectId&lt;/a&gt; can now be unmarshaled correctly from an empty or null JSON string, when it is used as a field in a struct submitted for unmarshaling by the &lt;a href=&quot;http://golang.org/pkg/encoding/json&quot;&gt;json package&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Improvement suggested by Jason Raede.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remove GridFS chunks if file insertion fails&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When writing a &lt;a href=&quot;http://gopkg.in/mgo.v2#GridFile&quot;&gt;GridFS file&lt;/a&gt;, the chunks that hold the file content are written into the database before the document representing the file itself is inserted. This ensures the file is made visible to concurrent readers atomically, when it&amp;#8217;s ready to be used by the application. If writing a chunk fails, the call to the &lt;a href=&quot;http://gopkg.in/mgo.v2#GridFile.Close&quot;&gt;file&amp;#8217;s Close&lt;/a&gt; method will do a best effort to clean up previously written chunks. This logic was improved so that calling Close will also attempt to remove chunks if inserting the file document itself failed.&lt;/p&gt;
&lt;p&gt;Improvement suggested by Ed Pelc.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Field weight support for text indexing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The new &lt;a href=&quot;http://gopkg.in/mgo.v2#Index.Weights&quot;&gt;Index.Weights field&lt;/a&gt; allows providing a map of field name to field weight for fine tuning text index creation, as described in the &lt;a href=&quot;http://docs.mongodb.org/manual/tutorial/control-results-of-text-search/&quot;&gt;MongoDB documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Feature requested by Egon Elbre.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fixed support for &lt;code&gt;$**&lt;/code&gt; text index field name&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Support for the &lt;a href=&quot;http://docs.mongodb.org/manual/tutorial/create-text-index-on-multiple-fields/#index-all-fields&quot;&gt;special &lt;code&gt;$**&lt;/code&gt;&lt;/a&gt; field name, which enables the indexing of all document fields, was fixed.&lt;/p&gt;
&lt;p&gt;Problem reported by Egon Elbre.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Consider only exported fields on &lt;em&gt;omitempty&lt;/em&gt; of structs&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The implementation of &lt;a href=&quot;http://gopkg.in/mgo.v2/bson#Marshal&quot;&gt;bson&amp;#8217;s omitempty&lt;/a&gt; feature was also considering the value of non-exported fields. This was fixed so that only exported fields are taken into account, which is both in line with the overall behavior of the package, and also prevents crashes in cases where the field value cannot be evaluated.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fix potential deadlock on Iter.Close&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It was possible for &lt;a href=&quot;http://gopkg.in/mgo.v2#Iter.Close&quot;&gt;Iter.Close&lt;/a&gt; to deadlock when the associated server was concurrently detected unavailable.&lt;/p&gt;
&lt;p&gt;Problem investigated and reported by John Morales.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Return ErrCursor on server cursor timeouts&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Attempting to iterate over a cursor that has timed out at the server side will now return mgo.ErrCursor.&lt;/p&gt;
&lt;p&gt;Feature implemented by Daniel Gottlieb.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Support for collection repairing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The new &lt;a href=&quot;http://gopkg.in/mgo.v2#Collection.Repair&quot;&gt;Collection.Repair method&lt;/a&gt; returns an iterator that goes over all recovered documents in the collection, in a best-effort manner. This is most useful when there are damaged data files. Multiple copies of the same document may be returned by the iterator.&lt;/p&gt;
&lt;p&gt;Feature contributed by Mike O&amp;#8217;Brien.&lt;/p&gt;</description>
	<pubDate>Sat, 24 Jan 2015 11:40:38 +0000</pubDate>
</item>
<item>
	<title>Tim Callaghan: So long, and thanks for all the help.</title>
	<guid>tag:blogger.com,1999:blog-8043938871710850997.post-8756554794176909192</guid>
	<link>http://www.acmebenchmarking.com/2015/01/todo-so-long-and-thanks-for-all-help.html</link>
	<description>&lt;br /&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://3.bp.blogspot.com/-xUqhWzPeRZI/VLe8vRP8GqI/AAAAAAAAB5U/MhWGyiTVIFo/s1600/fish.jpg&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://3.bp.blogspot.com/-xUqhWzPeRZI/VLe8vRP8GqI/AAAAAAAAB5U/MhWGyiTVIFo/s1600/fish.jpg&quot; height=&quot;320&quot; width=&quot;213&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;Today is my last day at &lt;a href=&quot;http://www.tokutek.com/&quot; target=&quot;_blank&quot;&gt;Tokutek&lt;/a&gt;. On Monday I'm starting a new opportunity as VP/Technology at &lt;a href=&quot;http://www.crunchtime.com/&quot; target=&quot;_blank&quot;&gt;CrunchTime!&lt;/a&gt;. If you are a web developer, database developer, or quality assurance engineer in the Boston area and looking for a new opportunity please contact me or visit the &lt;a href=&quot;http://www.crunchtime.com/about-us/careers/&quot; target=&quot;_blank&quot;&gt;CrunchTime! career page&lt;/a&gt;.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;I've really enjoyed my time at VoltDB and Tokutek. Working for &lt;a href=&quot;http://en.wikipedia.org/wiki/Michael_Stonebraker&quot;&gt;Mike Stonebraker&lt;/a&gt;&amp;nbsp;(at VoltDB) was on my career &quot;bucket list&quot; and in these past 3.5 years at Tokutek I've experienced the awesomeness of the MySQL ecosystem and the surging NoSQL database market.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;But now I'm ready to going back to consuming databases, not creating them. It's probably a good idea for anyone in technology to work on the other side of the producer/consumer line, nothing beats &quot;real-world&quot; experience.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;If you like what I've been writing here then keep this blog in your RSS reader and/or follow me on twitter using &lt;/span&gt;&lt;a href=&quot;https://twitter.com/tmcallaghan&quot; target=&quot;_blank&quot;&gt;@tmcallaghan&lt;/a&gt;&lt;span&gt; and &lt;/span&gt;&lt;a href=&quot;https://twitter.com/acmebench&quot; target=&quot;_blank&quot;&gt;@acmebench&lt;/a&gt;&lt;span&gt;. My goal is to keep blogging twice a month.&amp;nbsp;&lt;/span&gt;&lt;br /&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;span&gt;&lt;i&gt;I hope there will be less concern about my benchmarking objectivity, as I'll no longer have a stake in the benchmarked technologies.&lt;/i&gt;&lt;/span&gt;&lt;/blockquote&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;Here are some topics I've already started working on:&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Creating a brand new benchmark that has more &quot;real world&quot; appeal than existing ones. I've stared work on an &quot;email server&quot; workload with operations like send, read, label, search, forward, reply, etc. MongoDB has &lt;a href=&quot;http://www.mongodb.com/presentations/mythbusting-understanding-how-we-measure-performance-mongodb-0&quot; target=&quot;_blank&quot;&gt;hinted&lt;/a&gt; that they are creating an &quot;inbox fan in/out&quot; benchmark which might be similar (I wish they'd make it public), mine will be available on &lt;a href=&quot;https://github.com/tmcallaghan&quot; target=&quot;_blank&quot;&gt;my GitHub&lt;/a&gt; and will likely support multiple database technologies.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;A lot of MongoDB 2.8 benchmarks comparing the MMAPV1, &lt;a href=&quot;http://www.wiredtiger.com/&quot; target=&quot;_blank&quot;&gt;WiredTiger&lt;/a&gt;, and &lt;a href=&quot;http://www.tokutek.com/&quot; target=&quot;_blank&quot;&gt;TokuMXse&lt;/a&gt; Storage Engines as well as &lt;a href=&quot;http://www.tokutek.com/tokumx-for-mongodb/&quot; target=&quot;_blank&quot;&gt;TokuMX&lt;/a&gt;.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;http://www.mysql.com/&quot; target=&quot;_blank&quot;&gt;MySQL&lt;/a&gt; benchmarks including TokuDB plus the 5.7 enhancements.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Analysis of good and bad benchmarks that I've seen. Please email me or comment on this blog if you've found an interesting new benchmark that could use review.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;And, of course, blogs about the practice of benchmarking itself.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;Lastly, I'd like to give a shout out to many people and companies I've worked with over the past 6+ years. You've all been welcoming and supportive, and really made my job fun. Specifically,&lt;/span&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;My &lt;a href=&quot;http://www.dbms2.com/2010/05/25/voltdb-finally-launches/&quot; target=&quot;_blank&quot;&gt;&quot;greater-known but nonethemore smart&quot;&lt;/a&gt; brother, &lt;a href=&quot;http://smalldatum.blogspot.com/&quot; target=&quot;_blank&quot;&gt;Mark Callaghan&lt;/a&gt;.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;My amazing team at &lt;a href=&quot;http://www.tokutek.com/&quot; target=&quot;_blank&quot;&gt;Tokutek&lt;/a&gt;, a true bunch of wall-breakers.&amp;nbsp;I'll miss Rich, Zardosht, Leif, John, Christian, Joel, Joe, and Abdelhak.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/in/gnarvaja&quot; target=&quot;_blank&quot;&gt;Gerry&lt;/a&gt;&lt;span&gt; and &lt;/span&gt;&lt;a href=&quot;https://www.linkedin.com/in/sheeri&quot; target=&quot;_blank&quot;&gt;Sheeri&lt;/a&gt;&lt;span&gt; for letting me sing the jingle on the &lt;/span&gt;&lt;a href=&quot;http://www.oursql.com/&quot; target=&quot;_blank&quot;&gt;OurSQL Podcast&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;The entire MySQL ecosystem.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;http://www.percona.com/about-us/our-team/vadim-tkachenko&quot;&gt;Vadim&lt;/a&gt;, &lt;a href=&quot;http://www.percona.com/about-us/our-team/peter-zaitsev&quot;&gt;Peter&lt;/a&gt;, and &lt;a href=&quot;https://www.linkedin.com/in/jrobyoung&quot;&gt;Rob&lt;/a&gt; at Percona,&amp;nbsp;&lt;a href=&quot;https://www.linkedin.com/in/amrith&quot; target=&quot;_blank&quot;&gt;Amrith&lt;/a&gt; at &lt;a href=&quot;http://tesora.com/&quot; target=&quot;_blank&quot;&gt;Tesora&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/pub/robert-hodges/3/568/a64&quot; target=&quot;_blank&quot;&gt;Robert&lt;/a&gt; at &lt;a href=&quot;http://continuent.com/&quot; target=&quot;_blank&quot;&gt;Continuent&lt;/a&gt; [now VMware], &lt;a href=&quot;https://www.linkedin.com/in/xaprb&quot; target=&quot;_blank&quot;&gt;Baron&lt;/a&gt; at &lt;a href=&quot;http://www.vividcortex.com/&quot; target=&quot;_blank&quot;&gt;VividCortex&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/shlominoach&quot;&gt;Shlomi&lt;/a&gt; at &lt;a href=&quot;http://www.booking.com/&quot;&gt;Booking.com&lt;/a&gt;, and &lt;a href=&quot;https://www.linkedin.com/pub/henrik-ingo/3/232/8a7&quot;&gt;Henrik&lt;/a&gt; at &lt;a href=&quot;http://www.mongodb.com/&quot;&gt;MongoDB&lt;/a&gt;.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Too many to people to name from&amp;nbsp;&lt;/span&gt;&lt;a href=&quot;http://www.percona.com/&quot; target=&quot;_blank&quot;&gt;Percona&lt;/a&gt;&lt;span&gt;,&amp;nbsp;&lt;/span&gt;&lt;a href=&quot;http://www.mysql.com/&quot; target=&quot;_blank&quot;&gt;Oracle/MySQL&lt;/a&gt;&lt;span&gt;,&amp;nbsp;&lt;/span&gt;&lt;a href=&quot;http://dbhangops.github.io/&quot; target=&quot;_blank&quot;&gt;DbHangOps&lt;/a&gt;&lt;span&gt;, &amp;nbsp;&lt;/span&gt;&lt;a href=&quot;http://voltdb.com/&quot; target=&quot;_blank&quot;&gt;VoltDB&lt;/a&gt;&lt;span&gt;, and&amp;nbsp;&lt;/span&gt;&lt;a href=&quot;https://mariadb.com/&quot; target=&quot;_blank&quot;&gt;MariaDB&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;And lastly thanks to everyone who has attended one of my webinars or presentations, commented on my blogs, or used TokuDB/TokuMX (commercially or community).&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;&lt;span&gt;My personal email address is available by clicking the disclaimer widget on the right hand side of the screen.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;So long... and stay tuned.&amp;nbsp;&lt;/span&gt;&lt;/div&gt;</description>
	<pubDate>Fri, 23 Jan 2015 16:01:06 +0000</pubDate>
	<author>noreply@blogger.com (Tim Callaghan)</author>
</item>
<item>
	<title>MongoDB Spain: MongoDB 3.0 will be the new version, renaming from 2.8</title>
	<guid>http://www.mongodbspain.com/?p=2426</guid>
	<link>http://www.mongodbspain.com/en/2015/01/23/mongodb-2-8-renamed-to-mongodb-3-0/</link>
	<description>&lt;p&gt;A few minutes ago it became official by &lt;strong&gt;Eliot Horowitz&lt;/strong&gt; through the &lt;a title=&quot;MongoDB Blog&quot; href=&quot;https://www.mongodb.com/blog/post/renaming-our-upcoming-release-mongodb-30&quot; target=&quot;_blank&quot;&gt;official MongoDB blog&lt;/a&gt;: &lt;strong&gt;Next MongoDB version renamed from 2.8 to 3.0&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;After a brief introduction on the meaning of the version numbers of MongoDB, Eliot explains the main reason for this change is because the upcoming release represents a step-function release, and this change deserves to be recognized with a major release number.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update 01/23:&lt;/strong&gt; The latest development version &lt;a title=&quot;v3.0 RC-6&quot; href=&quot;https://www.mongodb.org/downloads&quot;&gt;v3.0 RC-6&lt;/a&gt; has been released and is available for &lt;a title=&quot;Download MongoDB 3.0 RC-6&quot; href=&quot;https://www.mongodb.org/downloads&quot;&gt;download&lt;/a&gt;. We highly recommend to checking out the changelog available on &lt;a title=&quot;MongoDB JIRA&quot; href=&quot;https://jira.mongodb.org/issues/?jql=project%20%3D%20SERVER%20AND%20fixVersion%20%3D%20%223.0.0-rc6%22%20ORDER%20BY%20priority%20DESC&amp;_ga=1.206987027.1210128471.1422317028&quot;&gt;JIRA&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The stable version is very close so stay tuned and please welcome &lt;strong&gt;MongoDB 3.0&lt;/strong&gt;!&lt;/p&gt;</description>
	<pubDate>Fri, 23 Jan 2015 00:16:38 +0000</pubDate>
</item>
<item>
	<title>Gustavo Niemeyer: A timely coffee hack</title>
	<guid>http://blog.labix.org/?p=2660</guid>
	<link>http://blog.labix.org/2015/01/21/a-timely-coffee-hack</link>
	<description>&lt;p&gt;It&amp;#8217;s somewhat ironic that just as Ubuntu &lt;a href=&quot;http://www.ubuntu.com/things&quot;&gt;readies itself&lt;/a&gt; for the starting wave of smart connected devices, my latest hardware hack was in fact a disconnected one. In my defense, it&amp;#8217;s quite important for these smart devices to preserve a convenient physical interface with the user, so this one was a personal lesson on that.&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;more-2660&quot;&gt;&lt;/span&gt;The device hacked was a capsule-based coffee machine which originally had just a manual handle for on/off operation. This was both boring to use and unfortunate in terms of the outcome being somewhat unpredictable given the variations in amount of water through the capsule. While the manufacturer does offer a modern version of the same machine with an automated system, buying a new one wouldn&amp;#8217;t be nearly as satisfying.&lt;/p&gt;
&lt;p&gt;So the first act was to take the machine apart and see how it basically worked. To my surprise, this one model is quite difficult to take apart, but it was doable without any visible damage. Once in, the machine was &amp;#8220;enhanced&amp;#8221; with an external barrel connector that can command the operation of the machine:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://blog.labix.org/wp-content/uploads/2015/01/IMG_20141103_003118.jpg&quot;&gt;&lt;img src=&quot;http://blog.labix.org/wp-content/uploads/2015/01/IMG_20141103_003118-768x1024.jpg&quot; alt=&quot;Open Coffee Machine&quot; width=&quot;320&quot; height=&quot;427&quot; class=&quot;aligncenter size-large wp-image-2661&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The connector wire was soldered to the right spots, routed away from the hot components, and includes a relay that does the operation safely without bridging the internal circuit into the external world. The proper way to do that would have been with an optocoupler, but without one at hand a relay should do.&lt;/p&gt;
&lt;p&gt;With the external connector in place, it was easy to evolve the controlling circuit without bothering with the mechanical side of it. The current version is based on an atmega328p MCU that sits inside a small box exposing a high-quality LED bargraph and a single button that selects the level, turns on the machine on long press, and cancels if pressed again before the selected level is completed:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;embed-youtube&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The MCU stays on 24/7, and when unused goes back into a deep sleep mode consuming only a few microamps from an old laptop battery cell that sits within the same box.&lt;/p&gt;
&lt;p&gt;Being a for-fun exercise, the controlling logic was written in assembly to get acquainted with the details of that MCU. The short amount of code is &lt;a href=&quot;https://github.com/niemeyer/avr/blob/master/coffee-timer/main.S&quot;&gt;available&lt;/a&gt; if you are curious.&lt;/p&gt;</description>
	<pubDate>Wed, 21 Jan 2015 09:46:50 +0000</pubDate>
</item>
<item>
	<title>Gustavo Niemeyer: Inline maps in gopkg.in/yaml.v2</title>
	<guid>http://blog.labix.org/?p=2646</guid>
	<link>http://blog.labix.org/2015/01/19/inline-maps-in-gopkg-inyaml-v2</link>
	<description>&lt;p&gt;After the &lt;a href=&quot;http://blog.labix.org/2015/01/13/improvements-on-gopkg-in&quot;&gt;updates to gopkg.in&lt;/a&gt; itself, it&amp;#8217;s time for &lt;a href=&quot;http://gopkg.in/yaml.v2&quot;&gt;gopkg.in/yaml.v2&lt;/a&gt; to receive some attention. The following improvements are now available in the yaml package:&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;more-2646&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;Support for omitempty on struct values&lt;/h2&gt;
&lt;p&gt;The &lt;em&gt;omitempty&lt;/em&gt; attribute can now be used in tags of fields with a struct type. In those cases, the given field and its value only become part of the generated yaml document if one or more of the fields exported by the field type contain non-empty values, according to the usual conventions for &lt;em&gt;omitempty&lt;/em&gt; .&lt;/p&gt;
&lt;p&gt;For instance, considering these two types:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;type TypeA struct {
        Maybe TypeB `yaml:&quot;,omitempty&quot;` 
}

type TypeB struct {
        N int
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;the yaml package would only serialize the Maybe mapping into the generated yaml document if its N field was non-zero.&lt;/p&gt;
&lt;h2&gt;Support for inlined maps&lt;/h2&gt;
&lt;p&gt;The yaml package was previously able to handle the inlining of structs. For example, in the following snippet TypeB would be handled as if its fields were part of TypeA during serialization or deserialization:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;type TypeA struct {
        Field TypeB `yaml:&quot;,inline&quot;`
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This convention for inlining differs from the standard &lt;a href=&quot;http://golang.org/pkg/encoding/json&quot;&gt;json package&lt;/a&gt;, which inlines anonymous fields instead of considering such an attribute. That difference is mainly historical: the base of the yaml package was copied from mgo&amp;#8217;s &lt;a href=&quot;http://gopkg.in/mgo.v2/bson&quot;&gt;bson package&lt;/a&gt;, which had this convention before the standard json package supported any inlining at all.&lt;/p&gt;
&lt;p&gt;Now the support for inlining maps, previously available in the bson package, is also being copied over. In practice, it allows unmarshaling a yaml document such as&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;a: 1
b: 2
c: 3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;into a type that looks like&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;type T struct {
        A int
        Rest map[string]int `yaml:&quot;,inline&quot;`
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and obtaining in the resulting &lt;em&gt;Rest&lt;/em&gt; field the value &lt;em&gt;map[string]int{&amp;#8220;b&amp;#8221;: 2, &amp;#8220;c&amp;#8221;: 3}&lt;/em&gt; , while field &lt;em&gt;A&lt;/em&gt; is set to 1 as usual. Serializing out that resulting value would reverse the process, and generate the original document including the extra fields.&lt;/p&gt;
&lt;p&gt;That&amp;#8217;s a convenient way to read documents with a partially known structure and manipulating them in a non-destructive way.&lt;/p&gt;
&lt;h2&gt;Bug fixes&lt;/h2&gt;
&lt;p&gt;A few problems were also fixed in this release. Most notably:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A spurious error was reported when custom unmarshalers handled errors internally by retrying. Reported a few times and fixed by Brian Bland.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An empty yaml list ([]) is now decoded into a struct field as an empty slice instead of a nil one. Reported by Christian Neumann.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Unmarshaling into a struct with a slice field would append to it instead of overwriting. Reported by Dan Kinder.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Do not use TextMarshaler interface on types that implement yaml.Getter. Reported by Sam Ghods.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
	<pubDate>Mon, 19 Jan 2015 21:05:35 +0000</pubDate>
</item>
<item>
	<title>Gustavo Niemeyer: No minor versions in Go import paths</title>
	<guid>http://blog.labix.org/?p=2633</guid>
	<link>http://blog.labix.org/2015/01/14/no-minor-versions-in-go-import-paths</link>
	<description>&lt;p&gt;This post provides the background for a deliberate and important decision in the design of &lt;a href=&quot;http://gopkg.in&quot;&gt;gopkg.in&lt;/a&gt; that people often wonder about: while the service does support full versions in tag and branch names (as in &amp;#8220;v1.2&amp;#8243; or &amp;#8220;v1.2.3&amp;#8243;), the URL must contain &lt;em&gt;only the major version&lt;/em&gt; (as in &amp;#8220;gopkg.in/mgo.v2&amp;#8243;) which gets mapped to the best matching version in the repository.&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;more-2633&quot;&gt;&lt;/span&gt;As will be detailed, there are multiple reasons for that behavior. The critical one is ensuring all packages in a build tree that depend on the same API of a given dependency (different majors means different APIs) may use the exact same version of that dependency. Without that, an application might easily get multiple copies unnecessarily and perhaps incorrectly.&lt;/p&gt;
&lt;p&gt;Consider this example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Application &lt;em&gt;A&lt;/em&gt; depends on packages &lt;em&gt;B&lt;/em&gt; and &lt;em&gt;C&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Package &lt;em&gt;B&lt;/em&gt; depends on &lt;em&gt;D&lt;/em&gt; 3.0.1&lt;/li&gt;
&lt;li&gt;Package &lt;em&gt;C&lt;/em&gt; depends on &lt;em&gt;D&lt;/em&gt; 3.0.2&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Under that scenario, when someone executes &lt;em&gt;go get&lt;/em&gt; on application &lt;em&gt;A,&lt;/em&gt; two independent copies of &lt;em&gt;D&lt;/em&gt; would be embedded in the binary. This happens because both &lt;em&gt;B&lt;/em&gt; and &lt;em&gt;C&lt;/em&gt; have exact control of the version in use. When everybody can pick their own preferred version, it&amp;#8217;s easy to end up with multiple of these.&lt;/p&gt;
&lt;p&gt;The current gopkg.in implementation solves that problem by requiring that both &lt;em&gt;B&lt;/em&gt; and &lt;em&gt;C&lt;/em&gt; necessarily depend on the &lt;em&gt;major version&lt;/em&gt; which defines &lt;em&gt;the API version&lt;/em&gt; they were coded against. So the scenario becomes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Application &lt;em&gt;A&lt;/em&gt; depends on packages &lt;em&gt;B&lt;/em&gt; and &lt;em&gt;C&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Package &lt;em&gt;B&lt;/em&gt; depends on &lt;em&gt;D&lt;/em&gt; 3.*&lt;/li&gt;
&lt;li&gt;Package &lt;em&gt;C&lt;/em&gt; depends on &lt;em&gt;D&lt;/em&gt; 3.*&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With that approach, when someone runs &lt;em&gt;go get&lt;/em&gt; to import the application it would get the newest version of &lt;em&gt;D&lt;/em&gt; that is still compatible with both &lt;em&gt;B&lt;/em&gt; and &lt;em&gt;C&lt;/em&gt; (might be 3.0.3, 3.1, etc), and both would use that one version. While by default this would just pick up the most recent version, the package might also be moved back to 3.0.2 or 3.0.1 without touching the code. So the approach in fact empowers the person &lt;em&gt;assembling the binary&lt;/em&gt; to experiment with specific versions, and gives package authors a framework where the default behavior tends to remain sane.&lt;/p&gt;
&lt;p&gt;This is the most important reason why gopkg.in works like this, but there are others. For example, to encode the micro version of a dependency on a package, the import paths of dependent code must be patched on every single minor release of the package (internal and external to the package itself), and the code must be repositioned in the local system to please the &lt;em&gt;go&lt;/em&gt; tool. This is rather inconvenient in practice.&lt;/p&gt;
&lt;p&gt;It&amp;#8217;s worth noting that the issues above describe the problem in terms of minor and patch versions, but the problem exists and is intensified when using individual source code revision numbers to refer to import paths, as it would be equivalent in this context to having a minor version on every single commit.&lt;/p&gt;
&lt;p&gt;Finally, when you do want exact control over what builds, &lt;a href=&quot;https://github.com/tools/godep&quot;&gt;godep&lt;/a&gt; may be used as a complement to gopkg.in. That partnership offers exact reproducibility via godep, and gives people stable APIs they can rely upon over longer periods with gopkg.in. Good match.&lt;/p&gt;</description>
	<pubDate>Wed, 14 Jan 2015 17:16:54 +0000</pubDate>
</item>
<item>
	<title>Tim Callaghan: Should vegetarians open steakhouse restaurants?</title>
	<guid>tag:blogger.com,1999:blog-8043938871710850997.post-2951329559818018446</guid>
	<link>http://www.acmebenchmarking.com/2015/01/should-vegetarians-open-steakhouse.html</link>
	<description>&lt;i&gt;&lt;span&gt;&quot;Should vegetarians open steakhouse restaurants?&quot;&lt;/span&gt;&lt;/i&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Though someone will probably give me several examples of why they should, I'll argue that they absolutely should not. How can someone who doesn't eat steak convince others to eat at their &quot;steak-only&quot; restaurant?&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;But this is something a &quot;professional technology benchmarker&quot; (PTB) struggles with on a regular basis. Hello, I'm Tim Callaghan, and I'm a PTB.&lt;/span&gt;&lt;br /&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;span&gt;&lt;b&gt;professional technology benchmarker&lt;/b&gt;, or PTB (noun) : One who compares two technologies as part of their job. One of these technologies is usually the product of the PTB's employer, the other is almost always not.&lt;/span&gt;&lt;/blockquote&gt;&lt;span&gt;In a past experience I was tasked with comparing the performance of a fully in-memory database with Oracle and MySQL on a &quot;TPC-C like&quot; workload. At the time I was an Oracle expert and working for the in-memory database company, but had never started a single MySQL server in my life. At Tokutek I've run numerous comparisons of TokuDB and TokuMX against InnoDB and MongoDB. In fact it's a large part of my job, and something I really enjoy.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;In benchmarking competing technologies I &lt;b&gt;&lt;u&gt;always&lt;/u&gt;&lt;/b&gt; follow the same exact process:&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;ol&gt;&lt;li&gt;&lt;span&gt;Decide which competitive advantage to showcase (keep it simple).&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Build the benchmark (borrow from existing apps).&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Execute the benchmark (record everything).&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Publish and explain the results (blog and encourage feedback).&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;&lt;span&gt;Step 3 is where I'm always overly cautious. Here's a punch list of rules I follow:&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;To the best extent possible, make sure that the benchmark environment is fair to everyone.&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Nothing invalidates results faster than a misconfigured system.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span&gt;Capture all details about the environment and publish them in your results.&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Hardware, operating system, configuration parameters.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span&gt;Get advice from the experts on any technology you aren't an expert in.&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Minimally, show them the results of your benchmark and ask for feedback prior to publishing.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;div&gt;&lt;span&gt;In my opinion, this last bullet is the most important one. When I first started at Tokutek I was asked to improve the benchmarking. Tokutek's only product at the time was TokuDB, a MySQL storage engine competing with InnoDB. There were several resources at Tokutek to help me configure TokuDB, but InnoDB was another story. I needed to configure a few brand new servers and get benchmarking immediately.&amp;nbsp;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;Did I tear open the server boxes and run benchmarks? Nope. Rather, I called &lt;a href=&quot;http://smalldatum.blogspot.com/&quot; target=&quot;_blank&quot;&gt;my brother&lt;/a&gt;. He told me to reach out to &lt;a href=&quot;https://twitter.com/datacharmer&quot; target=&quot;_blank&quot;&gt;Giuseppe Maxia&lt;/a&gt; (&lt;a href=&quot;http://datacharmer.blogspot.com/&quot; target=&quot;_blank&quot;&gt;The Data Charmer&lt;/a&gt;) about optimally configuring CentOS servers and &lt;a href=&quot;https://twitter.com/vadimtk&quot; target=&quot;_blank&quot;&gt;Vadim Tkachenko&lt;/a&gt; (&lt;a href=&quot;http://www.percona.com/blog/author/vadim/&quot; target=&quot;_blank&quot;&gt;MySQL Performance Expert&lt;/a&gt;) about configuring InnoDB for performance.&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;Prior to reaching out to Giuseppe and Vadim, I did my homework by reading as much of their web based content as possible. I then sent them emails asking for assistance, and was amazed at how much they were willing to help. That was over 3 years ago and they are still helpful whenever I have a question.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;So where am I going with this?&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;I recently wrote a &lt;a href=&quot;http://www.acmebenchmarking.com/2015/01/can-we-improve-current-state-of.html&quot; target=&quot;_blank&quot;&gt;blog&lt;/a&gt;&amp;nbsp;titled &quot;Can we improve the current state of benchmarking?&quot;. In it I proposed ways to improve the process of technology benchmarking, primarily peer review. I discussed a mistake in the implementation of the &lt;a href=&quot;http://stssoft.com/&quot; target=&quot;_blank&quot;&gt;STSsoft&lt;/a&gt; &lt;a href=&quot;http://stssoft.com/products/stsdb-4-0/benchmark/&quot; target=&quot;_blank&quot;&gt;Database Benchmark&lt;/a&gt;, specifically how it was incorrectly checking size for TokuDB. The benchmark code was checking uncompressed size, not compressed. A simple error, and one that could have easily been reviewed and discussed prior to the putting marketing claims around compression on their website.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;Equally concerning to me in the &lt;a href=&quot;http://stssoft.com/products/stsdb-4-0/benchmark/#tests-on-hdd-drives&quot; target=&quot;_blank&quot;&gt;benchmark results&lt;/a&gt; was the insertion performance of TokuDB. The &lt;a href=&quot;http://stssoft.com/products/stsdb-4-0/&quot; target=&quot;_blank&quot;&gt;STSdb product page&lt;/a&gt; claims a &quot;10x performance improvement&quot; over Fractal Trees. Even though the particular benchmark workload was a random insertion pattern, the TokuDB &quot;REPLACE INTO&quot; optimization should have handled it with ease. Granted, the hardware for the test was not ideal as an Intel Celeron processor and single 500G 7.2K SATA hard drive.&amp;nbsp;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;So I dug in and read the benchmark code some more...&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;In their performance chart it shows STSdb 4.0 inserting at a very high rate of speed, the exit throughput looks to be just above 50000 inserts per second. The TokuDB insert performance is horribly low, it's hard to read on the graph but I'd estimate it to be around 1500 inserts per second.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;In reading the benchmark code I found the bottleneck for TokuDB's performance was the IO performance. In my test a single SATA drive showed nearly 100% IO utilization. By default, TokuDB runs fully durable meaning that every commit is followed by an fsync() operation. I'm not sure what the STSdb durability guarantee is (I'm the vegetarian in their steakhouse), but given that their documentation states that ACID is on the road-map I find it hard to believe they are performing fsync() for each commit, nor do I understand what an STSdb commit even is. I'm confident that a consumer grade SATA drive isn't going to perform more than ~100 IOPs.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;So I ran two tests. One was to shutoff fsync-on-commit behavior in TokuDB. And the benchmark ran much faster. But I like the D in ACID, so I modified the benchmark application to perform 10000 inserts per batch instead of 1000, which reduces the number of fsync() operations by 90%. The results are dramatic.&lt;/span&gt;&lt;/div&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;span&gt;&lt;i&gt;Note that I'm running on TokuDB v7.5.3 for MySQL 5.5.40, stock defaults (no TokuDB variables defined in my.cnf other than a 256M cache and directIO), on an Ubuntu 14.04 desktop with a Core i7-4790K, 32GB RAM, and an Intel 480GB SSD. The benchmark client is running in a Windows 7 Virtual Machine (VMware Workstation 11.0) on the Ubuntu desktop.&lt;/i&gt;&lt;/span&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;span&gt;Insert performance, 100 million rows, random keys, &lt;b&gt;1000&lt;/b&gt; inserts per batch.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://3.bp.blogspot.com/-JwTRrOAWxpA/VK6NxRSfQII/AAAAAAAAB4U/Ey6XAx7_jAw/s1600/1000-per-batch.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://3.bp.blogspot.com/-JwTRrOAWxpA/VK6NxRSfQII/AAAAAAAAB4U/Ey6XAx7_jAw/s1600/1000-per-batch.png&quot; height=&quot;166&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;Insert performance, 100 million rows, random keys, &lt;b&gt;10000&lt;/b&gt; inserts per batch.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://3.bp.blogspot.com/-SZAVqIOyIVU/VK6N4KDfdWI/AAAAAAAAB4c/WqwkWjbbWKE/s1600/10000-per-batch.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://3.bp.blogspot.com/-SZAVqIOyIVU/VK6N4KDfdWI/AAAAAAAAB4c/WqwkWjbbWKE/s1600/10000-per-batch.png&quot; height=&quot;166&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;Increasing the batch size from 1000 to 10000 improved TokuDB insert throughput over 3x. This is largely explained by the fact that a single SATA disk offers low IOPs, so the fsync operations were gating performance with smaller batches. Disabling fsync-on-commit makes it run even faster.&lt;/span&gt;&lt;/div&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;span&gt;&lt;i&gt;&lt;b&gt;Note&lt;/b&gt;: I can't explain why my insert performance was far higher than theirs, as I only changed the stock TokuDB configuration to be a 256M cache and directIO (to make sure this isn't an in-memory test). I'd guess it's their CPU and hard drive, but I'm not sure. And yes, I'd be happy to help figure it out.&lt;/i&gt;&lt;/span&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;span&gt;So I'm back to where I started. How can &lt;b&gt;&lt;i&gt;&lt;u&gt;I&lt;/u&gt;&lt;/i&gt;&lt;/b&gt; improve things? I'm not an expert in every competing technology I benchmark against. Yet a&lt;/span&gt;&lt;span&gt;s a professional technical benchmarker I want people to trust my results.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;For now I can only wait for others to question my results, configurations, and benchmark applications. While I'm waiting I'll continue questioning the results of my peers.&amp;nbsp;&lt;/span&gt;&lt;span&gt;And it doesn't have to be all doom-and-gloom. I'll also be pointing out when I find a great benchmark, or benchmarker, or benchmarketer.&lt;/span&gt;&lt;/div&gt;</description>
	<pubDate>Thu, 08 Jan 2015 10:39:40 +0000</pubDate>
	<author>noreply@blogger.com (Tim Callaghan)</author>
</item>
<item>
	<title>Tim Callaghan: Can we improve the current state of benchmarketing?</title>
	<guid>tag:blogger.com,1999:blog-8043938871710850997.post-3928315741521739040</guid>
	<link>http://www.acmebenchmarking.com/2015/01/can-we-improve-current-state-of.html</link>
	<description>&lt;span&gt;I'm starting off 2015 with the following New Year's Resolution, to improve the state of benchmarking.&amp;nbsp;&lt;/span&gt;&lt;span&gt;About a month ago I noticed the following &lt;/span&gt;&lt;a href=&quot;https://twitter.com/iamic/status/541908179982884864&quot; target=&quot;_blank&quot;&gt;tweet&lt;/a&gt;&lt;span&gt;:&lt;/span&gt;&lt;br /&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;span&gt;Hey &lt;b&gt;@tokutek&lt;/b&gt;, please look at this: &lt;a href=&quot;http://stssoft.com/products/stsdb-4-0/benchmark&quot;&gt;http://stssoft.com/products/stsdb-4-0/benchmark&lt;/a&gt; …. Are the benchmarks rigged or correctly done? I'm curious to know!&lt;/span&gt;&lt;/blockquote&gt;&lt;span&gt;While I've never met Ian Campbell (&lt;a href=&quot;https://twitter.com/iamic&quot; target=&quot;_blank&quot;&gt;@iamic&lt;/a&gt;) he certainly knew how to call me to action. I immediately checked out the &lt;a href=&quot;http://stssoft.com/&quot; target=&quot;_blank&quot;&gt;STSsoft website&lt;/a&gt;, the &lt;a href=&quot;http://stssoft.com/products/stsdb-4-0/benchmark/&quot; target=&quot;_blank&quot;&gt;benchmark results page&lt;/a&gt;, and the &lt;a href=&quot;http://stssoft.com/products/database-benchmark/&quot; target=&quot;_blank&quot;&gt;benchmark code itself&lt;/a&gt;. My first reaction was that something had to be wrong, as the benchmark results showed TokuDB and MyISAM requiring the same amount of disk space. FYI, MyISAM does not compress at all unless it's in a &lt;a href=&quot;http://dev.mysql.com/doc/refman/5.5/en/myisam-storage-engine.html#idm140705558016896&quot; target=&quot;_blank&quot;&gt;compressed read-only mode&lt;/a&gt;.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Needless to say it was time to figure out what was really going on with this benchmark. As was the case with the &lt;a href=&quot;http://www.acmebenchmarking.com/2014/10/dissecting-enterprisedb-nosql-benchmark.html&quot; target=&quot;_blank&quot;&gt;EnterpriseDB NoSQL Benchmark&lt;/a&gt; that I reviewed back in October, I decided to dig in, review the &lt;a href=&quot;http://www.urbandictionary.com/define.php?term=benchmarketing&quot; target=&quot;_blank&quot;&gt;benchmarketing&lt;/a&gt;, and dissect the benchmark itself.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;&lt;b&gt;Step 1: Benchmarketing Review&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;To keep my process bounded, I decided to only review the compression claims as stated on&amp;nbsp;the &lt;a href=&quot;http://stssoft.com/products/stsdb-4-0/&quot; target=&quot;_blank&quot;&gt;STSdb v4.0 product page&lt;/a&gt;. &lt;i&gt;The page also makes serious performance claims versus Fractal Tree indexing technology that I'll likely test and blog in the future.&lt;/i&gt; The page clearly states:&lt;/span&gt;&lt;br /&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;span&gt;&quot;&lt;b&gt;Up to 3x&lt;/b&gt; more compact than TokuDB.&quot;&lt;/span&gt;&lt;/blockquote&gt;&lt;span&gt;Underneath the claim is a link to the &lt;a href=&quot;http://stssoft.com/products/stsdb-4-0/benchmark/&quot; target=&quot;_blank&quot;&gt;benchmark results page&lt;/a&gt;. In the size chart at the bottom of that page it shows STSdb 4.0 at 5365 MB, MyISAM at 7051 MB, and TokuDB at 7051 MB. So I was left wondering...&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;On what planet is 5365 3x smaller than 7051?&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;For the claim to be true the STSdb size would need to be 2350 MB.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span&gt;Why is the MyISAM size exactly the same as the TokuDB size?&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;&lt;span&gt;The answer to the second question causes a serious benchmarketing issue for the vendor. The SQL that the benchmark uses to determine size was generic for both MyISAM and TokuDB. In the TokuDB case it is calculating uncompressed size, which explains why it was the same as MyISAM. This could also have been checked by looking at the size of the files on disk.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&lt;i&gt;At the bottom of this blog I've included the benchmark code change required to properly determine the size for TokuDB.&lt;/i&gt;&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;b&gt;Step 2: Run the benchmark (including the fix)&lt;/b&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Here are my results for MyISAM and TokuDB. I'm including MyISAM results to show that I'm running the benchmark properly (comparing to the posted results).&lt;/span&gt;&lt;br /&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;span&gt;&lt;i&gt;Note that I'm running on TokuDB v7.5.3 for MySQL 5.5.40, stock defaults (no TokuDB variables defined in my.cnf), on an Ubuntu 14.04 desktop with a Core i7-4790K, 32GB RAM, and an Intel 480GB SSD. The benchmark client is running in a Windows 7 Virtual Machine (VMware Workstation 11.0) on the Ubuntu desktop.&lt;/i&gt;&lt;/span&gt;&lt;/blockquote&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://3.bp.blogspot.com/-8cgg-Yj3alc/VKvmiywVKSI/AAAAAAAAB34/K4JntIEZ48I/s1600/100mm-myisam.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://3.bp.blogspot.com/-8cgg-Yj3alc/VKvmiywVKSI/AAAAAAAAB34/K4JntIEZ48I/s1600/100mm-myisam.png&quot; height=&quot;363&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://4.bp.blogspot.com/-C5T5_W0K28Y/VKvmo1Zgo8I/AAAAAAAAB4A/B9S4WOdutmk/s1600/100mm-tokudb-zlib.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://4.bp.blogspot.com/-C5T5_W0K28Y/VKvmo1Zgo8I/AAAAAAAAB4A/B9S4WOdutmk/s1600/100mm-tokudb-zlib.png&quot; height=&quot;364&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;b&gt;Step 3: Analyze the results&lt;/b&gt;&lt;br /&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;span&gt;TokuDB is &lt;b&gt;18% smaller&lt;/b&gt; than STSdb (4413 MB vs. 5365 MB).&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;b&gt;Where do we go from here?&lt;/b&gt;&lt;br /&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;span&gt;Can we do better than this? I think we can. I propose the following:&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Do everything possible to make sure you publish accurate results.&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;If something looks too good to be true, it probably is.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span&gt;Review the benchmark efforts of others.&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Even if it's not comparing to your technology, peer review is needed.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span&gt;Challenge incorrect results.&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;I welcome others to review my benchmarks and my results as it only makes the benchmark more trustworthy.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;The correct SQL to determine TokuDB size:&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;try&lt;/span&gt;&lt;br /&gt;&lt;span&gt;{&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&amp;nbsp; string tables = String.Join(&quot; OR &quot;, Enumerable.Range(0, connections.Length).Select(x =&amp;gt; String.Format(&quot;table_name = '{0}'&quot;, GetTableName(x))));&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&amp;nbsp; string query = &quot;&quot;;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&amp;nbsp; if (StorageEngine == MySQLStorageEngine.TokuDB)&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&amp;nbsp; {&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; query = String.Format(&quot;select sum(bt_size_allocated) from information_schema.TokuDB_fractal_tree_info where table_schema='{0}' and ({1});&quot;, conn.Database, tables);&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&amp;nbsp; }&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&amp;nbsp; else&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&amp;nbsp; {&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; query = String.Format(&quot;SELECT SUM(Data_length + Index_length) FROM INFORMATION_SCHEMA.TABLES WHERE table_schema = '{0}' and ({1});&quot;, conn.Database, tables);&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&amp;nbsp; }&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&amp;nbsp; IDataReader reader = conn.ExecuteQuery(query);&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&amp;nbsp; long size = 0;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&amp;nbsp; if (reader.Read())&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&amp;nbsp; &amp;nbsp; size = reader.GetInt64(0);&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&amp;nbsp; reader.Close();&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&amp;nbsp; return size;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span&gt;}&lt;/span&gt;</description>
	<pubDate>Tue, 06 Jan 2015 11:28:15 +0000</pubDate>
	<author>noreply@blogger.com (Tim Callaghan)</author>
</item>
<item>
	<title>Tim Callaghan: Benchmarking MongoDB 2.8 MMAPV1 Collection Level Locking</title>
	<guid>tag:blogger.com,1999:blog-8043938871710850997.post-2088055100193322102</guid>
	<link>http://www.acmebenchmarking.com/2014/12/benchmarking-mongodb-28-mmapv1.html</link>
	<description>&lt;span&gt;While MongoDB 2.8 introduces a formal &lt;a href=&quot;http://docs.mongodb.org/manual/release-notes/2.8/#storage-engines-improved-concurrency-document-level-locking-with-compression&quot; target=&quot;_blank&quot;&gt;storage engine&lt;/a&gt; API and brings with it the new &lt;a href=&quot;http://www.wiredtiger.com/&quot; target=&quot;_blank&quot;&gt;WiredTiger&lt;/a&gt; storage engine, it also adds collection level locking to the existing memory mapped engine (MMAPV1) which will remain the default engine until MongoDB 3.0, &lt;a href=&quot;http://www.zdnet.com/article/mongodb-cto-how-our-new-wiredtiger-storage-engine-will-earn-its-stripes/&quot; target=&quot;_blank&quot;&gt;so says Eliot&lt;/a&gt;.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;The MongoDB community has been waiting a long time for collection level locking, the &lt;a href=&quot;https://jira.mongodb.org/browse/SERVER-1240&quot; target=&quot;_blank&quot;&gt;Jira ticket&lt;/a&gt; was created on June 15, 2010. When I saw the following Facebook post I got excited to give it a spin, but unfortunately the results were extremely poor using MongoDB 2.7.8 (I assume there were other bottlenecks that hadn't yet been removed, and understand that testing early software can be hit-or-miss).&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://3.bp.blogspot.com/-1NVbbS9XpUc/VI7of1yw8GI/AAAAAAAAB3A/9mYWJ2X18Nc/s1600/mongodb-cll.png&quot;&gt;&lt;span&gt;&lt;img border=&quot;0&quot; src=&quot;http://3.bp.blogspot.com/-1NVbbS9XpUc/VI7of1yw8GI/AAAAAAAAB3A/9mYWJ2X18Nc/s1600/mongodb-cll.png&quot; /&gt;&lt;/span&gt;&lt;/a&gt;&lt;/div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Don't get me wrong, I'm a big fireworks fan. With this sort of announcement I expected to see some sort of measurement validating the improvements of going from database level locking to a collection level locking. I've been unable to see any benchmark results, so I decided to run my standard set of 5 benchmarks to compare the MMAPV1 technology in MongoDB 2.6.5 versus MongoDB 2.8.0.RC2 to see how much improvement the lock refinement has made.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Rather than go into the technical details of locks I'll assume that a user's expectation of lock refinement is as follows:&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;span&gt;&lt;i&gt;With database level locking I can only write into a single collection at a time within a single database, regardless of the number of clients. With collection level locking I'll achieve additional throughput (insert/update/delete) by concurrently operating on multiple collections.&lt;/i&gt;&lt;/span&gt;&lt;/blockquote&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;With most technologies the performance improvement with concurrent clients generally scales through a particular number of them, levels off at some point, then often times gets worse with additional client load.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Having said all of that, here are my results. In future blogs I'll be drilling deeper into specifics, but for now thought the results were interesting enough to share as is. All benchmark code is available in my GitHub: &lt;a href=&quot;https://github.com/tmcallaghan/iibench-mongodb&quot; target=&quot;_blank&quot;&gt;iibench&lt;/a&gt;, &lt;a href=&quot;https://github.com/tmcallaghan/sysbench-mongodb&quot; target=&quot;_blank&quot;&gt;sysbench&lt;/a&gt;. The benchmarks were run on a dual socket Xeon 5520 (8 hardware threads plus hyperthreading), 8 drive 10K SAS RAID10 array, 48GB RAM.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;b&gt;Benchmark 1 : iiBench : 1 insert thread&lt;/b&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;This benchmark measures the sequential insertion performance into a single collection with 3 secondary indexes using a single insert client.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;2.6.5 = 3190 inserts per second&lt;/span&gt;&lt;br /&gt;&lt;span&gt;2.8.0.RC2 = 3414 inserts per second (7% faster)&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Since this test is single threaded I didn't expect to see any performance improvement. Having said that, measurable performance increases are always nice.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;b&gt;Benchmark 2 : iiBench : 4 insert threads&lt;/b&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;This benchmark measures the sequential insertion performance into a single collection with 3 secondary indexes using four concurrent insert clients.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;2.6.5 = 3177 inserts per second&lt;/span&gt;&lt;br /&gt;&lt;span&gt;2.8.0.RC2 = 3233 inserts per second (2% faster)&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;This test uses 4 insert threads, but all of them are inserting into the same collection so collection level locking provides no benefit.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;b&gt;Benchmark 3 : iiBench : 1 insert threads plus 1 query thread&lt;/b&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;This benchmark measures the sequential insertion performance into a single collection with 3 secondary indexes using a single insert client while simultaneously querying the collection via a second client.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;2.6.5 = 2700 inserts per second&lt;/span&gt;&lt;br /&gt;&lt;span&gt;2.8.0.RC2 = 3317 inserts per second (23% faster)&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Nothing about collection level locking should have made this benchmark faster, but I measured a 23% improvement. Good stuff.&lt;/span&gt;&lt;br /&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;b&gt;Benchmark 4 : Sysbench : Greater than RAM&lt;/b&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Sysbench is an interesting workload as it tests point queries, range queries, aggregation, inserts, updates, and deletes. This test was run with 16 collections so the opportunity for parallelism enabled by collection level locking was certainly present.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;First the 16 collections are loaded with data (insert only), with 8 concurrent loader threads each loading it's own collection. This experiment loads each collection with 10 million documents, so the amount of data exceeds the available RAM in the server (and requires IO).&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Load phase (journal on, oplog off):&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;2.6.5 = 8539 inserts per second&lt;/span&gt;&lt;br /&gt;&lt;span&gt;2.8.0.RC2 = 13954 inserts per second (63% faster)&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;I'd usually report a 63% improvement as fantastic, but with 8 concurrent loaders the opportunity for improvement was significantly higher. &lt;b&gt;&lt;i&gt;This was the test that should have shown the most dramatic performance improvement.&lt;/i&gt;&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Execute phase (journal on, oplog on):&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://2.bp.blogspot.com/-MWS-SIeOEnE/VI75Z3R7PDI/AAAAAAAAB3Q/RjwHnnQqHwM/s1600/sysbench-gt-ram.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://2.bp.blogspot.com/-MWS-SIeOEnE/VI75Z3R7PDI/AAAAAAAAB3Q/RjwHnnQqHwM/s1600/sysbench-gt-ram.png&quot; height=&quot;355&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;span&gt;The largest gain was at 64 client threads, where performance improved 136%. Nice improvement, but I expected more.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;b&gt;Benchmark 5 : Sysbench : In-Memory&lt;/b&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Again, the same Sysbench workload and schema but only 1 million documents per collection so the entire workload easily fits in RAM.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Load phase (journal on, oplog off):&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;2.6.5 = 27347 inserts per second&lt;/span&gt;&lt;br /&gt;&lt;span&gt;2.8.0.RC2 = 43045 inserts per second (72% faster)&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Again, I'd usually report a 72% improvement as fantastic, but with 8 concurrent loaders the opportunity for improvement was significantly higher.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Execute phase (journal on, oplog on):&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://3.bp.blogspot.com/-nALt_oTjJcc/VI77Yh--B3I/AAAAAAAAB3c/Yh-pl36yoMw/s1600/sysbench-in-ram.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://3.bp.blogspot.com/-nALt_oTjJcc/VI77Yh--B3I/AAAAAAAAB3c/Yh-pl36yoMw/s1600/sysbench-in-ram.png&quot; height=&quot;353&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&quot;separator&quot;&gt;&lt;br /&gt;&lt;/div&gt;&lt;span&gt;The largest gain this time was at 1024 client threads, where performance improved 118%. As with the greater than RAM Sysbench, nice improvement, but I expected more.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;While there are some measurable performance improvements in the collection level locking feature in MongoDB 2.8, I wonder how much additional time and effort will go into further improving the performance. Given that document level locking is a feature not coming to the MMAPV1 storage engine at all, I assume there won't be much effort into further performance improvements. Please comment with your own tests, I'd like to understand workloads that provide more benefit from this effort.&lt;/span&gt;&lt;/div&gt;</description>
	<pubDate>Mon, 15 Dec 2014 12:25:07 +0000</pubDate>
	<author>noreply@blogger.com (Tim Callaghan)</author>
</item>
<item>
	<title>Tim Callaghan: Announcing iiBench for MySQL in Java</title>
	<guid>tag:blogger.com,1999:blog-8043938871710850997.post-231576033373297200</guid>
	<link>http://www.acmebenchmarking.com/2014/11/announcing-iibench-for-mysql-in-java.html</link>
	<description>&lt;span&gt;I just pushed the new Java based iiBench for MySQL (and Percona Server and MariaDB), the code and documentation are available now in the &lt;a href=&quot;https://github.com/tmcallaghan/iibench-mysql&quot; target=&quot;_blank&quot;&gt;iibench-mysql Github repo&lt;/a&gt;. Pull request are welcome!&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;The &lt;a href=&quot;http://www.tokutek.com/resources/technology/iibench/&quot; target=&quot;_blank&quot;&gt;history of iiBench&lt;/a&gt; goes back to the early days of &lt;a href=&quot;http://www.tokutek.com/&quot; target=&quot;_blank&quot;&gt;Tokutek&lt;/a&gt;. Since &quot;indexed insertion&quot; is a strength of &lt;a href=&quot;http://en.wikipedia.org/wiki/Fractal_tree_index&quot; target=&quot;_blank&quot;&gt;Fractal Tree indexes&lt;/a&gt;, the first iiBench was created by Tokutek in C++ back in 2008. &lt;a href=&quot;http://smalldatum.blogspot.com/&quot; target=&quot;_blank&quot;&gt;Mark Callaghan&lt;/a&gt; rewrote iiBench in Python, adding several features along the way. His version of iiBench is available in &lt;a href=&quot;https://code.launchpad.net/~mdcallag/mysql-patch/mytools&quot; target=&quot;_blank&quot;&gt;Launchpad&lt;/a&gt;.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;So why did I create a new iiBench in Java?&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Raw Speed&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;By &lt;b&gt;&lt;i&gt;eliminating the insert calls&lt;/i&gt;&lt;/b&gt; from each version I tested how many inserts per second each version. Any time spent waiting for the next batch of inserts is time that could be put toward inserting rows (and yes I understand that concurrent clients can reduce this concern).&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;All tests were run on my desktop (Intel i7-4790K). As the below graph shows, the 1 thread version of the Java iiBench is almost 4x faster than the 4 threaded Python iiBench, and the 4 thread Java version scales quite nicely.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://1.bp.blogspot.com/-jGHWnU9Clmw/VHNPMlISYqI/AAAAAAAAB2g/qifnHqTxggw/s1600/20141124-iibench-raw-inserts-per-second.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://1.bp.blogspot.com/-jGHWnU9Clmw/VHNPMlISYqI/AAAAAAAAB2g/qifnHqTxggw/s1600/20141124-iibench-raw-inserts-per-second.png&quot; height=&quot;400&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Capability&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Because of the Python's &lt;a href=&quot;https://wiki.python.org/moin/GlobalInterpreterLock&quot; target=&quot;_blank&quot;&gt;Global Interpreter Lock&lt;/a&gt;, I need to run 4 copies of the Python iiBench to create 4 loader &quot;threads&quot;. Each of these benchmark clients creates it's own log files that need to be aggregated to show cumulative insertion performance. Java threading handles it cleanly, allowing a single client application to run regardless of how many client threads are needed.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span&gt;Miscellaneous&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Not really relevant to the version or language, but I cringe every time I need to get software from Launchpad. I am not a fan of &lt;a href=&quot;http://bazaar.canonical.com/en/&quot; target=&quot;_blank&quot;&gt;Bazaar&lt;/a&gt;.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;div&gt;&lt;span&gt;Again, contributions/comments/complaints are always welcome, here or in github!&lt;/span&gt;&lt;/div&gt;</description>
	<pubDate>Mon, 24 Nov 2014 11:31:11 +0000</pubDate>
	<author>noreply@blogger.com (Tim Callaghan)</author>
</item>
<item>
	<title>Tim Callaghan: Prediction: MongoDB 2.8 storage engines and the rise of the MongoDBA</title>
	<guid>tag:blogger.com,1999:blog-8043938871710850997.post-4499484215299547934</guid>
	<link>http://www.acmebenchmarking.com/2014/11/prediction-mongodb-28-storage-engines.html</link>
	<description>&lt;span&gt;MongoDB has always been about ease of use. With nothing more than the mongod binary, starting a MongoDB server is as simple as:&lt;/span&gt;&lt;br /&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;span&gt;./mongod --dbpath=/path/to/data&lt;/span&gt;&lt;/blockquote&gt;&lt;span&gt;As a long time user of Oracle and MySQL I'm extremely impressed by just how simple this is. It certainly encourages new users to try it out.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;In MongoDB 2.6 and earlier there has only been a single &quot;storage engine&quot; available in the server. That storage engine has very few tunable parameters, so the defaults are fine for most users. If you don't like the defaults you can probably change them with a little review of the documentation.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;MongoDB 2.8 adds the ability to support an unlimited number of storage engines via a storage engine API. Using the alternative &lt;a href=&quot;http://www.wiredtiger.com/&quot; target=&quot;_blank&quot;&gt;WiredTiger&lt;/a&gt; storage engine is as simple as asking for it on the command line:&lt;/span&gt;&lt;br /&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;span&gt;./mongod --dbpath=/path/to/data --storageEngine wiredtiger&lt;/span&gt;&lt;/blockquote&gt;&lt;span&gt;Simple, right? It is if the defaults work for your use-case and infrastructure. If not, the &lt;a href=&quot;http://docs.mongodb.org/manual/release-notes/2.8/?_ga=1.161065725.190157924.1335295563&quot; target=&quot;_blank&quot;&gt;MongoDB 2.8 release notes&lt;/a&gt; point you to the &lt;a href=&quot;http://source.wiredtiger.com/2.4.1/group__wt.html#ga9e6adae3fc6964ef837a62795c7840ed&quot; target=&quot;_blank&quot;&gt;WiredTiger configuration documentation&lt;/a&gt;. My guess is that most MongoDB users who visit that page will immediately Google for help or advice (look for yourself, there are a lot of complicated parameters, many of which need to be tuned in combination). I assume that MongoDB will reduce the parameters and provide more helpful documentation in the future, but that's an interesting challenge to solve.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;So back to my prediction. MongoDB's simplicity will [no, must] change over time. Picking the correct storage engine is only the first step. It must then be tuned according to the workload and hardware. Indexing and queries continue to grow in complexity, as does the MongoDB optimizer. There have even been hints of schema enforcement in past presentations. Sounds a lot like a DBA to me, a MongoDBA.&lt;/span&gt;</description>
	<pubDate>Mon, 17 Nov 2014 11:55:29 +0000</pubDate>
	<author>noreply@blogger.com (Tim Callaghan)</author>
</item>
<item>
	<title>Tim Callaghan: Dissecting the EnterpriseDB NoSQL Benchmark</title>
	<guid>tag:blogger.com,1999:blog-8043938871710850997.post-6434210369616321023</guid>
	<link>http://www.acmebenchmarking.com/2014/10/dissecting-enterprisedb-nosql-benchmark.html</link>
	<description>&lt;span&gt;In my last &lt;a href=&quot;http://www.acmebenchmarking.com/2014/10/tokumx-vs-postgresql-in-enterprisedbs.html&quot; target=&quot;_blank&quot;&gt;blog&lt;/a&gt; I analyzed the compression portion of the&amp;nbsp;&lt;a href=&quot;http://blogs.enterprisedb.com/2014/09/24/postgres-outperforms-mongodb-and-ushers-in-new-developer-reality/&quot; target=&quot;_blank&quot;&gt;EnterpriseDB NoSQL Benchmark&lt;/a&gt;. I concluded that the data set was far too easily compressed. This, in my opinion, invalidates any conclusions originally published comparisons attempt to portray.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;In this blog I want to describe each section of the original benchmark. My goal is not to rerun and compare results with Postgres, MongoDB, or TokuMX. Rather, I want to point out exactly what each individual benchmark is doing and what could have been done differently or better.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;Before I begin I want to point out that in no way does this specific benchmark disqualify Postgres as a potential NoSQL solution. Also, EnterpriseDB benchmarked both products with stock default configurations. I'm not sure why &lt;u&gt;not following&lt;/u&gt; the most basic &quot;best practices&quot; is a good idea. An example of this is &lt;a href=&quot;http://docs.mongodb.org/manual/administration/production-notes/#recommended-configuration&quot; target=&quot;_blank&quot;&gt;turning down the readahead&lt;/a&gt; for MongoDB.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&lt;b&gt;Benchmark - Prepare&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;For a given number of documents to insert, a two large flat files are created:&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;File 1 is pure JSON, in &quot;batches&quot; of 11 base documents, within which a small percentage of the document is variable and the rest is fixed. This file is appropriate for bulk loading. It is important to note that the &quot;description&quot; field of each document is several kilobytes in length and unchanged in each batch. Bottom line, these documents are highly compressible.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;File 2 is a single document insert statement appropriate to the destination platform (Postgres or MongoDB). While the format is different than file 1, the content of the rows is much the same and highly compressible. Note that neither Postgres nor MongoDB is generating extended inserts, just a single insert per statement.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;&lt;b&gt;Benchmark - Data Load&lt;/b&gt;&lt;br /&gt;&lt;span&gt;The data load benchmark is simply timing the process of bulk loading file 1 from the prepare phase. No indexes exist prior to loading, so this is a primary key only test. And since it's bulk loading a single file with a single loader, there is no concurrency for this test.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&lt;b&gt;Benchmark - Index???&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;You won't find this benchmark result listed because for some reason the &quot;create index&quot; step after bulk loading isn't timed. I'm not going to run this test myself to see which product won, but it seems crazy to me that this step wasn't timed. I'd like to understand this oversight, but the comment capability on the EnterpriseDB blog is now disabled. In any event, 3 indexes are created each on a different field: name, type, and brand.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&lt;b&gt;Benchmark - Select&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;I believe that a query workload should be part of any new benchmark, so it's nice to see that queries are included here. In a single thread four large queries are executed in order, each of which is matching 9% to 18% of the data set. It would be nice to see a more selective set of queries, plus some concurrency on this test.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&lt;b&gt;Benchmark - Insert&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;After the select benchmark completes, the table/collection is dropped and reloaded using file 2 from the prepare phase. This benchmark is different from the data load benchmark in that it does not utilize the servers bulk load functionality. I assume this is intended to show how the server will perform long term (after the initial data load completes), but a lack of indexes and no concurrency make this step largely uninteresting.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;b&gt;Benchmark - Size&lt;/b&gt;&lt;br /&gt;&lt;span&gt;After the data was bulk loaded (Data Load) a command is executed to determine how much disk space the unindexed form is consuming.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/span&gt;&lt;b&gt;&lt;span&gt;Overall Thoughts&lt;/span&gt;&lt;/b&gt;&lt;br /&gt;&lt;span&gt;While I don't think this benchmark will become a standard any time soon, there are some simple improvements to make the results more interesting:&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Bulk loading without indexes is not necessarily a bad idea, but the time required to build indexes after the load completes should be measured and reported.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Size on disk should include secondary indexes.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Some of the select workload should be randomly selecting more specific data. Put it in a loop and make it concurrent.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;Prior to starting the insert benchmark, create the secondary indexes. Also, make this test concurrent by instantiating several insert clients.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;</description>
	<pubDate>Mon, 27 Oct 2014 09:45:58 +0000</pubDate>
	<author>noreply@blogger.com (Tim Callaghan)</author>
</item>
<item>
	<title>Tim Callaghan: TokuMX vs. PostgreSQL in EnterpriseDB's NoSQL Compression Benchmark</title>
	<guid>tag:blogger.com,1999:blog-8043938871710850997.post-8640088732703339828</guid>
	<link>http://www.acmebenchmarking.com/2014/10/tokumx-vs-postgresql-in-enterprisedbs.html</link>
	<description>&lt;span&gt;Since this is my first blog I feel it's necessary to introduce myself. I'm Tim Callaghan, I work at &lt;a href=&quot;http://www.tokutek.com/&quot; target=&quot;_blank&quot;&gt;Tokutek&lt;/a&gt; (makers of &lt;a href=&quot;http://www.tokutek.com/tokudb-for-mysql/&quot; target=&quot;_blank&quot;&gt;TokuDB&lt;/a&gt; and &lt;a href=&quot;http://www.tokutek.com/tokumx-for-mongodb/&quot; target=&quot;_blank&quot;&gt;TokuMX&lt;/a&gt;), and I love benchmarking. While some of the content on this blog will certainly be about Tokutek technologies, I plan on exploring a wide variety of others as well. These are strictly my own personal views and opinions, and comments/feedback are always welcome. Lets get started...&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;A few weeks ago I noticed an &lt;a href=&quot;http://blogs.enterprisedb.com/2014/09/24/postgres-outperforms-mongodb-and-ushers-in-new-developer-reality/&quot; target=&quot;_blank&quot;&gt;EnterpriseDB NoSQL Benchmark&lt;/a&gt; that measured Data Load, Insert, Select, and Size. It wasn't just a NoSQL benchmark, it was specifically calling out &lt;a href=&quot;http://www.mongodb.com/&quot; target=&quot;_blank&quot;&gt;MongoDB&lt;/a&gt; by declaring &quot;Postgres Outperforms MongoDB and Ushers in New Developer Reality&quot;. Now the &lt;a href=&quot;http://blogs.enterprisedb.com/author/marc_linster/&quot; target=&quot;_blank&quot;&gt;blogger&lt;/a&gt; had my attention, I needed to learn more, so I dug in.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;I was concerned with how little the blog explained about the benchmark itself, the results were only presented as &quot;Relative Performance Comparisons&quot;. Was this an example of what &lt;a href=&quot;http://smalldatum.blogspot.com/&quot; target=&quot;_blank&quot;&gt;Mark Callaghan&lt;/a&gt; describes as &lt;a href=&quot;http://smalldatum.blogspot.com/2014/06/benchmarketing.html&quot; target=&quot;_blank&quot;&gt;benchmarketing&lt;/a&gt;?&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;I've created far too many benchmark applications in my life, and analyzed countless more. To EnterpriseDB's credit, they published their &lt;a href=&quot;https://github.com/EnterpriseDB/pg_nosql_benchmark&quot; target=&quot;_blank&quot;&gt;benchmark code&lt;/a&gt; on GitHub. After downloading the code and opening my editor I saw something I never would have imagined, a benchmark written completely in bash. &lt;i&gt;Yes, bash.&lt;/i&gt; I used to think I was a bash expert, but now I know differently.&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;The compression portion of the benchmark is simple. An extremely large JSON data set is created as a flat file, the flat file is loaded into MongoDB and Postgres, and the on-disk size is recorded for the compression comparison. Since the TokuMX binaries use the same names as MongoDB, I was able to easily run the test comparing TokuMX and Postgres. The blog stated that benchmark was intended to run without additional tuning, so I setup TokuMX to start with it's default configuration.&amp;nbsp;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&lt;b&gt;Results&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;http://1.bp.blogspot.com/-r7jAN2KZ2PI/VET_UZOA-gI/AAAAAAAAB1c/QhAmRNFkzEk/s1600/20141020-ab-nosql-compression.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://1.bp.blogspot.com/-r7jAN2KZ2PI/VET_UZOA-gI/AAAAAAAAB1c/QhAmRNFkzEk/s1600/20141020-ab-nosql-compression.png&quot; height=&quot;400&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&lt;b&gt;Details&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;The 10 million document raw file 25,287 MB. Each technology is compared here:&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Postgres 9.4 beta2 required 14,204 MB (56.17% raw size).&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Postgres 9.4 beta3 &lt;u&gt;also&lt;/u&gt; required 14,204 MB (56.17% raw size). This surprised me, as I tested this newer beta of Postgres specifically because of the following release note:&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;&lt;b&gt;&lt;i&gt;The JSON data set produced by this benchmark application contains very large amounts of redundant text. As a matter of fact, there are 11 &quot;base&quot; documents in total, each of which is only slightly modified with a random handful of numeric characters for each batch of 11 documents. So each batch of 11 documents is 99% identical to every other batch, and each batch is about 29KB in total.&lt;/i&gt;&lt;/b&gt;&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;MongoDB 2.6.4 required 41,035 MB (162.28% raw size). This is a well known behavior of MongoDB, as the default for &lt;a href=&quot;http://docs.mongodb.org/manual/core/storage/#power-of-2-sized-allocations&quot; target=&quot;_blank&quot;&gt;Power of 2 Sized Allocations&lt;/a&gt; was changed to &quot;enabled in MongoDB 2.6. This sizing strategy is meant to improve the chances that a document update can be done in-place, and thus reduce index updates.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;Disabling Power of 2 Sized Allocations in MongoDB 2.6.4 reduced the required space to 27,871 MB (110.22% of raw size).&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;TokuMX 2.0.0 required 1,199 MB (4.74% raw size).&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;&lt;span&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;b&gt;Thoughts and Analysis&lt;/b&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;TokuMX compression was the winner, hands down. Compared to Postgres, TokuMX required 91.56% less space on disk (1,199 MB vs. 14,204 MB). Nothing more to say here, it wasn't even close.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;span&gt;But lets be honest, &lt;b&gt;&lt;i&gt;this&amp;nbsp;&lt;/i&gt;&lt;/b&gt;&lt;/span&gt;&lt;span&gt;&lt;b&gt;&lt;i&gt;compression benchmark is not interesting!&lt;/i&gt;&lt;/b&gt;&amp;nbsp;The data is simply too easily compressed. It shows that you should never accept benchmark results without understanding the benchmark itself. Plus you should make sure that the results are interesting to your specific use-case.&lt;/span&gt;&lt;/div&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;I plan on reviewing load and query aspects of the benchmark in an upcoming blog. &lt;/span&gt;</description>
	<pubDate>Mon, 20 Oct 2014 10:52:54 +0000</pubDate>
	<author>noreply@blogger.com (Tim Callaghan)</author>
</item>
<item>
	<title>Baron Schwartz: MySQL isn't limited to nested-loop joins</title>
	<guid>http://www.xaprb.com/blog/2013/10/01/mysql-isnt-limited-to-nested-loop-joins/</guid>
	<link>http://www.xaprb.com/blog/2013/10/01/mysql-isnt-limited-to-nested-loop-joins/</link>
	<description>&lt;p&gt;I have followed the &amp;ldquo;Use the Index, Luke!&amp;rdquo; blog for a while. Today Marcus &lt;a href=&quot;http://use-the-index-luke.com/blog/2013-10-01/mysql-is-to-sql-like-mongodb-to-nosql&quot;&gt;wrote&lt;/a&gt; that (I&amp;rsquo;ll paraphrase) MongoDB disgraces NoSQL the same way that MySQL disgraces SQL. I agree with a lot of this, actually, although I&amp;rsquo;m not sure I&amp;rsquo;d put it so strongly. People often like products for good reasons, and to think that legions of developers are stupid or ill-educated is suspect, in my opinion.&lt;/p&gt;

&lt;p&gt;But that wasn&amp;rsquo;t what I meant to write about. I wanted to point out something about the blog post that&amp;rsquo;s a little outdated. He wrote, and this time I&amp;rsquo;ll quote, &amp;ldquo;MySQL is rather poor at joining because is only supports nested loops joins. Most other SQL database implement the hash join and sort/merge join algorithms too.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s no longer true that MySQL doesn&amp;rsquo;t support these, and hasn&amp;rsquo;t been for a while, depending on which version of MySQL you look at. What&amp;rsquo;s slightly unfortunate, in my opinion, is that MySQL doesn&amp;rsquo;t call out in the documentation that they&amp;rsquo;re actually implemented. MySQL documentation talks about Multi-Range Read, Block Nested-Loop, and Batched Key Access join &amp;ldquo;optimizations.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Functionally, these are closely related to combinations of hash and sort-merge join algorithms, and really represent mixtures of features from them combined in different ways, depending on the exact query. Most &amp;ldquo;sophisticated&amp;rdquo; RDBMSs also implement a lot of subtle variations &amp;ndash; edge-case optimizations are really worthwhile. It is rarely as cut-and-dried as pure hash-join or sort-merge join. And in the end, there is always &amp;ndash; always &amp;ndash; iteration over rows to match them up, regardless of the data structure used, regardless of the RDBMS. MySQL happens to call these variations &amp;ldquo;nested loop join optimizations&amp;rdquo; and similar phrases, but that&amp;rsquo;s what they are in other RDBMSs too.&lt;/p&gt;

&lt;p&gt;MySQL does very well on many types of joins for which sort-merge and hash-join algorithms are designed. See, for example, &lt;a href=&quot;http://www.mysqlperformanceblog.com/2012/03/21/multi-range-read-mrr-in-mysql-5-6-and-mariadb-5-5/&quot;&gt;this blog post&lt;/a&gt; and &lt;a href=&quot;http://www.mysqlperformanceblog.com/2012/03/12/index-condition-pushdown-in-mysql-5-6-and-mariadb-5-5-and-its-performance-impact/&quot;&gt;this one&lt;/a&gt; and also &lt;a href=&quot;http://www.mysqlperformanceblog.com/2012/05/31/a-case-for-mariadbs-hash-joins/&quot;&gt;this one on MariaDB&amp;rsquo;s further optimizations&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I think the MySQL documentation could help a little by calling things names that normal users understand. The names we see in the documentation are really reflective of how the optimizer internals gurus think about the algorithms, in my opinion. I think the names describe the implementation, not the end result. I&amp;rsquo;d suggest phrasing it differently for general consumption by the DBA public. Perhaps something like &amp;ldquo;sort-merge join implemented with a _____ algorithm.&amp;rdquo; Or perhaps &amp;ndash; and I will admit I don&amp;rsquo;t keep the details fresh in my mind so I&amp;rsquo;m not the one to ask for the right answer &amp;ndash; perhaps the algorithms MySQL uses really aren&amp;rsquo;t as related or comparable as I think they are, and a different type of explanation is in order. But I bet a lot of DBAs from SQL Server and Oracle Database backgrounds would find it helpful to have an explanation in familiar terms. (This concludes my free and probably unwanted advice to the MySQL docs team!)&lt;/p&gt;</description>
	<pubDate>Tue, 01 Oct 2013 00:00:00 +0000</pubDate>
</item>
<item>
	<title>Baron Schwartz: What TokuDB might mean for MongoDB</title>
	<guid>http://www.xaprb.com/blog/2013/04/29/what-tokudb-might-mean-for-mongodb/</guid>
	<link>http://www.xaprb.com/blog/2013/04/29/what-tokudb-might-mean-for-mongodb/</link>
	<description>&lt;p&gt;Last week &lt;a href=&quot;http://www.tokutek.com/&quot;&gt;Tokutek&lt;/a&gt; &lt;a href=&quot;http://www.tokutek.com/2013/04/announcing-tokudb-v7-open-source-and-more/&quot;&gt;announced&lt;/a&gt; that they&amp;rsquo;re open-sourcing their TokuDB storage engine for MySQL. If you&amp;rsquo;re not familiar with TokuDB, it&amp;rsquo;s an ACID-compliant storage engine with a high-performance index technology known as fractal tree indexing. Fractal trees have a number of nice characteristics, but perhaps the most interesting is that they deliver consistently high performance under varying conditions, such as when data grows much larger than memory or is updated frequently. B-tree indexes tend to get fragmented over time, and exhibit a performance cliff when data doesn&amp;rsquo;t fit in memory anymore.&lt;/p&gt;

&lt;p&gt;The MySQL community is excited about having access to TokuDB&amp;rsquo;s source code, and rightly so. TokuDB is, broadly speaking, aimed at the same category of use cases as Oracle&amp;rsquo;s InnoDB, which has been MySQL&amp;rsquo;s leading storage engine for a long time.&lt;/p&gt;

&lt;p&gt;MySQL&amp;rsquo;s market size is large for an opensource product (roughly $500M to $1B USD, depending on who you talk to), and in a big pond, a stone causes wide ripples. I think the most significant implications, though, are for MongoDB. Tokutek has published a series of &lt;a href=&quot;http://www.tokutek.com/tag/mongodb/&quot;&gt;benchmarks of MongoDB performance with TokuDB&lt;/a&gt; as the storage engine instead of MongoDB&amp;rsquo;s default storage engine. The results are compelling.&lt;/p&gt;

&lt;p&gt;I think TokuDB will rapidly become the storage engine of choice for MongoDB, and could catapult MongoDB into the lead in the NoSQL database arena. This would have profound implications for opensource databases of all flavors, not just NoSQL databases.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s worth revisiting a bit of ancient history for some context.&lt;/p&gt;

&lt;p&gt;Way back in the olden days, MySQL&amp;rsquo;s main storage engine was MyISAM. MyISAM is non-transactional and has table-level locking, meaning that a write (update, insert, delete, or similar) blocked all concurrent access to the table. This is okay for some uses, and can even be very good in special cases, but in the general case it is a disaster. MyISAM introduced some special workarounds for common cases (such as permitting nonblocking inserts to occur at the end of the table), but in the end, you can&amp;rsquo;t fix table-level locking. A mixed workload needs storage that&amp;rsquo;s designed for high read and write concurrency without blocking.&lt;/p&gt;

&lt;p&gt;MyISAM had other problems, such as lacking transactions, being prone to data corruption, and long repair times after a crash.&lt;/p&gt;

&lt;p&gt;As a result, MySQL as a whole was only interesting to a minority of users. For demanding applications it was little more than a curiosity.&lt;/p&gt;

&lt;p&gt;Then came InnoDB. InnoDB introduced ACID transactions, automatic crash recovery, and most importantly, row-based locking and MVCC, which allowed highly concurrent access to rows, so readers and writers don&amp;rsquo;t block each other. InnoDB was the magic that made MySQL a credible choice for a wide range of use cases.&lt;/p&gt;

&lt;p&gt;Most of the interesting chapters in MySQL&amp;rsquo;s history have involved InnoDB in one way or another. To list some highlights: Oracle bought InnoDB&amp;rsquo;s creator Innobase Oy, MySQL scrambled to find a replacement (Maria, Falcon, PBXT), Sun&amp;rsquo;s decision to acquire MySQL was said to be influenced by Falcon, Percona created XtraDB, and Oracle acquired Sun. Things are settling down now, but it&amp;rsquo;s easy to forget how much of a soap opera the MySQL world has lived through because of InnoDB not being owned by MySQL.&lt;/p&gt;

&lt;p&gt;And in the middle of all this came NoSQL databases. In the past half-dozen years, more databases have been invented, popularized, and forgotten than I care to think about. In many cases, though, these databases were criticized as ignoring or reinventing (badly) decades of learning in relational database technology, and even computer science in general. I know I&amp;rsquo;ve looked at my share of face-palm code.&lt;/p&gt;

&lt;p&gt;Databases, by and large, depend on reliable, high-performance storage and retrieval subsystems more than anything else. Many of the NoSQL databases have interesting ideas built on top of bad, bad, bad storage code.&lt;/p&gt;

&lt;p&gt;MongoDB is a case in point. MongoDB reinvented some of the worst parts of MySQL all over again. Storage was initially little more than mmap over a file. I think Mark Callaghan put it best in 2009, when he said &amp;ldquo;Reinventing MyISAM is not a feature.&amp;rdquo; MongoDB&amp;rsquo;s storage at that time really was MyISAM-like. It&amp;rsquo;s improved somewhat since then, but it hasn&amp;rsquo;t had the wholesale rip-and-replace improvement that I think is needed. Not only that, but MongoDB as a whole is still (predictably) built around the limitations of the underlying storage, with coarse-grained locking.&lt;/p&gt;

&lt;p&gt;But MongoDB, like MySQL, has been relevant in spite of these shortcomings. Form your own opinion about why this is, but from my point of view there are two main reasons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;MongoDB was born in an era when the popular databases were frustratingly slow and clunky to work with, and innovation was stalled due to the political drama surrounding them.&lt;/li&gt;
&lt;li&gt;MongoDB simply feels nice to developers. If you&amp;rsquo;re not a developer, this is a little hard to explain, but it just feels good, like your favorite pair of jeans. Like a hug from a good friend. Like a hammock and a summer day. The difference between an SQL database and MongoDB for many developers is like the difference between an iPod and a cheap knockoff MP3 player. I could go on and on.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It&amp;rsquo;s difficult to overstate the importance of this, because it means that MongoDB may well become an enterprise database, despite what bad opinions you may have about it now. Why is this? It&amp;rsquo;s because developers are king in the modern IT enterprise. Developers determine what technologies get adopted in IT. CTOs like to think the decisions come from the top down, but I&amp;rsquo;ve seen it work the other way time and time again. Developers start to use something that frustrates them less than the alternatives, and a groundswell begins that&amp;rsquo;s impossible to stop. Someday the CTO discovers that the question of whether to use technology X was decided by a junior developer long ago and deployed to production, and now it&amp;rsquo;s too late.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve done it myself. At Crutchfield I hijacked the company-wide policy that migration from legacy VB6 to .NET would proceed along the lines of a transition to VB.NET. I was fighting through awful code day in and day out, and I knew that a more restrictive language would prevent a lot of bad practices. So I wrote several major systems in C# without asking permission. It&amp;rsquo;s a lot easier to get forgiveness than permission. Then I showed off what I&amp;rsquo;d done. When I left Crutchfield, the IT department had chosen C#, not VB.NET, as its language of the future (even though there were, and probably still are, major VB.NET applications).&lt;/p&gt;

&lt;p&gt;Similarly, at Crutchfield I was provided a 15-inch CRT monitor to work on. This was 2003, you understand. Even at that time, it was awful. How can you expect a developer to work on a flickering, small monitor? I bought my own large-screen LCD and put it in my cubicle. Management ordered me to remove it because it was causing a flood of &amp;ldquo;hey, how did Baron get a nice monitor?&amp;rdquo; questions, but the camel already had a nose under the tent. I took my monitor home, but not too long after that we all started to get nicer monitors. I brought my own nice chair to work, too. All told I probably forced Crutchfield to spend thousands of dollars upgrading equipment. You have to be careful about headstrong kids like me &amp;ndash; don&amp;rsquo;t turn your backs on us for a moment.&lt;/p&gt;

&lt;p&gt;This story illustrates why MongoDB is likely to become a major database: because developers enjoy working with it. It feels pleasant and elegant. Remember, most technology decisions are based on how people feel, not on facts. We&amp;rsquo;re not rational beings, so don&amp;rsquo;t expect the best solution to win. Expect people to choose what makes them happy.&lt;/p&gt;

&lt;p&gt;And with the availability of TokuDB, MongoDB is lovable by a lot more people. With reliable storage and transactions, uncool kids can like it too.&lt;/p&gt;

&lt;p&gt;It goes further than just the storage engine. The kernel of MongoDB has code that needs to be fixed, such as the coarse-grained locking code. Tokutek basically forked MongoDB in order to insert TokuDB into it. They had to, in order to get all that locking out of the way and allow MongoDB to shine with TokuDB on the backend.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m not sure exactly how this will play out &amp;ndash; will Tokutek start offering a competitive product? Will there be opensource community-based forks of MongoDB that integrate TokuDB? Will 10gen do the engineering to offer TokuDB as a backend? Will 10gen and Tokutek partner to do the engineering and provide support? Will 10gen acquire Tokutek? Will a large company acquire both? You decide.&lt;/p&gt;

&lt;p&gt;But I believe that a few things are inevitable, and don&amp;rsquo;t require a crystal ball to guess.&lt;/p&gt;

&lt;p&gt;Anyone who cares about MongoDB is going to be using TokuDB as their storage backend within a matter of months. It&amp;rsquo;s happened before &amp;ndash; look at what happened to MySQL and InnoDB. Look at Riak; people dropped Bitcask like a hot potato when LevelDB storage arrived (although it hasn&amp;rsquo;t been a perfect solution).&lt;/p&gt;

&lt;p&gt;Just to be clear, I do not think that MongoDB&amp;rsquo;s parallels with MySQL&amp;rsquo;s history must inevitably repeat in all aspects of the story. The world of databases today (big data, cloud, mobile) is not in the same situation it was when MySQL was creeping into general awareness (web, gaming, social, general lack of good alternatives to commercial databases), and the reasons people use MongoDB now are different from the reasons people chose MySQL back in the day. Still, there&amp;rsquo;s a good chance that MySQL&amp;rsquo;s past can teach us about MongoDB&amp;rsquo;s future, and for some use cases, MongoDB deployments will soon accelerate rapidly. I expect a larger commercial ecosystem to emerge, too; right now the MongoDB market is worth tens of millions, and I&amp;rsquo;d guess in a few years we&amp;rsquo;ll look back and see a sharp inflection point in 2013 and 2014. TokuDB could help propel MongoDB&amp;rsquo;s market size into hundreds of millions of dollars, which is a position occupied uniquely by MySQL today in the opensource database world.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.youtube.com/watch?v=2UFc1pr2yUU&quot;&gt;It&amp;rsquo;s getting real&lt;/a&gt; in the MongoDB world &amp;ndash; this is going to be interesting to watch.&lt;/p&gt;</description>
	<pubDate>Mon, 29 Apr 2013 00:00:00 +0000</pubDate>
</item>
<item>
	<title>Baron Schwartz: Bold predictions on which NoSQL databases will survive</title>
	<guid>http://www.xaprb.com/blog/2013/01/10/bold-predictions-on-which-nosql-databases-will-survive/</guid>
	<link>http://www.xaprb.com/blog/2013/01/10/bold-predictions-on-which-nosql-databases-will-survive/</link>
	<description>&lt;p&gt;In case you&amp;rsquo;ve been &lt;a href=&quot;http://www.youtube.com/watch?v=cvXqm0RdJms&quot;&gt;living under a rock&lt;/a&gt; for the last 5 years, the NoSQL movement has changed. There was a time when everyone &amp;ndash; EVERYONE &amp;ndash; was dumping on relational databases, and MySQL in particular. Nonsense like &amp;ldquo;SQL itself is inherently unscalable&amp;rdquo; routinely came out of the mouths of otherwise usually sensible people. But that&amp;rsquo;s cooled off a little bit, thank heavens.&lt;/p&gt;

&lt;p&gt;And what&amp;rsquo;s the new hotness? Well, Big Data, of course! But I digress. In the world of databases, it&amp;rsquo;s move over NoSQL, heeeeeere&amp;rsquo;s NewSQL. I&amp;rsquo;m talkin&amp;rsquo; NuoDB, Clustrix, MySQL Cluster (NDB), and so forth. A lot of people now recognize that it wasn&amp;rsquo;t SQL or the relational model that was a problem &amp;ndash; it was the implementations that had some issues. The pendulum has swung a little away from vilifying SQL, and we don&amp;rsquo;t talk about NoSQL as much as we talk about document-oriented or key-value or whatever.&lt;/p&gt;

&lt;p&gt;Does that spell death for NoSQL databases? Not in my opinion. But I am just in the mood to stick my neck out a bit today, so I&amp;rsquo;m going to do something I don&amp;rsquo;t normally do &amp;ndash; predict the future. Here&amp;rsquo;s my prediction: &lt;strong&gt;there may be many NoSQL databases that live long and healthy lives, but among them will probably be MongoDB, Redis, and Riak&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Discuss!&lt;/p&gt;

&lt;p&gt;PS: this prediction is about what I think will happen. If I get one out of three right, I&amp;rsquo;ll be happy. It&amp;rsquo;s not an endorsement of any database, dismissal of any other database, or an opinion about what I think &lt;em&gt;should&lt;/em&gt; happen. Limitations and exclusions apply. Subject to credit approval, see store for details.&lt;/p&gt;</description>
	<pubDate>Thu, 10 Jan 2013 00:00:00 +0000</pubDate>
</item>
<item>
	<title>Baron Schwartz: Avoiding statement-based replication warnings</title>
	<guid>http://www.xaprb.com/blog/2012/08/23/avoiding-statement-based-replication-warnings/</guid>
	<link>http://www.xaprb.com/blog/2012/08/23/avoiding-statement-based-replication-warnings/</link>
	<description>&lt;p&gt;Although not perfect, MySQL replication was probably the killer feature that made MySQL the default database for web applications some time ago. Since then, MySQL replication has been improved greatly, with such notable changes as row-based replication. At the same time, the replication engineering team has made MySQL replication more conservative and less forgiving of foot-gun errors. These have gone a long way towards helping users avoid some of the problems that made replication sometimes drift out of sync with the master copy, sometimes silently.&lt;/p&gt;

&lt;p&gt;In some cases I think the strictness has gone a little too far. One example is the server&amp;rsquo;s identification of statements that are unsafe for replication because they are nondeterministic. Here is a statement in an application I manage, which is designed to claim some work from a queue. After running this statement, the application checks if any rows were affected, and if so, it then fetches and processes the rows:&lt;/p&gt;

&lt;pre&gt;update pending_jobs set token = ?
where token is null
  and (owner_pid is null or owner_pid &lt;pre&gt; ?)
order by id
limit 1;&lt;/pre&gt;

&lt;p&gt;MySQL will write to the server&amp;rsquo;s error log when this statement is issued and binlog_format=STATEMENT, because of the presence of a LIMIT in the statement: &lt;em&gt;120823 20:59:12 [Warning] Unsafe statement written to the binary log using statement format since BINLOG_FORMAT = STATEMENT. The statement is unsafe because it uses a LIMIT clause. This is unsafe because the set of rows included cannot be predicted. Statement: [statement follows]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This becomes a problem very quickly, because in fact the statement is deterministic and the rows to be affected can be predicted perfectly. The server is just being overly strict. The general technique illustrated here is a superior alternative to some other ways of &lt;a href=&quot;http://www.engineyard.com/blog/2011/5-subtle-ways-youre-using-mysql-as-a-queue-and-why-itll-bite-you/&quot;&gt;implementing a queue in a database table&lt;/a&gt;. But if a superior alternative floods the error log with spurious messages, it must be avoided anyway.&lt;/p&gt;

&lt;p&gt;The solution I chose in this case is a blend of SQL and application code. Part of the logic &amp;ndash; the limit &amp;ndash; must be handled in the application code, and pulled out of the UPDATE statement so the server will consider it to be deterministic. Here is pseudocode for the result:&lt;/p&gt;

&lt;pre&gt;
function claim_a_job() {
   $pid   = get_pid();
   $token = md5(rand(), time(), $pid);
   @jobs  = query(
            &quot;select id from pending_jobs
             where token is null and (owner_pid is null or owner_pid &lt;pre&gt; ?)
             order by id&quot;, $pid);
   foreach ( $job in @jobs ) {
      next unless query(&quot;update pending_jobs set token=?
                         where token is null and id=?&quot;, $token, $job);
      return $job;
   }
   return null;
}
&lt;/pre&gt;

&lt;p&gt;This code finds all unclaimed rows and tries to claim each one in turn. If there&amp;rsquo;s a race condition and another worker has claimed the job in the meantime, no rows will be updated. If the UPDATE affects a row, then the function claimed the job successfully, and the job&amp;rsquo;s ID is returned. The most important thing, however, is that the SQL lacks any constructs such as LIMIT that might cause errors to be spewed into the log. I want my logs to be silent so that I can detect when something really important actually happens.&lt;/p&gt;

&lt;p&gt;Percona Server has a feature to disable logging this warning, which is a mixed blessing. I want to find all such queries and examine them, because some of them might be a legitimate risk to replication integrity. If I disable the logging, it becomes much harder, though I can potentially do it by inspecting TCP traffic instead. I do wish that official MySQL supported the ability to silence warnings selectively, however.&lt;/p&gt;

&lt;p&gt;Another possible solution would be to switch to row-based binary logging, which comes with many other benefits as well. But such a change is not to be taken lightly; it requires a careful assessment of the server and its workload, lest there be unintended consequences.&lt;/p&gt;

&lt;p&gt;An even better solution would be to implement some additional features in the server. Many of the features that developers like the most about NoSQL databases such as MongoDB and Redis (or even PostgreSQL) are special-case behaviors to simplify things that are awkward to do in most databases. Examples include atomically adding and removing from a queue, and features to avoid polling, such as LISTEN and NOTIFY.&lt;/p&gt;&lt;/pre&gt;&lt;/pre&gt;</description>
	<pubDate>Thu, 23 Aug 2012 00:00:00 +0000</pubDate>
</item>
<item>
	<title>Baron Schwartz: Automated, integrated sharding: the new killer database feature</title>
	<guid>http://www.xaprb.com/blog/2012/04/09/automated-integrated-sharding-the-new-killer-database-feature/</guid>
	<link>http://www.xaprb.com/blog/2012/04/09/automated-integrated-sharding-the-new-killer-database-feature/</link>
	<description>&lt;p&gt;MySQL became wildly successful in part because it had built-in, simple replication. Sure, it had lots of interesting failure scenarios and was not great at first &amp;mdash; it is much better these days &amp;mdash; but it was nevertheless successful because there was a single, out-of-the-box, not-very-complex way to do replication. I have opined many times before that this was one of the killer features missing from PostgreSQL. I think that can large explain why MySQL became more popular more quickly.&lt;/p&gt;

&lt;p&gt;The new killer feature is automatic sharding, in my opinion. If you&amp;rsquo;re not accustomed to the word, &amp;ldquo;sharding&amp;rdquo; means partitioning of a large dataset across many servers.&lt;/p&gt;

&lt;p&gt;It is easy to poke fun at &lt;a href=&quot;http://www.mongodb.org/&quot;&gt;MongoDB&amp;rsquo;s&lt;/a&gt; current limitations, but for all that, it has a story to tell about sharding. There is One Right Way To Do It in MongoDB, and it&amp;rsquo;s a part of the product.&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t see sharding being added into the core of MySQL itself, but there are some very interesting efforts headed towards MySQL. There are at least the following companies providing sharding via a proxy or middleware solution, with a lot of other features also available in some products:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.scalebase.com/&quot;&gt;Scalebase&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.scalearc.com/&quot;&gt;ScaleArc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.dbshards.com/&quot;&gt;dbShards&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.parelastic.com/&quot;&gt;ParElastic&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition, there are community-based efforts, such as &lt;a href=&quot;http://code.google.com/p/shard-query/&quot;&gt;Shard-Query&lt;/a&gt; and the &lt;a href=&quot;http://spiderformysql.com/&quot;&gt;Spider&lt;/a&gt; storage engine. And there&amp;rsquo;s &lt;a href=&quot;http://mysql.com/products/cluster/&quot;&gt;MySQL (NDB) Cluster&lt;/a&gt;, and commercial rip-out-and-plug-in replacements for MySQL such as &lt;a href=&quot;http://www.clustrix.com/&quot;&gt;Clustrix&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Am I missing any? I probably am. You can see and talk to many of these companies at this week&amp;rsquo;s &lt;a href=&quot;http://www.percona.com/live/mysql-conference-2012/&quot;&gt;MySQL conference&lt;/a&gt;, by the way.&lt;/p&gt;</description>
	<pubDate>Mon, 09 Apr 2012 00:00:00 +0000</pubDate>
</item>

</channel>
</rss>
