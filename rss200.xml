<?xml version="1.0"?>
<rss version="2.0">

<channel>
	<title>Community Redis Aggregator</title>
	<link>http://www.planetredis.org/</link>
	<language>en</language>
	<description>Community Redis Aggregator - http://www.planetredis.org/</description>

<item>
	<title>Redis Labs: CVE-2015-4335/DSA-3279 – Redis Lua Sandbox Escape</title>
	<guid isPermaLink="false">http://redislabs.com/?p=10142</guid>
	<link>https://redislabs.com/blog/cve-2015-4335-dsa-3279-redis-lua-sandbox-escape</link>
	<description>&lt;p&gt;&lt;img alt=&quot;&quot; class=&quot;aligncenter size-full wp-image-10143&quot; height=&quot;200&quot; src=&quot;https://redislabs.com/wp-content/uploads/2015/06/20150608_pmr_art.png&quot; title=&quot;Pimp My Redis - The Lua Sandbox Escape&quot; width=&quot;635&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;What happened?&lt;/b&gt; Last week, in a post titled &lt;a href=&quot;http://benmmurphy.github.io/blog/2015/06/04/redis-eval-lua-sandbox-escape/&quot; target=&quot;_blank&quot;&gt;&quot;Redis EVAL Lua Sandbox Escape&quot;&lt;/a&gt;, security researcher Ben Murphy unveiled the details of a known Lua exploit that can be used for breaking out of Redis' sandbox. Mr. Murphy was also kind enough to provide a patch that addresses said issue, which in turn was immediately included in &lt;a href=&quot;https://groups.google.com/forum/#!msg/redis-db/4Y6OqK8gEyk/Dg-5cejl-eUJ&quot; target=&quot;_blank&quot;&gt;new releases for Redis v2.8.21 and v3.0.2&lt;/a&gt; from antirez, a.k.a. Salvatore Sanfilippo (as reported in last week's &lt;a href=&quot;https://redislabs.com/redis-watch-archive/46#lua-sandbox-escape&quot; target=&quot;_blank&quot;&gt;Redis Watch&lt;/a&gt; newsletter).&lt;/p&gt;

&lt;p&gt;&lt;b&gt;How serious is this?&lt;/b&gt; Any security vulnerability is serious business, but while a malicious person can indeed exploit vulnerability to wreck havoc, it does require a non-trivial amount of skill do so. For example, refer to this sample code that's designed to attack Lua 5.1 on a Windows 32-bit platform: &lt;a href=&quot;https://gist.github.com/corsix/6575486&quot; target=&quot;_blank&quot;&gt;https://gist.github.com/corsix/6575486&lt;/a&gt;. Furthermore, the exploit itself is but one step in a process that would involve attacking additional components of the compromised system. The bottom line is that while you should definitely take steps to protect your Redis servers against this exploit, keep in mind that the risk isn't considered high.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;How can you protect yourself?&lt;/b&gt; There are several steps you can take to protect your Redis from would-be-hackers' pwnage. Here they are in order of importance:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;&lt;b&gt;Use password authentication&lt;/b&gt; – If you're not using it, anyone can connect to it and &lt;a href=&quot;https://twitter.com/itamarhaber/status/591017658604216321&quot; target=&quot;_blank&quot;&gt;pimp your Redis&lt;/a&gt;.&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;Use password authentication&lt;/b&gt; – Seriously, use a password.&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;Moar security&lt;/b&gt; – If you're using Redis Cloud or Redis Labs Enterprise Cluster (RLEC), consider using the access control lists provided from your dashboard to restrict access by security group and/or IP addresses or subnets. If you're running your own Redis servers, a firewall with some hard-and-sane policies can get you the same effect.&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;SSL authentication and encryption&lt;/b&gt; – Protect yourself further by switching to a Redis Cloud or RLEC &lt;a href=&quot;https://redislabs.com/kb/read-more-ssl&quot; target=&quot;_blank&quot;&gt;SSL-enabled database&lt;/a&gt;. Do-it-yourselfers can &lt;a href=&quot;https://redislabs.com/blog/using-stunnel-to-secure-redis&quot; target=&quot;_blank&quot;&gt;use stunnel&lt;/a&gt; to secure their servers in the same manner.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;b&gt;Upgrade your Redis&lt;/b&gt;. The new versions of open source Redis include the fix so you should be all good once you upgrade (if you're running your own). RLEC users only need to install the newest software update in order apply the patch to their databases. We've also backported the patch to our cloud service, so new databases are immune to the vulnerability and we're applying it online to existing instances. If you have any concerns or questions please contact our help desk at: &lt;a href=&quot;mailto:support@redislabs.com&quot; target=&quot;_blank&quot;&gt;support@redislabs.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Quick n' dirty n' awesome tip:&lt;/b&gt; here's a little nugget from &lt;a href=&quot;https://news.ycombinator.com/item?id=9658937&quot; target=&quot;_blank&quot;&gt;geocar&lt;/a&gt; – rename QUIT to POST to deal with that SSRF threat &amp;lt;- lovely!&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Redis Trivia:&lt;/b&gt; AFAIK, this is the first Redis vulnerability that was registered in the MITRE Common Vulnerabilities and Exposures Directory (&lt;a href=&quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2015-4335&quot; target=&quot;_blank&quot;&gt;CVE-2015-4335&lt;/a&gt; which is no longer up for some reason, did it escape too?) and at the Debian Security Advisory (&lt;a href=&quot;https://www.debian.org/security/2015/dsa-3279&quot; target=&quot;_blank&quot;&gt;DSA-3279&lt;/a&gt;) so double yay! Also note how, by sheer conspicuous coincidence, the DSA's ID for the vulnerability is given by the following formulae:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://redislabs.com/wp-content/uploads/2015/06/lua_sandbox_dsa.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I'm still working on cracking the logic behind the CVE ID thoughâ€¦ Any Suggestions? Questions? Feedback? &lt;a href=&quot;mailto:itamar@redislabs.com&quot; target=&quot;_blank&quot;&gt;Email&lt;/a&gt; or &lt;a href=&quot;https://twitter.com/itamarhaber&quot; target=&quot;_blank&quot;&gt;tweet&lt;/a&gt; at me – I'm highly available :)&lt;/p&gt;</description>
	<pubDate>Tue, 09 Jun 2015 10:52:45 +0000</pubDate>
</item>
<item>
	<title>Redis Labs: Introduction to Redis Labs Enterprise Cluster (RLEC)</title>
	<guid isPermaLink="false">http://redislabs.com/?p=9657</guid>
	<link>https://redislabs.com/blog/introduction-to-redis-labs-enterprise-cluster</link>
	<description>&lt;p&gt;&lt;img alt=&quot;Redis Lab Enterprise Cluster (RLEC): Introduction&quot; class=&quot;aligncenter size-full wp-image-10121&quot; height=&quot;200&quot; src=&quot;https://redislabs.com/wp-content/uploads/2015/06/2015-06-07.png&quot; title=&quot;Redis Lab Enterprise Cluster (RLEC): Introduction&quot; width=&quot;635&quot; /&gt;&lt;/p&gt;

&lt;h3 dir=&quot;ltr&quot;&gt;Who is RLEC good for?&lt;/h3&gt;

&lt;p dir=&quot;ltr&quot;&gt;Redis is a great technology, it is VERY fast, it is very simple to use and at the same time very sophisticated and powerful. For these reasons Redis has seen tremendous adoption and has become a mainstay of modern day application stacks.&lt;/p&gt;

&lt;p dir=&quot;ltr&quot;&gt;When you need to use Redis in more complex scenarios and treat it as a full blown database (with high-availability and scalability in order to be able to grow to big datasets) or as a highly available and scalable cache, you can use enhancements like Redis Sentinel and Redis Cluster. Once again great technologies, but not trivial to get initially deployed or manage afterwards.&lt;/p&gt;

&lt;p dir=&quot;ltr&quot;&gt;Some customers would like to use Redis as a full blown product vs. a set of great technologies. What differentiates a technology from a product in my view is the ability to seamlessly use Redis for any need, from the most simple usage scenario, like a volatile cache on a single server, to the most complex scenario, like a huge highly-available sharded database that can span multiple servers and survive server crashes.&lt;/p&gt;

&lt;p dir=&quot;ltr&quot;&gt;Another aspect that differentiates using a technology vs. a product in my mind is getting a full-blown management UI to manage and monitor the Redis databases, as well as end-to-end support for your Redis deployment and how to use it.&lt;/p&gt;

&lt;p dir=&quot;ltr&quot;&gt;Lastly, in spite of the growing demand by companies to use open-source technologies, many companies would like the technology to be commercially supported and backed by a company that is committed to the future of the product and can provide support for any need. If you ever consult the big analyst firms, they always recommend against using the open-source technology as-is without getting commercial support for it.&lt;/p&gt;

&lt;p dir=&quot;ltr&quot;&gt;For the above reasons we have created Redis Labs Enterprise Cluster (RLEC): to completely relieve you of the burden related to deploying, provisioning, configuring, managing and monitoring Redis. RLEC makes it really simple to ensure high-availability across racks or data-centers, create and manage a clustered Redis database and provide automatic cluster rebalancing. With RLEC, you also gain commercial support for your Redis usage, provided by Redis Labs.&lt;/p&gt;

&lt;h3 dir=&quot;ltr&quot;&gt;What does RLEC give you beyond open-source Redis?&lt;/h3&gt;

&lt;p dir=&quot;ltr&quot;&gt;RLEC is practically a multi-tenant system that acts as a container for running multiple databases on the same cluster infrastructure. Each database can be from the following configuration:&lt;/p&gt;

&lt;ul&gt;
	&lt;li dir=&quot;ltr&quot;&gt;Simple Redis database (i.e. a single master shard)&lt;/li&gt;
	&lt;li dir=&quot;ltr&quot;&gt;Highly-available Redis database (i.e. master / slave shards or master / multi-slave shards)&lt;/li&gt;
	&lt;li dir=&quot;ltr&quot;&gt;Redis clustered database (i.e. multiple master shards)&lt;/li&gt;
	&lt;li dir=&quot;ltr&quot;&gt;Highly-available Redis clustered database (i.e. multiple master shards each with its own slave shard, or multiple master / multi-slave shards)&lt;/li&gt;
&lt;/ul&gt;

&lt;p dir=&quot;ltr&quot;&gt;As a result, after installing RLEC (which is a simple task by itself that I will cover in an upcoming blog post) you can easily provision and manage multiple Redis databases, spanning multiple servers, while maintaining high-availability, and be able to scale up or down the databases as your needs change. You can do all of this through RLEC’s management UI, through the &lt;code&gt;rladmin&lt;/code&gt; Command Line Interface (CLI), or through a REST API. In addition you can view various statistics to understand what is going on in each of your databases, each of the cluster nodes, or in the cluster as a whole. Apart from viewing these, you can sign up to receive alerts in the UI itself, and via email, as well as be notified by the cluster, on different events or thresholds, so you can be aware and take actions.&lt;/p&gt;

&lt;p dir=&quot;ltr&quot;&gt;There are many more advanced capabilities and logic that we have implemented as part of RLEC that are too long or sophisticated to cover in this post, but you can read more about these in the &lt;a href=&quot;https://redislabs.com/redis-enterprise-documentation/overview&quot; target=&quot;_blank&quot;&gt;RLEC documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h3 dir=&quot;ltr&quot;&gt;Why should you trust RLEC?&lt;/h3&gt;

&lt;p dir=&quot;ltr&quot;&gt;It is important to note that although RLEC is inherently running the open source Redis, we, at Redis Labs, have implemented a lot of intellectual property within RLEC to better manage Redis as a highly-available and scalable database.&lt;/p&gt;

&lt;p dir=&quot;ltr&quot;&gt;This intellectual property has been built for several years while running the same technology as a cloud service named &lt;a href=&quot;https://redislabs.com/redis-cloud&quot; target=&quot;_blank&quot;&gt; Redis Cloud&lt;/a&gt; that is available on all major cloud platforms (AWS, Azure, GCE, SoftLayer, etc.) and has helped thousands of our customers survive multiple server failures, as well as entire datacenter failures.&lt;/p&gt;

&lt;p dir=&quot;ltr&quot;&gt;What we’ve done now in RLEC is taken this technology we’ve built during the years and repackaged it as downloadable and installable software that you can install anywhere you’d like, whether on your own cloud instances, or your servers running within your private data-center. We’ve gotten demand for this ability from customers who were not able to use our cloud service for various reasons, such as customers who cannot run on a public cloud platform, customers who want to run within their virtual private cloud and cannot connect from it to an external service, or customers who need to comply to different information security standards.&lt;/p&gt;

&lt;h3&gt;So how do you get started with RLEC?&lt;/h3&gt;

&lt;p&gt;Like I indicated above, getting started with RLEC is very simple, and I will cover more detailed information on that in a future post, but you can probably figure it out yourself using our documentation and get started by:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;Download RLEC from &lt;a href=&quot;https://redislabs.com/redis-enterprise-downloads&quot; target=&quot;_blank&quot;&gt;https://redislabs.com/redis-enterprise-downloads&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;Follow the instructions at &lt;a href=&quot;https://redislabs.com/redis-enterprise-documentation/installing-and-upgrading/accessing-and-installing-the-setup-package&quot; target=&quot;_blank&quot;&gt;https://redislabs.com/redis-enterprise-documentation/installing-and-upgrading/accessing-and-installing-the-setup-package&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;div&gt;If you have any feedback or questions please don’t hesitate to reach out to me at: &lt;a href=&quot;mailto:itai.raz@redislabs.com&quot;&gt;itai.raz@redislabs.com&lt;/a&gt;.&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://redislabs.com/redis-enterprise-downloads&quot; target=&quot;_blank&quot;&gt;&lt;img alt=&quot;&quot; class=&quot;aligncenter size-full wp-image-9953&quot; height=&quot;200&quot; src=&quot;https://redislabs.com/wp-content/uploads/2015/06/download-now.png&quot; title=&quot;RLEC - Download Now!&quot; width=&quot;635&quot; /&gt;&lt;/a&gt;&lt;/p&gt;</description>
	<pubDate>Mon, 08 Jun 2015 13:00:14 +0000</pubDate>
</item>
<item>
	<title>Redis Labs: Rising NoSQL Star: Aerospike, Cassandra, Couchbase or Redis?</title>
	<guid isPermaLink="false">http://redislabs.com/?p=9946</guid>
	<link>https://redislabs.com/blog/nosql-performance-aerospike-cassandra-datastax-couchbase-redis</link>
	<description>&lt;p&gt;&lt;img alt=&quot;&quot; class=&quot;aligncenter size-full wp-image-9953&quot; height=&quot;200&quot; src=&quot;https://redislabs.com/wp-content/uploads/2015/06/2015-06-04.png&quot; title=&quot;Rising NoSQL Star: Redis outshines the competition, leaves 2nd places for Aerospike, Cassandra and Couchbase&quot; width=&quot;635&quot; /&gt; &lt;/p&gt;

&lt;blockquote&gt;
	&lt;p dir=&quot;ltr&quot;&gt;&quot;I always knew I was a star, and now the rest of the world seems to agree with me.&quot; — Freddie Mercury&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A new NoSQL benchmark was just released by &lt;a href=&quot;http://www.avalonconsult.com/&quot; target=&quot;_blank&quot;&gt;Avalon Consulting, LLC&lt;/a&gt;, and I couldn’t be happier to brag that Redis out-performed its competitors by a landslide. With more than double the throughput and half the latency of other NoSQL databases, our Redis Labs Enterprise Cluster dominated in a real-world application scenario. The Avalon benchmark report is &lt;a href=&quot;http://redislabs.com/cbc-2015-15-nosql-benchmark&quot; target=&quot;_blank&quot;&gt;freely available here&lt;/a&gt; and the results speak for themselves.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://redislabs.com/cbc-2015-15-nosql-benchmark&quot; target=&quot;_blank&quot;&gt;&lt;img class=&quot;aligncenter size-full&quot; src=&quot;https://redislabs.com/wp-content/images/lp/2015-q2/graph.png&quot; title=&quot;Rising NoSQL Star: Redis outshines the competition, leaves 2nd places for Aerospike, Cassandra and Couchbase&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;But before we get into all the fun background on this particular test, let’s acknowledge a few things about benchmarks. There's no way around it - performing meaningful comparisons between NoSQL solutions is a hard task. That is because of benchmarking's &quot;original sin&quot; — results from any benchmark are truly relevant only to the specific application that was used for the test (see &lt;a href=&quot;https://gist.github.com/itamarhaber/2dad94e3bdd2980bce73&quot; target=&quot;_blank&quot;&gt;Haber's Benchmarking Theorem&lt;/a&gt;). This fact is compounded by the diverse capabilities of all the different NoSQL databases. Typical benchmark models tend to generalize a specific use case, and in the process they distance themselves from the underlying data management system and fail to leverage its strengths.&lt;/p&gt;

&lt;p dir=&quot;ltr&quot;&gt;This by itself is hardly news and the past is riddled with attempts at &lt;a href=&quot;https://redislabs.com/blog/nosql-bar-datastax-aerospike-couchbase-redis-google-cloud-platform&quot; target=&quot;_blank&quot;&gt;comparing apples to oranges&lt;/a&gt;. I gave an entire presentation at RedisConf 20Fifteen on this subject ( &lt;a href=&quot;https://www.youtube.com/watch?v=aotCPUtahDU&quot; target=&quot;_blank&quot;&gt;&quot;Benchmarking Redis By Itself and Versus Other NoSQL Databases&quot;&lt;/a&gt;). If you've watched it then you already know I believe that the only way to compare apples with oranges is through an applicative benchmark, in which the test application is optimized independently for each DBMS. That RedisConf talk, it turns out, was only the warm-up act for a session by Lahav Savir, CEO of &lt;a href=&quot;http://www.emind.co/&quot; target=&quot;_blank&quot;&gt;Emind&lt;/a&gt;, who presented a real life benchmark using that exact approach (&lt;a href=&quot;https://www.youtube.com/watch?v=aotCPUtahDU&amp;amp;t=20m02s&quot; target=&quot;_blank&quot;&gt;&quot;Real-Time Vote Platform Benchmark&quot;&lt;/a&gt;).&lt;/p&gt;

&lt;p dir=&quot;ltr&quot;&gt;Emind's use case is a great example of how Redis is put to use in tackling some of the hairier challenges that real-time analytics presents in the context of Big Data and the IoT. The story behind Emind's benchmark brings together all my passions: data, people, technology and the cloud. It is a brilliant experiment designed to identify the best-performing NoSQL database for a real-time voting platform. The voting platform supports large events such as televised talent shows (think &quot;American Idol&quot; or &quot;Rising Star&quot;), where the audience is actively involved and directs the course of the show by voting. The volume and velocity of votes that must be tallied as they come in is staggering, so the platform’s performance must have superstar qualities (much like the shows' participants) to support that kind of traffic.&lt;/p&gt;

&lt;p dir=&quot;ltr&quot;&gt;Emind's team identified several NoSQL technologies that could potential power their platform: Aerospike, Cassandra, Couchbase and Redis. While all candidates seemed promising, Emind needed to be sure that it chose the database that would best meet its requirements. To do that, Emind's engineers built a &lt;a href=&quot;https://github.com/emind-systems/real_time_vote_benchmark&quot; target=&quot;_blank&quot;&gt;mock application&lt;/a&gt; (&quot;mockapp&quot;) in Go that simulated the voting process and tailored it to use each of the different candidates.&lt;/p&gt;

&lt;p dir=&quot;ltr&quot;&gt;Emind then solicited the services of Avalon Consulting, LLC to ensure the benchmark was executed optimally and impartially. Avalon reviewed and optimized the mockapp's source code, approached each database vendor (Aerospike, Datastax, Couchbase and Redis Labs) for guidance and certification of its respective solution's deployment, executed the benchmark and compiled a &lt;a href=&quot;http://redislabs.com/cbc-2015-15-nosql-benchmark&quot; target=&quot;_blank&quot;&gt;comprehensive report&lt;/a&gt; with the results. Check out the full write-up to find out more about how my favorite performer is a rocking superstar. Questions? Feedback? &lt;a href=&quot;mailto:itamar@redislabs.com&quot;&gt;Email&lt;/a&gt; or &lt;a href=&quot;https://twitter.com/itamarhaber&quot; target=&quot;_blank&quot;&gt;tweet&lt;/a&gt; me – I'm highly available :)&lt;/p&gt;</description>
	<pubDate>Thu, 04 Jun 2015 13:00:16 +0000</pubDate>
</item>
<item>
	<title>Pivotal: 6 Free Technical Classes From Pivotal Education</title>
	<guid isPermaLink="false">http://blog.pivotal.io/?p=32930</guid>
	<link>http://blog.pivotal.io/big-data-pivotal/features/6-free-technical-classes-from-pivotal-education</link>
	<description>&lt;p&gt;&lt;img alt=&quot;sfeatured-pivotal-education-free&quot; class=&quot;alignleft wp-image-33049 size-full&quot; height=&quot;220&quot; src=&quot;http://blog.pivotal.io/wp-content/uploads/2015/06/sfeatured-pivotal-education-free.png&quot; width=&quot;440&quot; /&gt;Pivotal Education makes it easy to fully realize the capabilities of our technologies by offering a series of free introductory training courses. Ideal for developers, system architects, and data practitioners, these online courses engage students through a hands-on, sandbox lab environment and provide a technical foundation for more advanced topics taught in other Pivotal courses. The offerings are also uniquely designed to enable technologists at any point of engagement with Pivotal—whether during evaluation or after deployment—to become more well-versed, efficient, and effective in their efforts.&lt;/p&gt;
&lt;p&gt;Our free, introductory training classes span across a wide array of Pivotal technologies, including:  &lt;a href=&quot;http://pivotal.io/big-data/pivotal-hd&quot; target=&quot;_blank&quot;&gt;Pivotal HD&lt;/a&gt;, &lt;a href=&quot;http://pivotal.io/platform-as-a-service/pivotal-cloud-foundry&quot; target=&quot;_blank&quot;&gt;Pivotal Cloud Foundry&lt;/a&gt;, &lt;a href=&quot;http://pivotal.io/big-data/pivotal-greenplum-database&quot; target=&quot;_blank&quot;&gt;Pivotal Greenplum Database&lt;/a&gt;, Pivotal &lt;a href=&quot;http://pivotal.io/big-data/pivotal-hawq&quot; target=&quot;_blank&quot;&gt;HAWQ&lt;/a&gt;, &lt;a href=&quot;http://redis.io/&quot; target=&quot;_blank&quot;&gt;Redis&lt;/a&gt;, and &lt;a href=&quot;http://pivotal.io/big-data/pivotal-gemfire&quot; target=&quot;_blank&quot;&gt;Pivotal GemFire&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://pivotal.io/training#introduction-to-greenplum-database&quot; target=&quot;_blank&quot;&gt;Intro to Pivotal Greenplum Database&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt;
This course is designed to provide an introduction to students new to Pivotal Greenplum Database (GPDB). As a one day course, students will gain a high level overview of features found in GPDB. This introduction will serve as a foundation for further courses that are offered for administrators, DBAs and end users of GPDB.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://pivotal.io/training#introduction-to-gemfire&quot; target=&quot;_blank&quot;&gt;Intro to GemFire&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt;
This course is designed to provide an introduction to students new to Pivotal GemFire. As a 1 day course, students will gain a high level overview of features found in GemFire. This introduction will serve as a foundation for further courses that are offered for administrator, developers and architects using GemFire. Some basic high level lab exercises are also offered to enhance the learning experience.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://pivotal.io/training#introduction-to-pivotal-hd&quot; target=&quot;_blank&quot;&gt;Intro to Pivotal HD&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt;
This course is designed to provide an introduction to students new to the advanced analytic capabilities of Pivotal HD, our &lt;a href=&quot;https://hadoop.apache.org/&quot; target=&quot;_blank&quot;&gt;Apache Hadoop®&lt;/a&gt; distribution. As a one day course, students will gain a high level overview of features found in the Pivotal HD. This introduction will serve as a foundation for further courses that are offered for administrators and architects using Pivotal HD.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://pivotal.io/training#ntroduction-to-pivotal-hd-and-hawq&quot; target=&quot;_blank&quot;&gt;Intro to Pivotal HAWQ&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt;
This course is designed to provide an introduction to students new to running SQL on Hadoop®. In this one day course, students will gain a high level overview of features found in HAWQ. This introduction will serve as a foundation for further courses that are offered for administrators, architects and end users of HAWQ.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://pivotal.io/training#enterprise-paas&quot; target=&quot;_blank&quot;&gt;Intro to Pivotal Cloud Foundry&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt;
This course provides students with the concepts and hands-on experience needed to work with and deploy applications on Pivotal Cloud Foundry. Students will gain familiarity in general Cloud Foundry concepts (applications, buildpacks, manifests, organizations, spaces, users, roles, domains, routes, services), how to push applications to Cloud Foundry in various languages, services, user provided services, manifests, YAML, environment variables, autoconfiguration, logging and loggregator.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://pivotal.io/training#introduction-to-redis&quot; target=&quot;_blank&quot;&gt;Intro to Redis (2 day course)&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt;
This hands-on course provides a broad overview of the Redis NoOSQL datastore, starting with an explanation of product functionality and typical use cases. Students will learn how to install and configure Redis, and examine some of the common commands for manipulating data. They will have the opportunity to create a simple Redis Java application, and review tips and techniques for improving performance. The course explains the use of persistence and replication in Redis, and also includes coverage of some administrative topics such as security and monitoring.&lt;/p&gt;
&lt;p&gt;Whether you are evaluating Pivotal’s products for your company, have already deployed these technologies, or are a student training looking to expand your skillset, Pivotal Education courses present an opportunity to become well-versed in these topics with minimal time commitment and no financial cost.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://pivotal.io/training&quot; target=&quot;_blank&quot;&gt;&lt;b&gt;Learn more about Pivotal’s Education and Training Courses&lt;/b&gt;&lt;/a&gt;&lt;/p&gt;</description>
	<pubDate>Mon, 01 Jun 2015 22:21:02 +0000</pubDate>
</item>
<item>
	<title>Redis Labs: A Guy Walks Into a NoSQL Bar [Director's Cut]</title>
	<guid isPermaLink="false">http://redislabs.com/?p=9738</guid>
	<link>https://redislabs.com/blog/nosql-bar-datastax-aerospike-couchbase-redis-google-cloud-platform</link>
	<description>&lt;p&gt;&lt;img alt=&quot;A Guy Walks Into a NoSQL Bar&quot; class=&quot;aligncenter size-full wp-image-9745&quot; height=&quot;200&quot; src=&quot;https://redislabs.com/wp-content/uploads/2015/05/20150529-art.png&quot; title=&quot;A Guy Walks Into a NoSQL Bar&quot; width=&quot;635&quot; /&gt;&lt;/p&gt;

&lt;h2 dir=&quot;ltr&quot;&gt;or DataStax, Aerospike, Couchbase and Redis Labs: Serving 1M Writes/Sec From Google Compute Engine&lt;/h2&gt;

&lt;p dir=&quot;ltr&quot;&gt;A couple of months ago I published a post over at the Google Cloud Platform blog called &lt;a href=&quot;http://googlecloudplatform.blogspot.com/2015/04/a-guy-walks-into-a-NoSQL-bar-and-asks-how-many-servers-to-get-1Mil-ops-a-second.html&quot; target=&quot;_blank&quot;&gt;&quot;A guy walks into a NoSQL bar and asks: how many servers to get 1 million ops. a second?&quot; &lt;/a&gt; That particular post had gone through several editing cycles and I'm very happy with the final result. When I wrote that particular post, I abbreviated my cheeky story, but given &lt;a href=&quot;http://blog.couchbase.com/couchbase-server-hits-1m-writes-with-3b-items-with-50-nodes-on-google-cloud&quot; target=&quot;_blank&quot;&gt;Couchbase's recent announcement&lt;/a&gt; about getting to 1M writes/sec with &lt;strong&gt;only 50 servers (!)&lt;/strong&gt;, I thought it would be relevant to share the full glory of my analogy here.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; the joke is still not very funny – suggestions for improvement are welcome!&lt;/p&gt;

&lt;h3 dir=&quot;ltr&quot;&gt;Preface&lt;/h3&gt;

&lt;p dir=&quot;ltr&quot;&gt;Comparisons are as hard to do right as they are easy to do wrong and when the subjects of comparison are different the problem is only compounded. Therefore, when practicing comparative “advertising” I often feel that the results begotten should be interpreted in spirit rather be presented as numbers to quibble about. With that in mind, let me share a brief related story…&lt;/p&gt;

&lt;h3 dir=&quot;ltr&quot;&gt;Prologue&lt;/h3&gt;

&lt;p dir=&quot;ltr&quot;&gt;A guy walks into a NoSQL bar looking quite upset. &quot;Can I have a glass of red wine and a glass of white wine?&quot; he asks the bartender. &quot;Right away sir,&quot; says the bartender, &quot;but may I ask why the long face?&quot; &quot;I was in that other NoSQL bar further down the road, the Spiky Aerrow,&quot; answers the patron, &quot;and asked for that exact same order. The server there is known for his speed (some people swear he can do several things at once) as well as the amazing way he prepares drinks, so I was kind of expecting a show.&quot;&lt;/p&gt;

&lt;p&gt;The bartender, whose bar has been open for quite a few years, nods sympathetically and says, &quot;I'm guessing he didn't quite dazzle you?&quot; &quot;On the contrary,&quot; replies the customer, it was an awesome show. To prepare my glass of red, that spectacular master of liquids uncorked no less than 30 bottles of wine in a fraction of a second! He then continued by whirlwinding between all these bottles and pouring a little wine from each into my glass. A lot of wine splashed around and spilled, given the speeds involved, but it was a heck of a show. And then, he did the same thing for the white… but with 50 bottles!&quot;&lt;/p&gt;

&lt;h3 dir=&quot;ltr&quot;&gt;Introduction&lt;/h3&gt;

&lt;p dir=&quot;ltr&quot;&gt;Some time ago, we at Redis Labs set out to answer the question: &quot;How many Google Compute Engine nodes do you need to serve 1 million write operations per second?&quot; The question, besides sounding like a variation of the lightbulb joke, is quite open-ended in terms of defining the actual nature of the write operation based on &lt;a href=&quot;http://googlecloudplatform.blogspot.com/2014/12/aerospike-hits-one-million-writes-Per-Second-with-just-50-Nodes-on-Google-Compute-Engine.html&quot; target=&quot;_blank&quot;&gt; previous&lt;/a&gt; &lt;a href=&quot;http://googlecloudplatform.blogspot.com/2014/03/cassandra-hits-one-million-writes-per-second-on-google-compute-engine.html&quot; target=&quot;_blank&quot;&gt;work&lt;/a&gt; on the subject. Given our experience with cloud platforms in general, and particularly with Google's, we were fairly confident that we'd need only a few so we set out to find out.&lt;/p&gt;

&lt;h3 dir=&quot;ltr&quot;&gt;Benchmark Setup&lt;/h3&gt;

&lt;p dir=&quot;ltr&quot;&gt;To run the benchmark, we chose the biggest Google Compute Engine nodes currently available in us-central – the n1-highcpu-16 type – and we ended up using just two such servers to run our Redis Labs Enterprise Cluster software (freely downloadable from &lt;a href=&quot;https://redislabs.com/redis-enterprise-downloads&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;). We used another pair of servers – this time n1-highcpu-8 – to generate the load with &lt;a href=&quot;https://github.com/RedisLabs/memtier_benchmark&quot; target=&quot;_blank&quot;&gt;memtier_benchmark&lt;/a&gt; using the following command line arguments:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;memtier_benchmark -s 10.0.0.1 -p 6379 --ratio=1:1 --test-time=120 -d 100 -t 2 -c 50 --pipeline=50 --ratio=1:0&lt;/code&gt;&lt;/p&gt;

&lt;h3 dir=&quot;ltr&quot;&gt;Benchmark Results&lt;/h3&gt;

&lt;p dir=&quot;ltr&quot;&gt;We ran the benchmark three times, first serving only reds reads, then serving only &lt;s&gt;whites&lt;/s&gt; writes, and finally both in equal mix. Here are the results from those tests:&lt;/p&gt;

&lt;ol&gt;
	&lt;li dir=&quot;ltr&quot;&gt;For read-only operations, our Redis database provided throughput of &lt;strong&gt;1.29M read operations per second&lt;/strong&gt; at an average latency of 0.15 millisecond per operation.&lt;/li&gt;
	&lt;li dir=&quot;ltr&quot;&gt;With the write-only load, the cluster's measured throughput was at &lt;strong&gt;1.14M operations per second&lt;/strong&gt; at 0.36sec average latency per request.&lt;/li&gt;
	&lt;li dir=&quot;ltr&quot;&gt;An equal mix of read and write operations gave a throughput of &lt;strong&gt;1.16M operations per second&lt;/strong&gt; at an average latency of 0.17msec per operation.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img alt=&quot;Redis Labs Enterprise Cluster on Google Cloud Platform - Over 1M ops/sec&quot; class=&quot;aligncenter size-full wp-image-9741&quot; height=&quot;492&quot; src=&quot;https://redislabs.com/wp-content/uploads/2015/05/20150529-nosql-bar-graph.png&quot; title=&quot;Redis Labs Enterprise Cluster on Google Cloud Platform - Over 1M ops/sec&quot; width=&quot;789&quot; /&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h3 dir=&quot;ltr&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p dir=&quot;ltr&quot;&gt;Redis needed only two Google Cloud Engine servers in the cluster to cross the 1M op/sec threshold.&lt;/p&gt;

&lt;h3 dir=&quot;ltr&quot;&gt;Epilogue&lt;/h3&gt;

&lt;p dir=&quot;ltr&quot;&gt;Before the man can finish his story, the bartender had finished preparing the order (using just one bottle of white and one bottle of red, mind you). He placed the drinks alongside the bill in front of the customer, and said &quot;sounds like an amazing show indeed, but I still don't understand why you're so unhappy.&quot; &quot;I'll tell you why,&quot; answered the man, &quot;guess who had to pay for all 80 bottles of spilled wine?&quot;&lt;/p&gt;

&lt;p dir=&quot;ltr&quot;&gt;The barman smiled and said, &quot;You should consider yourself lucky then, sir. I heard of a place further uptown called Dat Stacks where for every glass of white wine they charge for 300 bottles.&quot;&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;1M Ops/Sec on Google Cloud Platform: Your bill, sir&quot; class=&quot;aligncenter size-full wp-image-9743&quot; height=&quot;1670&quot; src=&quot;https://redislabs.com/wp-content/uploads/2015/05/20150529-nosql-bar-info.png&quot; title=&quot;1M Ops/Sec on Google Cloud Platform: Your bill, sir&quot; width=&quot;1566&quot; /&gt;&lt;/p&gt;

&lt;p dir=&quot;ltr&quot;&gt;Ok, so maybe this joke isn't really that funny, but neither is paying too much for less-than-top-notch performance :) With Redis, reaching and exceeding the 1 million operations per second mark doesn't require a truckload of cloud servers – just one or two will do nicely.&lt;/p&gt;</description>
	<pubDate>Fri, 29 May 2015 16:45:56 +0000</pubDate>
</item>
<item>
	<title>Redis Labs: Redis Labs at MongoDB World!</title>
	<guid isPermaLink="false">http://redislabs.com/?p=9611</guid>
	<link>https://redislabs.com/blog/redis-labs-at-mongodb-world</link>
	<description>&lt;p dir=&quot;ltr&quot;&gt;&lt;img alt=&quot;Redis Cloud - 30MB RAM, 30 Connections for FREE&quot; class=&quot;alignnone size-full wp-image-9594&quot; height=&quot;200&quot; src=&quot;https://redislabs.com/wp-content/uploads/2015/05/mongo-db.png&quot; title=&quot;Redis Labs at MongoDB World!&quot; width=&quot;635&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Redis Labs team will be in New York next month to sponsor MongoDB World, MongoDB’s largest end user event, which brings together thousands of Mongo developers, enthusiasts, and influencers from around the world.&lt;/p&gt;

&lt;p&gt;The event will take place on June 1 – 2 at the Manhattan Sheraton in New York City and welcomes developers and ecosystem partners like Redis Labs. MongoDB and Redis offer complimentary uses cases for both startup and enterprise developers. As we’ve &lt;a href=&quot;https://redislabs.com/blog/database-usage-survey-of-aws-reinvent-2014-developers&quot;&gt;shared in the past&lt;/a&gt;, we find that many developers who use Redis choose to use a wide variety of databases together, including MongoDB and MySQL, to solve a range of challenges for performance-driven use cases.&lt;/p&gt;

&lt;p&gt;As usual, the Redis Labs team has a great lineup of activities planned for the event, including:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;&lt;strong&gt;Awesome Redis Swag.&lt;/strong&gt; Stop by our booth to pick up our popular Redis Geek shirts, onesies and laptop swag!&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;Featured Speaking Session.&lt;/strong&gt; Our very own Itamar Haber, Chief Developer Advocate, will be giving a speaking session entitled “Redis &amp;amp; MongoDB: Stop Big Data Indigestion Before It Starts” on June 2nd at 10:00am. Join us for a highly actionable talk and lively Q&amp;amp;A!&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;1-on-1 Meetings.&lt;/strong&gt; Book a meeting with any of our team members to discuss how to tap into Redis' performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Questions? Feel free to &lt;a href=&quot;mailto:cameron@redislabs.com&quot; target=&quot;_blank&quot;&gt;email me&lt;/a&gt; or reach out on &lt;a href=&quot;https://twitter.com/cameronperon&quot; target=&quot;_blank&quot;&gt;Twitter!&lt;/a&gt;&lt;/p&gt;</description>
	<pubDate>Thu, 21 May 2015 09:27:07 +0000</pubDate>
</item>
<item>
	<title>Pivotal: Pivotal Data Roadshow: A View From the Road</title>
	<guid isPermaLink="false">http://blog.pivotal.io/?p=11883</guid>
	<link>http://blog.pivotal.io/big-data-pivotal/products/pivotal-data-roadshow-a-view-from-the-road</link>
	<description>&lt;p&gt;&lt;img alt=&quot;featured-BigDataRoadshow&quot; class=&quot;alignleft size-full wp-image-11895&quot; height=&quot;200&quot; src=&quot;http://blog.pivotal.io/wp-content/uploads/2015/04/featured-BigDataRoadshow.png&quot; width=&quot;200&quot; /&gt;People in the market are hungry for hands-on experience and training with big data technologies.  We are seeing this through the dramatic response to the &lt;a href=&quot;https://pivotal.io/big-data/data-roadshow&quot; target=&quot;_blank&quot; title=&quot;Pivotal Data Roadshow&quot;&gt;Pivotal Data Roadshow&lt;/a&gt;, which is touring cities across North America this month.  We have been packing the house with nearly 100 participants in each city.  We think this is a good gauge of just how much Apache HadoopⓇ and &lt;a href=&quot;http://pivotal.io/big-data/hadoop/sql-on-hadoop&quot; target=&quot;_blank&quot; title=&quot;SQL on Hadoop&quot;&gt;SQL on Hadoop&lt;/a&gt; have been embraced within the enterprise to handle big data storage and analytics workloads.&lt;/p&gt;
&lt;p&gt;Big Data technology continues to advance quickly. Just 18 months ago, Pivotal released the world’s first closed-loop platform for analytics and data-driven applications. While big data and &lt;a href=&quot;http://hadoop.apache.org&quot; target=&quot;_blank&quot; title=&quot;Apache HadoopⓇ&quot;&gt;Apache HadoopⓇ&lt;/a&gt; were moving from niche concerns to tech buzzword status, Pivotal was merging HadoopⓇ with &lt;a href=&quot;http://pivotal.io/big-data/pivotal-hawq&quot; target=&quot;_blank&quot; title=&quot;HAWQ&quot;&gt;HAWQ&lt;/a&gt;, a robust SQL-on-Hadoop engine, and with real time and &lt;a href=&quot;http://pivotal.io/big-data/pivotal-gemfire&quot; target=&quot;_blank&quot; title=&quot;in-memory data processin&quot;&gt;in-memory data processing&lt;/a&gt; to round out our vision of how companies should operationalize big data over a &lt;a href=&quot;http://pivotal.io/big-data/businessdatalake&quot; target=&quot;_blank&quot; title=&quot;Business Data Lake&quot;&gt;Business Data Lake&lt;/a&gt;. It is exciting for us to learn from the roadshows that enterprises are now eagerly embracing SQL-on-Hadoop and in-memory processing technologies to solve business problems, substantiating Pivotal’s strategy and investments over the past five years.&lt;/p&gt;
&lt;p&gt;Attendees of the Pivotal Data Roadshows have included executives, administrators and operations staff, developers, data scientists, and even college students. This variety demonstrates the enormous impact these technologies have on entire industries today and how important they are to each of their futures. Many people in our audiences have already been leveraging our Data Lake solutions as &lt;a href=&quot;http://pivotal.io/big-data/pivotal-big-data-suite&quot; target=&quot;_blank&quot; title=&quot;Pivotal Big Data Suite&quot;&gt;Pivotal Big Data Suite&lt;/a&gt; customers. These attendees have been impressed by &lt;a href=&quot;http://blog.pivotal.io/big-data-pivotal/news-2/pivotal-big-data-suite-open-agile-cloud-ready&quot; target=&quot;_blank&quot; title=&quot;new customer-centric entitlements&quot;&gt;new customer-centric entitlements&lt;/a&gt; within the Pivotal Big Data Suite, which includes an instance of &lt;a href=&quot;http://pivotal.io/platform-as-a-service/pivotal-cloud-foundry&quot; target=&quot;_blank&quot; title=&quot;Pivotal Cloud Foundry&quot;&gt;Pivotal Cloud Foundry&lt;/a&gt;, as well as powerful new additions for real time data ingestion and analysis via tools such as &lt;a href=&quot;http://blog.pivotal.io/pivotal/products/spring-xd-for-real-time-analytics&quot; target=&quot;_blank&quot; title=&quot;Spring XD&quot;&gt;Spring XD &lt;/a&gt;and &lt;a href=&quot;http://blog.pivotal.io/pivotal/case-studies-2/using-redis-at-pinterest-for-billions-of-relationships&quot; target=&quot;_blank&quot; title=&quot;Redis&quot;&gt;Redis&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;image01&quot; class=&quot;alignnone size-full wp-image-11897&quot; height=&quot;466&quot; src=&quot;http://blog.pivotal.io/wp-content/uploads/2015/04/image011.png&quot; width=&quot;615&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Demonstrating another key component of &lt;a href=&quot;http://pivotal.io/new-approach-to-big-data&quot; target=&quot;_blank&quot; title=&quot;our 2015 strategy&quot;&gt;our 2015 strategy&lt;/a&gt;, the entire technical portion of the Pivotal Data Roadshow really highlights our robust cloud capabilities. All our attendee sessions are hosted in an enterprise public cloud infrastructure, spun up quickly and seamlessly in minutes.  This enables us to rapidly provision 50 IoT analytics labs across our entire product set. Attendees quickly initiate analytics workflows that ingest Twitter streams and execute real time analytics across our in memory database, Gemfire. Most impressively, this all gets developed in under 5 minutes, using only two Spring XD commands, replacing what used to be hours of complex and specialized programming.  Spring XD really has emerged as the star of the show, thanks to the streamlined, easy-to-understand interface it provides for our products, third party technologies, and its ability to manage either private, public, big, or fast data jobs.&lt;/p&gt;
&lt;p&gt;Attendees have stayed hours past the scheduled end time, enjoying building out new analytics pipelines with our expert engineering teams. They also left with their lab environments equipped to continue their experiences on their own cloud, learning into the future.&lt;/p&gt;
&lt;p&gt;If you’re interested in how big data and advanced analytics are changing companies, and would like to get your hands directly on the world’s best technology for leveraging them, be sure to check out one of the upcoming Pivotal Data Roadshows.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;image00&quot; class=&quot;alignnone size-full wp-image-11898&quot; height=&quot;273&quot; src=&quot;http://blog.pivotal.io/wp-content/uploads/2015/04/image001.png&quot; width=&quot;787&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Upcoming Dates for the Pivotal Data Roadshow&lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Toronto, April 14&lt;/li&gt;
&lt;li&gt;New York City, April 16&lt;/li&gt;
&lt;li&gt;Washington, DC, April 21&lt;/li&gt;
&lt;li&gt;Atlanta, April 23&lt;/li&gt;
&lt;li&gt;Denver, April 28&lt;/li&gt;
&lt;li&gt;Dallas/Fort Worth, April 30&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;b&gt;Further information:&lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://pivotal.io/big-data/data-roadshow&quot; target=&quot;_blank&quot; title=&quot;Register for the Pivotal Data Roadshow&quot;&gt;Register for the Pivotal Data Roadshow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://pivotal.io/big-data/pivotal-big-data-suite&quot; target=&quot;_blank&quot; title=&quot;Learn more about the Pivotal Big Data Suite&quot;&gt;Learn more about the Pivotal Big Data Suite&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learn more about &lt;a href=&quot;http://blog.pivotal.io/pivotal/products/spring-xd-for-real-time-analytics&quot; target=&quot;_blank&quot; title=&quot;Spring XD&quot;&gt;Spring XD&lt;/a&gt; and &lt;a href=&quot;http://blog.pivotal.io/pivotal/case-studies-2/using-redis-at-pinterest-for-billions-of-relationships&quot; target=&quot;_blank&quot; title=&quot;Redis&quot;&gt;Redis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;i&gt;Editor’s Note: Apache, Apache Hadoop, Hadoop, and the yellow elephant logo are either registered trademarks or trademarks of the Apache Software Foundation in the United States and/or other countries.&lt;/i&gt;&lt;/p&gt;</description>
	<pubDate>Mon, 06 Apr 2015 15:09:30 +0000</pubDate>
</item>
<item>
	<title>Pivotal: Mobile Video Big Data Architecture with Spring XD/Hadoop/HAWQ/Redis: Measuring Live Usage</title>
	<guid isPermaLink="false">http://blog.pivotal.io/?p=10844</guid>
	<link>http://blog.pivotal.io/pivotal/case-studies-2/mobile-video-big-data-architecture-with-spring-xdhadoophawqredis-measuring-live-usage</link>
	<description>&lt;p&gt;&lt;a href=&quot;http://blog.pivotal.io/wp-content/uploads/2014/10/featured-football-spectators-phone.jpg&quot;&gt;&lt;img alt=&quot;featured-football-spectators-phone&quot; class=&quot;alignleft size-full wp-image-10870&quot; height=&quot;200&quot; src=&quot;http://blog.pivotal.io/wp-content/uploads/2014/10/featured-football-spectators-phone.jpg&quot; width=&quot;200&quot; /&gt;&lt;/a&gt;Earlier this year, &lt;a href=&quot;http://www.emarketer.com/Article/Tablets-Challenge-PCs-Leading-Digital-Video-Channel-US/1010807&quot; target=&quot;_blank&quot; title=&quot;mobile video usage overtook video usage on personal computers&quot;&gt;mobile video usage overtook video usage on personal computers&lt;/a&gt; and research continues to show how important mobile video is to consumers. With these trends, mobile carriers and media companies are trying to learn more about usage to improve user experience and advertising revenues, especially with big advertising money makers like sports.&lt;/p&gt;
&lt;p&gt;Earlier this year, a major sports network and mobile carrier approached Pivotal to create a system for measuring cellular data and live video usage through mobile applications. The approach used &lt;a href=&quot;http://projects.spring.io/spring-xd/&quot; target=&quot;_blank&quot; title=&quot;Spring XD&quot;&gt;Spring XD&lt;/a&gt;, &lt;a href=&quot;http://www.pivotal.io/big-data/pivotal-hd&quot; target=&quot;_blank&quot; title=&quot;Pivotal HD&quot;&gt;Pivotal HD&lt;/a&gt;, &lt;a href=&quot;http://www.pivotal.io/big-data/redis&quot; target=&quot;_blank&quot; title=&quot;Redis&quot;&gt;Redis&lt;/a&gt;, and &lt;a href=&quot;http://projects.spring.io/spring-boot/&quot; target=&quot;_blank&quot; title=&quot;Spring Boot&quot;&gt;Spring Boot&lt;/a&gt; to quickly build a big data platform and set of analytical dashboards. The resulting analytics application is helping business executives from both companies understand how live video is used and what peak data usage looks like. The whole project only took a few resources and a few months.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Mobile/Media Big Data Architecture Overview&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;At a high level, data moves through a factory as depicted in the diagram below. Raw data is captured from mobile phones in JSON format by a Spring XD cluster where several processes are performed. The data is then stored in an HDFS cluster where Spring XD batch jobs use SQL via the HAWQ interface to HDFS and store the calculated reports in Redis. Spring Boot is then used with Angular.js and D3.js to show analytics to end users.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://blog.pivotal.io/wp-content/uploads/2014/10/ProLeague+Mobile-SolutionsArchitecture.png&quot;&gt;&lt;img alt=&quot;ProLeague+Mobile-SolutionsArchitecture&quot; class=&quot;alignnone size-full wp-image-10871&quot; height=&quot;239&quot; src=&quot;http://blog.pivotal.io/wp-content/uploads/2014/10/ProLeague+Mobile-SolutionsArchitecture.png&quot; width=&quot;615&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Capturing Raw Data from Mobile Devices in JSON&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;To produce the raw data, a mobile device is instrumented to capture the number of bytes used while viewing a video. The mobile application sends a “heartbeat” message that is formatted in JSON and contains the number of bytes consumed since the last report. Generally, the mobile app sends one of these reports every minute. Since there can be a large number of viewers during a live game, the system receives millions of heartbeat messages per hour. When designing the system, we decided to keep all of the heartbeat data over time, as with the concept of keeping all data inside a &lt;a href=&quot;http://blog.pivotal.io/pivotal/news-2/from-data-silos-to-data-lakes-realizing-the-accessible-dream&quot; target=&quot;_blank&quot; title=&quot;data lake&quot;&gt;data lake&lt;/a&gt;. This way, teams could use the data to produce additional analysis in the future. In addition, the customers had a quick timeline. They needed the system in place prior to the start of the next sports season.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Ingesting and Processing Data with Spring XD and HDFS&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;To handle the data ingestion, the Pivotal team utilized &lt;a href=&quot;http://projects.spring.io/spring-xd/&quot; target=&quot;_blank&quot; title=&quot;Spring XD&quot;&gt;Spring XD&lt;/a&gt; to receive and transform the raw data. The processed data was stored in an &lt;a href=&quot;http://hadoop.apache.org&quot; target=&quot;_blank&quot; title=&quot;Apache Hadoop&quot;&gt;Apache Hadoop®&lt;/a&gt; File System (HDFS) cluster as part of &lt;a href=&quot;http://www.pivotal.io/big-data/pivotal-hd&quot; target=&quot;_blank&quot; title=&quot;Pivotal HD&quot;&gt;Pivotal HD&lt;/a&gt;. Spring XD and HDFS provided a flexible, scalable solution to handle the massive amount of incoming JSON heartbeat data—as much as a half terabyte per season of sports.&lt;/p&gt;
&lt;p&gt;After the data was stored in HDFS, we used Pivotal HAWQ and SQL to generate the necessary reports from the data stored in HDFS. Spring XD’s batch job capability was leveraged to execute the reports after a game was done. The batch jobs produced JSON reports and stored them in &lt;a href=&quot;http://blog.pivotal.io/tag/redis&quot; target=&quot;_blank&quot; title=&quot;Redis&quot;&gt;Redis&lt;/a&gt;, providing immediate access to a web-based dashboard. Spring XD and Pivotal HD provided clustering and failover support to ensure business continuity in the event of a server failure.&lt;/p&gt;
&lt;p&gt;Spring XD turned out to be a perfect solution because it was designed with a distributed and extensible pipeline for ingesting data, processing it, performing real-time analytics on the received data, and then exporting it. Using Spring XD, we quickly implemented a system to receive the raw heartbeat data from the mobile applications. Spring XD provides several “sources” and “sinks” out of the box, and we were able to modify the existing HTTP source and HDFS sink to receive and store the raw heartbeat data. We also created a custom “processor” module that enabled us to filter and validate the raw data, ensuring that it was formatted as expected and had valid data in the expected JSON fields. Spring XD also provided a mechanism to “tap” data from one stream to another. Using a tap, it was simple for us to configure a stream to count the number of heartbeat packets received. This allowed us to ensure the system worked as expected—we could see the amount of live data going through the system in real-time. Since Spring XD supports a distributed runtime deployment, it also ensured that inbound data would continue to process in the event of any server failure.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Creating Analytical Reports with Spring XD, HAWQ, Redis, and Spring Boot&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;With HDFS storing the raw data, we used the &lt;a href=&quot;http://blog.pivotal.io/pivotal/products/json-on-hadoop-example-for-extending-hawq-data-formats-using-pivotal-extension-framework-pxf&quot; target=&quot;_blank&quot; title=&quot;Pivotal Xtension Framework&quot;&gt;Pivotal Xtension Framework&lt;/a&gt; (PXF) and &lt;a href=&quot;http://blog.pivotal.io/tag/hawq&quot; target=&quot;_blank&quot; title=&quot;HAWQ&quot;&gt;HAWQ&lt;/a&gt; to perform SQL queries directly against the JSON data. PXF maps the JSON to columns in a database, enabling HAWQ to execute SQL queries directly against the JSON data in HDFS. Using SQL, the product manager could then define the report queries.&lt;/p&gt;
&lt;p&gt;For the next step, we turned to Spring XD’s batch capabilities. Custom “jobs” were configured with the appropriate SQL queries to execute. The jobs are scheduled to execute early in the morning, after a game has occurred, and the report results are stored in Redis. Redis stores all the reports so that users can view any of them without needing to spend the time computing them on demand. Finally, we implemented a simple REST API, using &lt;a href=&quot;http://blog.pivotal.io/tag/spring-boot&quot; target=&quot;_blank&quot; title=&quot;Spring Boot&quot;&gt;Spring Boot&lt;/a&gt;, to serve the JSON reports to the user interface.&lt;/p&gt;
&lt;p&gt;A web-based dashboard was also built and allows users to easily view the weekly reports as they are available. Here, Spring Boot was used with JavaScript libraries like &lt;a href=&quot;https://angularjs.org/&quot; target=&quot;_blank&quot; title=&quot;Angular.js&quot;&gt;Angular.js&lt;/a&gt; and &lt;a href=&quot;http://d3js.org/&quot; target=&quot;_blank&quot; title=&quot;D3.js&quot;&gt;D3.js&lt;/a&gt;. Spring Boot and Spring Security were used to quickly secure the dashboard by adding a login page and requiring HTTPS for access.&lt;/p&gt;
&lt;p&gt;Together, Spring XD, Pivotal HD and HAWQ, Redis, and Spring Boot were used to quickly create a big data solution for massive ingesting, analyzing, and reporting on cellular data use for live, game-day videos. Now, the client can identify trends and optimize media revenue.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Learn More:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spring XD: &lt;a href=&quot;http://projects.spring.io/spring-xd/&quot; target=&quot;_blank&quot; title=&quot;Project and Documentation&quot;&gt;Project and Documentation&lt;/a&gt; | &lt;a href=&quot;http://blog.pivotal.io/tag/spring-xd&quot; target=&quot;_blank&quot; title=&quot;Blog Articles&quot;&gt;Blog Articles&lt;/a&gt; | &lt;a href=&quot;http://blog.pivotal.io/pivotal/products/spring-xd-for-real-time-analytics&quot; target=&quot;_blank&quot; title=&quot;Using XD for Real Time Twitter&quot;&gt;Using XD for Real Time Twitter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pivotal HD (SQL on HDFS via HAWQ): &lt;a href=&quot;http://www.pivotal.io/big-data/pivotal-hd&quot; target=&quot;_blank&quot; title=&quot;Product&quot;&gt;Product&lt;/a&gt; | &lt;a href=&quot;http://pivotalhd.docs.pivotal.io/doc/2010/index.html&quot; target=&quot;_blank&quot; title=&quot;Documentation&quot;&gt;Documentation&lt;/a&gt; | &lt;a href=&quot;http://blog.pivotal.io/tag/pivotal-hd&quot; target=&quot;_blank&quot; title=&quot;Blog Articles&quot;&gt;Blog Articles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Redis: &lt;a href=&quot;http://blog.pivotal.io/tag/redis/redis.io&quot; target=&quot;_blank&quot; title=&quot;Website&quot;&gt;Website&lt;/a&gt; | &lt;a href=&quot;http://blog.pivotal.io/tag/redis&quot; target=&quot;_blank&quot; title=&quot;Blog Articles&quot;&gt;Blog Articles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Spring Boot: &lt;a href=&quot;http://projects.spring.io/spring-boot/&quot; target=&quot;_blank&quot; title=&quot;Project and Documentation&quot;&gt;Project and Documentation&lt;/a&gt; | &lt;a href=&quot;http://blog.pivotal.io/tag/spring-boot&quot; target=&quot;_blank&quot; title=&quot;Blog Articles&quot;&gt;Blog Articles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pivotal Open Source: &lt;a href=&quot;http://www.pivotal.io/oss&quot; target=&quot;_blank&quot; title=&quot;Products and Projects&quot;&gt;Products and Projects&lt;/a&gt; | &lt;a href=&quot;http://blog.pivotal.io/tag/open-source&quot; target=&quot;_blank&quot; title=&quot;Blog Articles&quot;&gt;Blog Articles&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Editor’s Note&lt;/strong&gt;: Apache, Apache Hadoop, Hadoop, and the yellow elephant logo are either registered trademarks or trademarks of the Apache Software Foundation in the United States and/or other countries.&lt;/em&gt;&lt;/p&gt;</description>
	<pubDate>Mon, 13 Oct 2014 09:43:58 +0000</pubDate>
</item>
<item>
	<title>Pivotal: Apache Way at Pivotal</title>
	<guid isPermaLink="false">http://blog.pivotal.io/?p=10829</guid>
	<link>http://blog.pivotal.io/pivotal/news-2/apache-way-at-pivotal</link>
	<description>&lt;p&gt;&lt;img alt=&quot;featured-feathers&quot; class=&quot;alignleft size-full wp-image-10839&quot; height=&quot;200&quot; src=&quot;http://blog.pivotal.io/wp-content/uploads/2014/10/featured-feathers.png&quot; width=&quot;200&quot; /&gt;Ever since I joined Pivotal, one of my favorite topics of cocktail conversation has become explaining to folks unfamiliar with the company just how much of a &lt;a href=&quot;http://blog.pivotal.io/pivotal/products/open-source-is-pivotal&quot; target=&quot;_blank&quot; title=&quot;Open Source is Pivotal&quot;&gt;quintessential open source shop we are&lt;/a&gt;. Just rattling off the names of the projects and communities we stand behind feels like listing ‘who’s who’ of open source. Being a betting man as I am, I typically introduce Pivotal by making a $1 bet that whoever I’m talking to is likely to have used software we contribute to at some point in their career. Given that this list includes &lt;a href=&quot;http://www.pivotal.io/platform-as-a-service/pivotal-cf&quot; target=&quot;_blank&quot; title=&quot;Cloud Foundry&quot;&gt;Cloud Foundry&lt;/a&gt;, &lt;a href=&quot;http://projects.spring.io/spring-framework/&quot; target=&quot;_blank&quot; title=&quot;Spring Framework&quot;&gt;Spring Framework&lt;/a&gt;, &lt;a href=&quot;http://groovy.codehaus.org/&quot; target=&quot;_blank&quot; title=&quot;Groovy&quot;&gt;Groovy&lt;/a&gt;, &lt;a href=&quot;https://grails.org/&quot; target=&quot;_blank&quot; title=&quot;Grails&quot;&gt;Grails&lt;/a&gt;, &lt;a href=&quot;http://www.pivotal.io/big-data/redis&quot; target=&quot;_blank&quot; title=&quot;Redis&quot;&gt;Redis&lt;/a&gt;, &lt;a href=&quot;http://www.pivotal.io/products/pivotal-rabbitmq&quot; target=&quot;_blank&quot; title=&quot;RabbitMQ&quot;&gt;RabbitMQ&lt;/a&gt;, &lt;a href=&quot;http://tomcat.apache.org/&quot; target=&quot;_blank&quot; title=&quot;Tomcat&quot;&gt;Tomcat&lt;/a&gt; and most recently &lt;a href=&quot;http://hadoop.apache.org&quot; target=&quot;_blank&quot; title=&quot;Apache Hadoop&quot;&gt;Apache Hadoop®&lt;/a&gt; the only folks I ever lose to are the poor souls still stuck on Windows 95 or mainframes. Paying $1 to them feels less like losing a bet, and more like a random act of kindness.&lt;/p&gt;
&lt;p&gt;As a side note, this incredible diversity of Pivotal’s open source portfolio helps our business model a great deal. After all, the days when one could have a sizable stand-alone business built around a single open source project are as distant as dial-up internet access. There’s a lot to be said on what a winning open-source strategy looks like (I may dare defining one in one of my future blog posts). Whatever it is, if you are an enterprise infrastructure vendor in 2014, you can’t survive without having one. The market has clearly spoken in favor of enterprise infrastructure products built around open source projects. With this came a realization that the only sustainable way of doing so was to build long-term relationships with open source communities. This, of course, meant that business now found themselves thinking deep and hard about open source community governance.&lt;/p&gt;
&lt;p&gt;It is perhaps ironic, that the single most important detail of how open source communities operate doesn’t get as much air time as licensing or patenting. Somehow there’s a perception that open source communities don’t even need much of governance. They are expected to self-organize into utopian worker collectives that could serve as a living illustration to “from each according to his ability, to each according to his need” principle. Nothing can be further from the truth. Each big, successful open source community is unique and typically very particular about the governance model. Open source communities, and by extension projects that they work on, live and die by how effective their governance model is. A great community can alway fix technology, a poor community is bound to ruin the best technology that is entrusted to it.&lt;/p&gt;
&lt;p&gt;Now, remember my earlier point about the sheer size of Pivotal’s open source portfolio? Community governance is where it starts to matter yet again. Because you see, regardless of how much a runaway success Spring is (and standing at 8+ million users nobody would argue it isn’t) the governance model that made it so successful is unique to it. It is true that Spring uses Apache License, but the similarities with the Apache Software Foundation (ASF) governance model ends there.&lt;/p&gt;
&lt;p&gt;ASF governance model, also known as “The Apache Way”, is a very unique, extremely well thought out approach to building successful open source communities. Communities that are capable of innovating on a diverse set of projects all living under the Foundation’s umbrella. “The Apache Way” has been covered in great details over the years, its elevator pitch fits in just three words: “Community over code”. No really, that is it—everything else is an implementation detail.&lt;/p&gt;
&lt;p&gt;It must be said, that even before I joined Pivotal, the company has had a relationship with the ASF. We have been a constant sponsor of the Foundation and with guys like Mark Thomas and Bill Rowe working for Spring side of Pivotal “The Apache Way” was practiced quite well when it comes to Apache Tomcat and Apache HTTPD. At the same time, the scale at which Datafabrics side of Pivotal has embraced ASF projects around Apache Hadoop® ecosystem required a different level of focus on ASF.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.pivotal.io/careers&quot;&gt;&lt;img alt=&quot;cta-hiring&quot; class=&quot;alignnone size-full wp-image-9320&quot; height=&quot;135&quot; src=&quot;http://blog.pivotal.io/wp-content/uploads/2014/06/cta-hiring.png&quot; width=&quot;600&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So that was part of my mission when joining Pivotal: become a resident ASF guy for data technologies organization and &lt;em&gt;make sure that in a year’s time Pivotal becomes a company that grows the pool of Apache Committers, not a company that just fights over the existing pool.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This project started with putting a framework in place that enables Pivotal engineers to collaborate on ASF software with minimum distraction (this required a close collaboration with our outstanding legal team). Our next step was to review our product roadmap and based on customer’s feedback start focusing on a project that they needed. This is what our &lt;a href=&quot;http://blog.pivotal.io/pivotal/features/strengthening-hadoop-in-the-enterprise-with-apache-ambari&quot; target=&quot;_blank&quot; title=&quot;Strengthening Apache Hadoop in the Enterprise with Apache Ambari&quot;&gt;previous announcement around Apache® Ambari&lt;/a&gt; was all about.&lt;/p&gt;
&lt;p&gt;In the short couple of month we have not only contributed a &lt;a href=&quot;https://issues.apache.org/jira/browse/AMBARI-7292&quot; target=&quot;_blank&quot; title=&quot;AMBARI-7292&quot;&gt;wide&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/AMBARI-7551&quot; target=&quot;_blank&quot; title=&quot;AMBARI-7551&quot;&gt;variety&lt;/a&gt; of improvements to it, but also helped the project get more stable by working on&lt;a href=&quot;https://builds.apache.org/view/All/job/Ambari-trunk-Commit/&quot; target=&quot;_blank&quot; title=&quot;Ambari-trunk-Commit&quot;&gt; build &lt;/a&gt;and &lt;a href=&quot;https://issues.apache.org/jira/browse/AMBARI-7209&quot; target=&quot;_blank&quot; title=&quot;AMBARI-7209&quot;&gt;CI automation&lt;/a&gt; and suggesting &lt;a href=&quot;http://markmail.org/message/rs5j5j365txapalm&quot; target=&quot;_blank&quot; title=&quot;Proposal&quot;&gt;process improvements&lt;/a&gt; to innovate even quicker while not losing the sense of project stability. Pivotal’s product has benefited from this relationship, but so did Apache® Ambari: it has become better and more featureful, but most importantly its community has grown stronger and more diverse. In fact, recognizing all the hard work that one of Pivotal’s engineers Jun Aoki has put into the project, the Apache® Ambari PMC has &lt;a href=&quot;http://markmail.org/message/dwfwtercyj3s6moq&quot; target=&quot;_blank&quot; title=&quot;Jun invited to Ambari&quot;&gt;voted&lt;/a&gt; Jun in as a committer.&lt;/p&gt;
&lt;p&gt;This may be a small step for Jun (and, to be sure, the first one in many other ASF projects that he is going to join), but it actually signifies a giant leap for Pivotal and the kind of role it is assuming in ASF from now on. We’re already delivering on that fundamental vision of making sure that our engineers get fully empowered to lead most impactful open source projects within the Foundation. If you are an engineer dreaming to add the coveted Committer/PMC of Apache X to your resume, join us. We can’t guarantee that you will get that line (after all, ASF recognizes your personal merit and your personal merit alone) but we can guarantee that we will move heaven and earth to empower you to do so.&lt;/p&gt;
&lt;p&gt;After all a stronger Apache Software Foundation (ASF) will benefit the entire industry. It is, you see, that proverbial tide that lifts all boats big and small.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Editor’s Note&lt;/strong&gt;: Apache, Apache Hadoop, Hadoop, and the yellow elephant logo are either registered trademarks or trademarks of the Apache Software Foundation in the United States and/or other countries.&lt;/em&gt;&lt;/p&gt;</description>
	<pubDate>Wed, 08 Oct 2014 16:27:06 +0000</pubDate>
</item>
<item>
	<title>Pivotal: An Introduction to the Pivotal CF Mobile Suite API Gateway</title>
	<guid isPermaLink="false">http://blog.pivotal.io/?p=10510</guid>
	<link>http://blog.pivotal.io/pivotal-cloud-foundry/news-2/an-introduction-to-the-pivotal-cf-mobile-suite-api-gateway</link>
	<description>&lt;p&gt;&lt;img alt=&quot;featured-PCF-mobile&quot; class=&quot;alignleft size-full wp-image-10327&quot; height=&quot;200&quot; src=&quot;http://blog.pivotal.io/wp-content/uploads/2014/08/featured-PCF-mobile.png&quot; width=&quot;200&quot; /&gt;The API Gateway is one of the products available on the Pivotal CF Mobile Services. It allows developers to easily create edge gateways proxy calls between devices and your organization internal services.&lt;/p&gt;
&lt;p&gt;There are several reasons why one would consider having such services in place:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Centralized authentication&lt;/strong&gt;: Your internal services may have different authentication mechanisms, if any, and you may want to expose a API key style of authentication to external consumers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reduce chattiness&lt;/strong&gt;: Allows mobile devices to reduce the number of calls to your backend by aggregating invocations to multiple services at once.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data type conversion&lt;/strong&gt;: Internal services may have different formats such as REST or SOAP, an edge gateway can serve as a central transformation point.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data filtering&lt;/strong&gt;: You can filter data either based on role based access or perhaps just for the sake of saving bandwidth to certain clients.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rate limiting&lt;/strong&gt;: Control the number of requests clients can make to your services. This can avoid possible DDOS on your services as well as control access to certain billable services such as third-party services you may use.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One of the benefits of using an edge gateway is that you can perform all of the above without having to modify internal services—the gateway will sit between your services and the clients. The picture below demonstrates this:&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;Screen Shot 2014-09-24 at 11.05.48 AM&quot; class=&quot;aligncenter size-full wp-image-10604&quot; height=&quot;1094&quot; src=&quot;http://blog.pivotal.io/wp-content/uploads/2014/09/Screen-Shot-2014-09-24-at-11.05.48-AM.png&quot; width=&quot;1244&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Pivotal’s API Gateway uses javascript as its core language, but not your usual javascript engine. It’s powered by Nashorn, the latest JDK script engine. This approach allows developers to enjoy the benefits of using a simple script language such as javascript, while still being able to seamless integrate with Spring beans. You can invoke any Java Spring bean from your javascript code.&lt;/p&gt;
&lt;p&gt;The picture below depicts the internals of a gateway application. Note that you can add your own java beans as well as javascript code.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;Screen Shot 2014-09-24 at 11.05.58 AM&quot; class=&quot;aligncenter size-full wp-image-10605&quot; height=&quot;876&quot; src=&quot;http://blog.pivotal.io/wp-content/uploads/2014/09/Screen-Shot-2014-09-24-at-11.05.58-AM.png&quot; width=&quot;1092&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;Implementing Rate Limit&lt;/h2&gt;
&lt;p&gt;In this post, we will show how easy is to implement a rate limiting edge gateway service using Pivotal’s API Gateway and Redis.&lt;/p&gt;
&lt;p&gt;Rate limiting is an important feature that you should consider when exposing your APIs to external users. It prevents possible DDOS on your internal services by discarding requests from customers which exceeded the maximum stipulated quota. It also allows you to have control of your billing expenses on your IaaS provider by controlling traffic and usage.&lt;/p&gt;
&lt;p&gt;When it comes to rate limiting there isn’t a standard on how to deal with it, but several service providers such as &lt;a href=&quot;https://dev.twitter.com/rest/public/rate-limiting&quot;&gt;twitter&lt;/a&gt; and &lt;a href=&quot;https://developer.github.com/v3/rate_limit/&quot; title=&quot;GitHub&quot;&gt;github&lt;/a&gt; utilizes a combination of HTTP response headers and an HTTP Response code 429 (too many requests) when the client quota has exceeded.&lt;/p&gt;
&lt;p&gt;There are many ways of implementing rate limiting, but in a nutshell we need to be able to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Identify the caller (via an API key or IP address)&lt;/li&gt;
&lt;li&gt;Log the number of requests&lt;/li&gt;
&lt;li&gt;Set an expiration time for the request window (1 minute, 1 hour, 1 day)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Rate control using Redis&lt;/h2&gt;
&lt;p&gt;Redis is a great choice for implementing rate limiting. It offers the capability of setting keys timeouts as well as atomic increase operations on a numeric key.&lt;/p&gt;
&lt;p&gt;The pattern is actually described on the &lt;a href=&quot;http://redis.io/commands/INCR&quot;&gt;redis incr command&lt;/a&gt;. We will use a slight variation of that, which allows us to control the window interval as opposed to only using a one second window.&lt;/p&gt;
&lt;p&gt;Redis stores one value with a JSON configuration object for each API key in this way:&lt;/p&gt;
&lt;p&gt;{&lt;br /&gt;
“key”:”mykey”,&lt;br /&gt;
“value”:10,&lt;br /&gt;
“current”:null,&lt;br /&gt;
“reset”:null,&lt;br /&gt;
“window”:60&lt;br /&gt;
}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;key&lt;/strong&gt;: The API Key associated with this config&lt;br /&gt;
&lt;strong&gt;value&lt;/strong&gt;: The request limit&lt;br /&gt;
&lt;strong&gt;current&lt;/strong&gt;: The current request count for a given period (not used on configuration)&lt;br /&gt;
&lt;strong&gt;reset&lt;/strong&gt;: The Unix epoch in milliseconds of the next reset of the count&lt;br /&gt;
&lt;strong&gt;window&lt;/strong&gt;: Controls how long the counter will be held in memory&lt;/p&gt;
&lt;p&gt;The above configuration creates a rate limit control that allows 10 requests per minute. Using Spring-Data-Redis, all we need is one Bean to get the information for each API request.&lt;/p&gt;
&lt;p&gt;The code below shows the java spring bean we created to keep track of requests per api:&lt;/p&gt;
&lt;pre class=&quot;brush: plain; title: ; notranslate&quot;&gt;package io.pivotal.api.rate;

import java.io.IOException;
import java.util.List;
import java.util.concurrent.TimeUnit;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.dao.DataAccessException;
import org.springframework.data.redis.core.RedisOperations;
import org.springframework.data.redis.core.SessionCallback;
import org.springframework.data.redis.core.StringRedisTemplate;
import org.springframework.stereotype.Component;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;

@Component(&quot;rateLimiter&quot;)
public class RateLimiterImpl {

	@Autowired
	private StringRedisTemplate template;

	@Autowired
	private ObjectMapper mapper;

	/**
	 *  Reads the current value of a given key on a window of time. It contains both the value and next reset window
	 *  This entry is stored in redis as a key on the format : 'apikey:timewindow'
	 *
	 * */

	public Rate getCurrent(final Rate rate){
		Long now = System.currentTimeMillis();
		Long time = (Long)(now/(1000*rate.getWindow()));
		final String key = rate.getKey()+&quot;:&quot;+time;
		List&amp;lt;Object&amp;gt; results = template.execute(new SessionCallback&amp;lt;List&amp;lt;Object&amp;gt;&amp;gt;() {

			@Override
			public  List&amp;lt;Object&amp;gt; execute(RedisOperations ops) throws DataAccessException {
				ops.multi();
				ops.boundValueOps(key).increment(1L);
				ops.expire(key, rate.getWindow(), TimeUnit.SECONDS);
				return ops.exec();
			}
		});
		Long current = (Long)results.get(0);
		rate.setCurrent(current.intValue());
		rate.setReset(time*(rate.getWindow()*1000));
		return rate;
	}
	/**
	 * Fetches the configured rate for a given API. This entry only contains the key, the window and value
	 * it does not store the value of the current invocation or the next reset window.
	 * This entry is represented in redis just by the key 'apikey'
	 *
	 * */

	public Rate getRate(String apikey){
		Rate rate = null;
		try {
			rate = mapper.readValue(template.opsForValue().get(apikey), Rate.class);
			rate = getCurrent(rate);
		} catch (IOException e) {
			e.printStackTrace();
		}
		return rate;
	}

	public void setRate(Rate rate){
		try {
			template.opsForValue().set(rate.getKey(), mapper.writeValueAsString(rate));
		} catch (JsonProcessingException e) {
			e.printStackTrace();
		}
	}
}
&lt;/pre&gt;
&lt;p&gt;For sake of simplicity, we use the same JSON object to represent the configuration for a given API—for example, the number of requests and the window duration—as well as the current value for that instance in time.&lt;/p&gt;
&lt;p&gt;We use two separate keys to store this. For the configuration, it’s a key that has not ttl and its stored using the apikey as the key. For the current value, we store using apikey:window as the key and we set a TTL on it.&lt;/p&gt;
&lt;p&gt;For each call we create an unique key composed of the API key plus the current time divided by the rate window. This will give an unique key for each window of invocation. On each call we increment the key and increase its time to live by the rate window.&lt;/p&gt;
&lt;p&gt;When the time window expires we generate a new key with a zero counter, the old key would still live for as long as the length of the window, but since it would never be refreshed, Redis will eventually expire it.&lt;/p&gt;
&lt;h2&gt;Javascript meets Java&lt;/h2&gt;
&lt;p&gt;As mentioned before, one of the greatests features of API Gateway is the ability to mesh Javascript code with Java. Writing your application in Javascript will allow you to leverage the simplicity of the language, and its great support for JSON. This reduces quite significantly the amount of boilerplate that would be needed to write APIs, while allowing you to access your java backend components as first class citizens.&lt;/p&gt;
&lt;p&gt;API Gateway has a native spring global function that allows you to invoke any spring bean that is part of your application. Using this feature we can easily access our RateLimiter service bean.&lt;/p&gt;
&lt;p&gt;To enable the rate limit for any request to this API endpoint we’ve created a decorator around the original Router that checks for the limits of each request:&lt;/p&gt;
&lt;pre class=&quot;brush: plain; title: ; notranslate&quot;&gt;var RatedRouter = function(router) {
	this.router = router;
}

function getApiKey(req){
	var header = req.headers[&quot;Authorization&quot;];
	return header.split(&quot; &quot;)[1];
}

function checkLimit(req,res){
	var api = getApiKey(req);
	var rate = spring.getBean(&quot;rateLimiter&quot;).getRate(api);
	var remaining = rate.value-rate.current;
	console.log(JSON.stringify(rate));
	res.setHeader(&quot;X-RateLimit-Limit&quot;,rate.value);
	res.setHeader(&quot;X-RateLimit-Remaining&quot;, remaining);
	res.setHeader(&quot;X-RateLimit-Reset&quot;,rate.reset);
	return (remaining &amp;gt; 0);
}

RatedRouter.prototype = {

	get: function(path, handle){
		this.router.get(path, function(req, res){
			if(!checkLimit(req,res)){
				res.setStatus(429);
			}else{
				handle(req,res);
			}
		});
	},
	post: function(path, handle){
		this.router.post(path, function(req, res){
			if(!checkLimit(req,res)){
				res.setStatus(429);
			}else{
				handle(req,res);
			}
		});
	},

	put: function(path, handle){
		this.router.put(path, function(req, res){
			if(!checkLimit(req,res)){
				res.setStatus(429);
			}else{
				handle(req,res);
			}
		});
	},

	delete : function(path, handle){
		this.router.delete(path, function(req, res){
			if(!checkLimit(req,res)){
				res.setStatus(429);
			}else{
				handle(req,res);
			}
		});
	}

}

module.exports = RatedRouter
&lt;/pre&gt;
&lt;p&gt;The checkLimit function will retrieve the limits for this request, and add the proper X-Rate-Limit headers to each response.&lt;/p&gt;
&lt;p&gt;Each method on this router will check if the limit was reached and return a 429 (Too Many Requests) empty response if true, or just invoke the original handler that was passed to it (with the enhanced headers).&lt;/p&gt;
&lt;p&gt;To demonstrate this new rate limited router, we will modify the http client sample (&lt;a href=&quot;https://github.com/cfmobile/api-gateway-samples/tree/master/sample-http&quot; target=&quot;_blank&quot;&gt;see here&lt;/a&gt;) and make sure that the calls to the external endpoint now have rate limiting enabled.&lt;/p&gt;
&lt;pre class=&quot;brush: plain; title: ; notranslate&quot;&gt;var Router = require(&quot;Router&quot;);
var RatedRouter = require(&quot;modules/RatedRouter&quot;)
var _router = new Router();
var appRouter = new RatedRouter(_router);

var fbGraphClient = require('http')({
	baseUrl : 'http://graph.facebook.com/',
});

appRouter.get(&quot;/fb/pivotal&quot;, function(req, res) {
	var result = fbGraphClient.getJSON('pivotalsoftware', function(response) {
		return {
			reponse_code_from_fb : response.statusCode,
			data_from_fb : response.body
		};
	});
	res.setBody(result);
});

module.exports = _router;
&lt;/pre&gt;
&lt;p&gt;The new script is exact the same as the one on the samples, but now calls respect a limit that is stored on your redis database.&lt;/p&gt;
&lt;p&gt;To test it out, let’s create a script that tries to invoke this endpoint 11 times in windows of less than 60 seconds.&lt;/p&gt;
&lt;pre class=&quot;brush: plain; title: ; notranslate&quot;&gt;for i in {1..11}; do curl -i -H &quot;Authorization: apikey mykey&quot; http://localhost:8080/api/v1/fb/pivotal;  done

HTTP/1.1 200 OK
X-Application-Context: application:9999
X-RateLimit-Limit: 10
X-RateLimit-Remaining: 9
X-RateLimit-Reset: 1410442320000
Content-Type: application/json;charset=UTF-8
Transfer-Encoding: chunked
Server: Jetty(8.1.15.v20140411)

{hidden response body}

HTTP/1.1 200 OK
X-Application-Context: application:9999
X-RateLimit-Limit: 10
X-RateLimit-Remaining: 8
X-RateLimit-Reset: 1410442320000
Content-Type: application/json;charset=UTF-8
Transfer-Encoding: chunked
Server: Jetty(8.1.15.v20140411)
.
.
.
HTTP/1.1 429 429
X-Application-Context: application:9999
X-RateLimit-Limit: 10
X-RateLimit-Remaining: 0
X-RateLimit-Reset: 1410442320000
Content-Type: application/json;charset=UTF-8
Transfer-Encoding: chunked
Server: Jetty(8.1.15.v20140411)
&lt;/pre&gt;
&lt;p&gt;As you can see after the 10th call the server stops forwarding requests to the fbClient service and returns 429. The client receives a X-RateLimit-Reset header that informs when it would be ok to call the service again.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;This post was intended to demonstrate how easy it is to add features to your edge gateway when using Pivotal’s API Gateway. With very few lines of code we were able to enable rate limiting to our internal services without having to change any existing component.&lt;/p&gt;
&lt;p&gt;The seamless integration between javascript and java puts the best of both languages at your disposal. You can still rely on enterprise grade components built using Spring Framework, while leveraging a dynamic scripting language running on an asynchronous framework that the API Gateway provides.&lt;/p&gt;</description>
	<pubDate>Wed, 24 Sep 2014 16:31:58 +0000</pubDate>
</item>

</channel>
</rss>
